"x"
0,"TECH RECHERAGE PAPER
1-AI RESEARCH 
A-Can large language models be democratized
In early May, Meta released Open Pretrained Transformer (OPT-175B), a large language model (LLM) that can perform various tasks. Large language models have become one of the hottest areas of research in artificial intelligence in the past few years.
OPT-175B is the latest entrant in the LLM arms race triggered by OpenAI’s GPT-3, a deep neural network with 175 billion parameters. GPT-3 showed that LLMs can perform many tasks without undergoing extra training and only seeing a few examples (zero- or few-shot learning). Microsoft later integrated GPT-3 into several of its products, showing not only the scientific but also the commercial promises of LLMs.
What makes OPT-175B unique is Meta’s commitment to “openness,” as the model’s name implies. Meta has made the model available to the public (with some caveats). It has also released a ton of details about the training and development process. In a post published on the Meta AI blog, the company described its release of OPT-175B as “Democratizing access to large-scale language models.”
Meta’s move toward transparency is commendable. However, the competition over large language models has reached a point where it can no longer be democratized.
Looking inside large language models
Meta’s release of OPT-175B has some key features. It includes both pretrained models as well as the code needed to train and use the LLM. Pretrained models are especially useful for organizations that do not have the computational resources for training the model (training neural networks is much more resource-intensive than running them). It will also help reduce the massive carbon footprint caused by the computational resources needed to train large neural networks.
Like GPT-3, OPT comes in different sizes, ranging from 125 million to 175 billion parameters (models with more parameters have more capacity for learning). At the time of this writing, all models up to OPT-30B are accessible for download. The full 175-billion-parameter model will be made available to select researchers and institutions that fill a request form.
According to the Meta AI blog, “To maintain integrity and prevent misuse, we are releasing our model under a noncommercial license to focus on research use cases. Access to the model will be granted to academic researchers; those affiliated with organizations in government, civil society, and academia; along with industry research laboratories around the world.”
In addition to the models, Meta has released a full logbook that provides a detailed technical timeline of the development and training process of large language models. Published papers usually only include information about the final model. The logbook gives valuable insights about “how much compute was used to train OPT-175B and the human overhead required when underlying infrastructure or the training process itself becomes unstable at scale,” according to Meta.
Contrast with GPT-3
In its blog post, Meta states that large language models are mostly accessible through “paid APIs” and that restricted access to LLMs has “limited researchers’ ability to understand how and why these large language models work, hindering progress on efforts to improve their robustness and mitigate known issues such as bias and toxicity.”
This is a jab at OpenAI (and by extension Microsoft), which released GPT-3 as a black-box API service instead of making its model’s weights and source code available to the public. Among the reasons OpenAI stated for not making GPT-3 public was controlling misuse and development of harmful applications.
Meta believes that by making the models available to a wider audience, it will be in a better position to study and prevent any harm they can cause.
Here’s how Meta describes the effort: “We hope that OPT-175B will bring more voices to the frontier of large language model creation, help the community collectively design responsible release strategies, and add an unprecedented level of transparency and openness to the development of large language models in the field.”
The costs of large language models
However, it is worth noting that “transparency and openness” is not the equivalent of “democratizing large language models.” The costs of training, configuring, and running large language models remain prohibitive and are likely to grow in the future.
According to Meta’s blog post, its researchers have managed to considerably reduce the costs of training large language models. The company says that the model’s carbon footprint has been reduced to a seventh of GPT-3. Experts I had previously spoken to estimated GPT-3’s training costs to be up to $27.6 million.
This means that OPT-175B will still cost several million dollars to train. Fortunately, the pretrained model will obviate the need to train the model, and Meta says it will provide the codebase used to train and deploy the full model “using only 16 NVIDIA V100 GPUs.” This is the equivalent of an Nvidia DGX-2, which costs about $400,000, not a small sum for a cash-constrained research lab or an individual researcher. (According to a paper that provides more details on OPT-175B, Meta trained their own model with 992 80GB A100 GPUs, which are significantly faster than the V100.)
Meta AI’s logbook further confirms that training large language models is a very complicated task. The timeline of OPT-175B is filled with server crashes, hardware failures, and other complications that require a highly technical staff. The researchers also had to restart the training process several times, tweak hyperparameters, and change loss functions. All of these incur extra costs that small labs can’t afford.
The (undemocratic) future of large language models
Language models such as OPT and GPT are based on the transformer architecture. One of the key features of transformers is their ability to process large sequential data (e.g., text) in parallel and at scale.
In recent years, researchers have shown that by adding more layers and parameters to transformer models, they can improve their performance on language tasks. Some researchers believe that reaching higher levels of intelligence is only a scale problem. Accordingly, cash-rich research labs like Meta AI, DeepMind (owned by Alphabet), and OpenAI (backed by Microsoft) are moving toward creating larger and larger neural networks.
Last year, Microsoft and Nvidia created a 530-billion parameter language model called Megatron-Turing (MT-NLG). Last month, Google introduced the Pathways Language Model (PaLM), an LLM with 540 billion parameters. And there are rumors that OpenAI will release GPT-4 in the next few months.
However, larger neural networks also require larger financial and technical resources. And while larger language models will have new bells and whistles (and new failures), they will inevitably centralize power within the hands of a few wealthy companies by making it even harder for smaller research labs and independent researchers to work on large language models.
On the commercial side, big tech companies will have an even greater advantage. Running large language models is very expensive and challenging. Companies like Google and Microsoft have special servers and processors that allow them to run these models at scale and in a profitable way. For smaller companies, the overhead of running their own version of an LLM like GPT-3 is too prohibitive. Just as most businesses use cloud hosting services instead of setting up their own servers and data centers, out-of-the-box systems like the GPT-3 API will gain more traction as large language models become more popular.
This in turn will further centralize AI in the hands of big tech companies. More AI research labs will have to enter partnerships with big tech to fund their research. And this will give big tech more power to decide the future directions of AI research (which will probably be aligned with their financial interests). This can come at the cost of areas of research that do not have a short-term return on investment.
The bottom line is that, while we celebrate Meta’s move to bring transparency to LLMs, let’s not forget that the very nature of large language models is undemocratic and in favor of the very companies that are publicizing them.

B-This deep learning technique solves one of the tough challenges of robotics
For humans, working with deformable objects is not significantly more difficult than handling rigid objects. We learn naturally to shape them, fold them, and manipulate them in different ways and still recognize them.
But for robots and artificial intelligence systems, manipulating deformable objects present a huge challenge. Consider the series of steps that a robot must take to shape a ball of dough into pizza crusts. It must keep track of the dough as it changes shape, and at the same time, it must choose the right tool for each step of the work. These are challenging tasks for current AI systems, which are more stable in handling rigid-body objects, which have more predictable states.
Now, a new deep learning technique developed by researchers at MIT, Carnegie Mellon University, and the University of California at San Diego, shows promise to make robotics systems more stable in handling deformable objects. Called DiffSkill, the technique uses deep neural networks to learn simple skills and a planning module for combining the skills to solve tasks that require multiple steps and tools.
Handling deformable objects with reinforcement learning and deep learning
If an AI system wants to handle an object, it has to be able to detect and define its state and predict how it will look in the future. This is a problem that has been largely solved for rigid objects. With a good set of training examples, a deep neural network will be able to detect a rigid object from different angles. However, when it comes to deformable objects, the space of possible states becomes much more complicated.
“For rigid objects, we can describe its state with six numbers: Three numbers for its XYZ coordinates and another three numbers for its orientation,” Xingyu Lin, Ph.D. student at CMU and lead author of the DiffSkill paper, told TechTalks.
“However, deformable bodies, such as the dough or fabrics, have infinite degrees of freedom, making it much more difficult to describe their states precisely. Furthermore, the ways they deform are also harder to model in a mathematical way compared to rigid bodies.”
The development of differentiable physics simulators enabled the application of gradient-based methods to solve deformable object manipulation tasks. This is in contrast to the traditional reinforcement learning approach that tries to learn the dynamics of the environment and objects through pure trial-and-error interactions.
DiffSkill was inspired by PlasticineLab, a differentiable physics simulator that was presented at the ICLR conference in 2021. PlasticineLab showed that differentiable simulators can help short-horizon tasks.
But differentiable simulators still struggle with long-horizon problems that require multiple steps and the use of different tools. AI systems based on differentiable simulators also require the agent to know the full simulation state and relevant physical parameters of the environment. This is especially limiting for real-world applications, where the agent usually perceives the world through visual and depth sensory data (RGB-D).
“We started to ask if we can extract [the steps required to accomplish a task] as skills and also learn abstract notions about the skills so that we can chain them to solve more complex tasks,” Lin said.
DiffSkill is a framework where the AI agent learns skill abstraction using the differentiable physics model and composes them to accomplish complicated manipulation tasks.
Lin’s past work was focused on using reinforcement learning for the manipulation of deformable objects such as cloth, ropes, and liquids. For DiffSkill, he chose dough manipulation because of the challenges it poses.
“Dough manipulation is particularly interesting because it cannot be easily performed with the robot gripper, but requires using different tools sequentially, something humans are good at but is not very common for robots to do,” Lin said.
Once trained, DiffSkill can successfully accomplish a set of dough manipulation tasks using only RGB-D input.
Learning abstract skills with neural networks
DiffSkill is composed of two key components: a “neural skill abstractor” that uses neural networks to learn individual skills and a “planner” that composes the skill to solve long-horizon tasks.
DiffSkill uses a differentiable physics simulator to generate training examples for the skill abstractor. These samples show how to achieve a short-horizon goal with a single tool, such as using a roller to spread the dough or a spatula to displace the dough.
These examples are presented to the skill abstractor as RGB-D videos. Given an image observation, the skill abstractor must predict whether the desired goal is feasible or not. The model learns and tunes its parameters by comparing its prediction with the actual outcome of the physics simulator.
At the same time, DiffSkill trains a variational autoencoder (VAE) to learn a latent-space representation of the examples generated by the physics simulator. The VAE encodes images in a lower-dimension space that preserves important features and discards information that is not relevant to the task. By transferring the high-dimensional image space into the latent space, the VAE plays an important role in enabling DiffSkill to plan over long horizons and predict outcomes by observing sensory data.
One of the important challenges of training the VAE is making sure it learns the right features and generalizes to the real world, where the composition of visual data is different from those generated by the physics simulator. For example, the color of the roller pin or the table is not relevant to the task, but the position and angle of the roller and the location of the dough are.
Currently, the researchers are using a technique called “domain randomization,” which randomizes the irrelevant properties of the training environment such as background and lighting, and keeps the important features such as the position and orientation of tools. This makes the VAE more stable when applied to the real world.
“Doing this is not easy, as we need to cover all possible variations that are different between the simulation and the real world [known as the sim2real gap],” Lin said. “A better way is to use a 3D point cloud as representation of the scene, which is much easier to transfer from simulation to the real world. In fact, we are working on a follow-up project using point cloud as input.”
Planning long-horizon deformable object tasks
Once the skill abstractor is trained, DiffSkill uses the planner module to solve long-horizon tasks. The planner must determine the number and sequence of skills needed to go from the initial state to the destination.
This planner iterates over possible combinations of skills and the intermediate outcomes they yield. The variational autoencoder comes in handy here. Instead of predicting full image outcomes, DiffSkill uses the VAE to predict the latent-space outcome of intermediate steps toward the final goal.
The combination of abstract skills and latent-space representations makes it much more computationally efficient to draw a trajectory from the initial state to the goal. In fact, the researchers didn’t need to optimize the search function and used an exhaustive search of all combinations.
“The computation is not too much since we are planning over the skills and the horizon is not very long,” Lin said. “This exhaustive search eliminates the need for designing a sketch for the planner and might lead to novel solutions not considered by the designer in a more general way, although we did not observe this in the limited tasks we tried. Furthermore, more sophisticated search techniques could be applied as well”
According to the DiffSkill paper, “optimization can be done efficiently in around 10 seconds for each skill combination on a single NVIDIA 2080Ti GPU.”
Preparing the pizza dough with DiffSkill
The researchers tested the performance of DiffSkill against several baseline methods that have been applied to deformable objects, including two model-free reinforcement learning algorithms and a trajectory optimizer that only uses the physics simulator.
The models were tested on several tasks that require multiple steps and tools. For example, in one of the tasks, the AI agent must lift the dough with a spatula, place it on a cutting board, and spread it with a roller.
The results show that DiffSkill is significantly better than other techniques at solving long-horizon, multiple-tool tasks using only sensory information. The experiments show that when well trained, DiffSkill’s planner can find good intermediate states between the initial and goal states and find decent sequences of skills to solve tasks.
“One takeaway is that a set of skills can provide very important temporal abstraction, allowing us to reason over long-horizon,” Lin said. “This is also similar to how human approaches different tasks: thinking at different temporal abstractions instead of thinking what to do at every next second.”
However, there are also limits to DiffSkill’s capacity. For example, when performing one of the tasks that required three-stage planning, DiffSkill’s performance degrades significantly (though it is still better than other techniques). Lin also mentioned that in some cases, the feasibility predictor produces false positives. The researchers believe that learning a better latent space can help solve this problem.
The researchers are also exploring other directions to improve DiffSkill, including a more efficient planner algorithm that can be used for longer horizon tasks.
Lin hopes that one day, he can use DiffSkill on real pizza-making robots. “We are still far from this. Various challenges emerge from control, sim2real transfer, and safety. But we are now more confident at trying some long-horizon tasks,” he said.
C-FOMO is a TinyML neural network for real-time object detection
A new machine learning technique developed by researchers at Edge Impulse, a platform for creating ML models for the edge, makes it possible to run real-time object detection on devices with very small computation and memory capacity. Called Faster Objects, More Objects (FOMO), the new deep learning architecture can unlock new computer vision applications.
Most object-detection deep learning models have memory and computation requirements that are beyond the capacity of small processors. FOMO, on the other hand, only requires several hundred kilobytes of memory, which makes it a great technique for TinyML, a subfield of machine learning focused on running ML models on microcontrollers and other memory-constrained devices that have limited or no internet connectivity.
Image classification vs object detection
TinyML has made great progress in image classification, where the machine learning model must only predict the presence of a certain type of object in an image. On the other hand, object detection requires the model to identify more than object as well as the bounding box of each instance.
Object detection models are much more complex than image classification networks and require more memory.
“We added computer vision support to Edge Impulse back in 2020, and we’ve seen a tremendous pickup of applications (40 percent of our projects are computer vision applications),” Jan Jongboom, CTO at Edge Impulse, told TechTalks. “But with the current state-of-the-art models you could only do image classification on microcontrollers.”
Image classification is very useful for many applications. For example, a security camera can use TinyML image classification to determine whether there’s a person in the frame or not. However, much more can be done.
“It was a big nuisance that you’re limited to these very basic classification tasks. There’s a lot of value in seeing ‘there are three people here’ or ‘this label is in the top left corner,’ e.g., counting things is one of the biggest asks we see in the market today,” Jongboom says.
Earlier object detection ML models had to process the input image several times to locate the objects, which made them slow and computationally expensive. More recent models such as YOLO (You Only Look Once) use single-shot detection to provide near real-time object detection. But their memory requirements are still large. Even models designed for edge applications are hard to run on small devices.
“YOLOv5 or MobileNet SSD are just insanely large networks that never will fit on MCU and barely fit on Raspberry Pi–class devices,” Jongboom says.
Moreover, these models are bad at detecting small objects and they need a lot of data. For example, YOLOv5 recommends more than 10,000 training instances per object class.
The idea behind FOMO is that not all object-detection applications require the high-precision output that state-of-the-art deep learning models provide. By finding the right tradeoff between accuracy, speed, and memory, you can shrink your deep learning models to very small sizes while keeping them useful.
Instead of detecting bounding boxes, FOMO predicts the object’s center. This is because many object detection applications are just interested in the location of objects in the frame and not their sizes. Detecting centroids is much more compute-efficient than bounding box prediction and requires less data.
Redefining object detection deep learning architectures
FOMO also applies a major structural change to traditional deep learning architectures.
Single-shot object detectors are composed of a set of convolutional layers that extract features and several fully-connected layers that predict the bounding box. The convolution layers extract visual features in a hierarchical way. The first layer detects simple things such as lines and edges in different directions. Each convolutional layer is usually coupled with a pooling layer, which reduces the size of the layer’s output and keeps the most prominent features in each area.
The pooling layer’s output is then fed to the next convolutional layer, which extracts higher-level features, such as corners, arcs, and circles. As more convolutional and pooling layers are added, the feature maps zoom out and can detect complicated things such as faces and objects.
Finally, the fully connected layers flatten the output of the final convolution layer and try to predict the class and bounding box of objects.
FOMO removes the fully connected layers and the last few convolution layers. This turns the output of the neural network into a sized-down version of the image, with each output value representing a small patch of the input image. The network is then trained on a special loss function so that each output unit predicts the class probabilities for the corresponding patch in the input image. The output effectively becomes a heatmap for object types.
here are several key benefits to this approach. First, FOMO is compatible with existing architectures. For example, FOMO can be applied to MobileNetV2, a popular deep learning model for image classification on edge devices.
Also, by considerably reducing the size of the neural network, FOMO lowers the memory and compute requirements of object detection models. According to Edge Impulse, it is 30 times faster than MobileNet SSD while it can run on devices that have less than 200 kilobytes of RAM.
For example, the following video shows a FOMO neural network detecting objects at 30 frames per second on an Arduino Nicla Vision with a little over 200 kilobytes of memory. On a Raspberry Pi 4, FOMO can detect objects at 60fps as opposed to the 2fps performance of MobileNet SSD.
Jongboom told me that FOMO was inspired by work that Mat Kelcey, Principal Engineer at Edge Impulse, did around neural network architecture for counting bees.
“Traditional object detection algorithms (YOLOv5, MobileNet SSD) are bad at this type of problem (similar-sized objects, lots of very small objects) so he designed a custom architecture that optimizes for these problems,” he said.
The granularity of FOMO’s output can be configured based on the application and can detect many instances of objects in a single image.
Limits of FOMO
The benefits of FOMO do not come without tradeoffs. It works best when objects are of the same size. It’s like a grid of equally sized squares, each of which detects one object. Therefore, if there is one very large object in the foreground and many small objects in the background, it will not work so well.
Also, when objects are too close to each other or overlapping, they will occupy the same grid square, which reduces the accuracy of the object detector (see video below). You can overcome this limit to an extent by reducing FOMO’s cell size or increasing the image resolution.
FOMO is especially useful when the camera is in a fixed location, for example scanning objects on a conveyor belt or counting cars in a parking lot.
The Edge Impulse team plans to expand on their work in the future, including making the model even smaller, under 100 kilobytes and making it better at transfer learning.
D-New RL technique achieves superior performance in control tasks
Reinforcement learning is one of the fascinating fields of computer science, and it has proven useful in solving some of the toughest challenges of artificial intelligence and robotics. Some scientists believe that reinforcement learning will play a key role in cracking the enigma of human-level artificial intelligence.
But many hurdles stand between current reinforcement learning systems and a possible path toward more general and robust forms of AI. Many RL systems struggle with long-term planning, training-sample efficiency, transferring knowledge to new tasks, dealing with the inconsistencies of input signals and rewards, and other challenges that occur in real-world applications. There are dozens of reinforcement learning algorithms—and more recently deep RL—each of which addresses some of these challenges while struggling with others.
A new reinforcement learning technique developed by researchers at the University of California, San Diego, brings together two major branches of RL to create more efficient and robust agents. Dubbed Temporal Difference Learning for Model Predictive Control (TD-MPC), the new technique combines the strengths of “model-based” and “model-free” RL to match and outperform state-of-the-art algorithms in challenging control tasks.
Model-free vs model-based reinforcement learning
In reinforcement learning, an agent seeks a goal such as moving to a destination location, winning a game, reducing energy consumption in a factory, or maximizing ad clicks. The agent can interact with its environment through a set of actions, such as displacing pieces on a chessboard, displaying an ad on a website, or moving a limb on a robot. Every action changes the state of the environment and provides the agent with a reward, a feedback signal that helps it determine whether it is getting closer to its goal or not. Through multiple training episodes, RL agents learn action policies that maximize their rewards.
One common way to categorize RL algorithms is to split them between model-based and model-free techniques.
Model-free RL agents develop their policies by trying out many actions and learning successful sequences. A model-free RL agent learns the values of actions purely based on past experience. For example, consider an RL algorithm for driving cars. An untrained RL agent will not know that driving straight over a cliff will cause the car to drop and crash. But once it experiences it, it will get the negative reward signal and learn to take another action (hit the brakes or steer in another direction?) the next time it reaches a cliff.
On the other hand, model-based RL tries to learn the dynamics that govern its environment. The benefit of modeling the environment is that the RL agent can predict the outcome of actions it hasn’t taken before. In the self-driving car example mentioned above, a model-based RL agent doesn’t need to experience jumping over a cliff or bridge to realize that they lead to bad outcomes.
Model-based RL is very appealing because it is more sample-efficient in comparison to model-free algorithms. Model-based RL agents need less experience gathering and can “imagine” different scenarios without taking actual actions in the environment. However, learning an accurate model of the environment is often very difficult, especially in complex environments where states and actions are continuous (as opposed to discrete actions such as in chess and go) and the environment is stochastic and can change without the agent taking any action (such as the real world).
Therefore, model-free reinforcement learning remains the main solution for many applications, especially where the RL agents must handle continuous actions and states.
Temporal Difference Learning for Model Predictive Control (TD-MPC)
Temporal Difference Learning for Model Predictive Control, the new technique developed by the researchers at UCSD, combines the strengths of model-free and model-based reinforcement learning to overcome their respective weaknesses and train RL agents to become more robust and sample-efficient.
The model-based algorithm they used is “model predictive control.” MPC is very useful for finding local solutions to control tasks. But it has a limited time horizon and performs poorly when faced with problems that require long-term planning, especially in applications where compute resources are scarce.
“A key feature of MPC is that it approximates a global, long-horizon solution by solving a local, finite-horizon optimization problem,” Nicklas Hansen, lead author of the TD-MPC paper, told TechTalks. “This is both a blessing and a curse, as it ties performance with the planning horizon and hence computational budget. This is a non-issue in applications where models are easy to evaluate, but when the model is a neural network, it can be rather costly to do planning over long horizons.”
MPC performs very well when the model is accurate and there are sufficient compute resources for planning. But as learning an accurate model of the environment becomes harder, engineers must adopt large models that are slow and costly to train. In comparison, model-free RL algorithms directly optimize a policy for long-horizon performance, which makes them incredibly fast but somewhat less expressive.
“We wanted to incorporate the key strengths of model-free RL into the MPC framework to get the best of both worlds,” Hansen said.
To achieve this goal, the researchers chose to optimize a combination of short-term model-based predictions and long-term value estimations using “temporal difference learning” (TD learning), a technique commonly used in model-free RL algorithms. TD learning is a mechanism to calculate the values of different action trajectories for RL agents.
TD-MPC uses model-based reinforcement learning to optimize short-term decisions and the model-free value function to estimate long-term decisions. For example, in a humanoid locomotion task, the model-based component can control the joint movements while the model-free component chooses the trajectory of the agent.
Learning latent representations of the environment
One of the great challenges of modeling complex environments is finding the right elements and features to extract and predict. Many popular model-based RL systems try to learn and predict full image frames when dealing with visual input. This is a very difficult task that requires immense computational resources and yields unstable results.
To make the environment modeling more efficient, Hansen and his colleagues designed a “Task-Oriented Latent Dynamics” (TOLD) model that is jointly learned with a terminal value function using TD-learning. Basically, instead of trying to model the entire input space, TOLD extracts a smaller subset of features that are relevant to the task and goal of the RL agent.
“Whereas most previous data-driven MPC methods try to model everything in the environment via a future state or image prediction loss, our TD-MPC algorithm instead learns a task-oriented latent dynamics model through reward and value predictions,” Hansen said. “This allows us to only model parts of the environment that are actually relevant to planning, which we find is both easier to opti"
1,"View sample internet research paper. Browse research paper examples for more inspiration. If you need a thorough research paper written according to all the academic standards, you can always turn to our experienced writers for help. This is how your paper can get an A! Feel free to contact our writing service for professional assistance. We offer high-quality assignments for reasonable rates.
The Internet is a vast global system of interconnected technical networks made up of heterogeneous information and communication technologies. It is also a social and economic assemblage that allows diverse forms of communication, creativity, and cultural exchange at a scope and scale unknown before the late twentieth century.
The terms Internet and net are often used when discussing the social implications of new information technologies, such as the creation of new communal bonds across great distances or new forms of wealth and inequality. Such a usage is imprecise: The Internet is distinct from the applications and technologies that are built upon it, such as e-mail, the World Wide Web, online gaming, filesharing networks, and e-commerce and e-governance initiatives. There are also many networks that are or were once distinct from the Internet, such as mobile telephone networks and electronic financial networks.
Stated more precisely, the Internet is an infrastructural substrate that possesses innovative social, cultural, and economic features allowing creativity (or innovation) based on openness and a particular standardization process. It is a necessary, but not a sufficient, condition for many of the social and cultural implications often attributed to it. Understanding the particularity of the Internet can be key to differentiating its implications and potential impact on society from the impacts of “information technology” and computers more generally.
HISTORY AND STRUCTURE OF THE INTERNET
The Internet developed through military, university, corporate, and amateur user innovations occurring more or less constantly beginning in the late 1960s. Despite its complexity, it is unlike familiar complex technical objects—for example, a jumbo jetliner—that are designed, tested, and refined by a strict hierarchy of experts who attempt to possess a complete overview of the object and its final state. By contrast, the Internet has been subject to innovation, experimentation, and refinement by a much less well-defined collective of diverse users with wide-ranging goals and interests.
In 1968 the Internet was known as the ARPAnet, named for its principal funding agency, the U.S. Department of Defense Advanced Research Projects Agency (ARPA). It was a small but extensive research project organized by the Information Processing Techniques Office at ARPA that focused on advanced concepts in computing, specifically graphics, time-sharing, and networking. The primary goal of the network was to allow separate administratively bounded resources (computers and software at particular geographical sites) to be shared across those boundaries, without forcing standardization across all of them. The participants were primarily university researchers in computer and engineering departments. Separate experiments in networking, both corporate and academic, were also under way during this period, such as the creation of “Ethernet” by Robert Metcalfe at Xerox PARC and the X.25 network protocols standardized by the International Telecommunications Union.
By 1978 the ARPAnet had grown to encompass dozens of universities and military research sites in the United States. At this point the project leaders at ARPA recognized a need for a specific kind of standardization to keep the network feasible, namely a common operating system and networking software that could run on all of the diverse hardware connected to the network. Based on its widespread adoption in the 1970s, the UNIX operating system was chosen by ARPA as one official platform for the Internet. UNIX was known for its portability (ability to be installed on different kinds of hardware) and extensibility (ease with which new components could be added to the core system). Bill Joy (who later cofounded Sun Microsystems) is credited with the first widespread implementation of the Internet Protocol (IP) software in a UNIX operating system, a version known as Berkeley Systems Distribution (BSD).
The Internet officially began (in name and in practice) in 1983, the date set by an ad hoc group of engineers known as the Network Working Group (NWG) as the deadline for all connected computers to begin using the Transmission Control Protocol and Internet Protocol (TCP/IP) protocols. These protocols were originally designed in 1973 and consistently improved over the ensuing ten years, but only in 1983 did they become the protocols that would define the Internet. At roughly the same time, ARPA and the Department of Defense split the existing ARPAnet in two, keeping “Milnet” for sensitive military use and leaving ARPAnet for research purposes and for civilian uses.
From 1983 to 1993, in addition to being a research network, the Internet became an underground, subcultural phenomenon, familiar to amateur computer enthusiasts, university students and faculty, and “hackers.” The Internet’s glamour was largely associated with the arcane nature of interaction it demanded—largely text-based, and demanding access to and knowledge of the UNIX operating system. Thus, owners of the more widespread personal computers made by IBM and Apple were largely excluded from the Internet (though a number of other similar networks such as Bulletin Board Services, BITNet, and FidoNET existed for PC users).
A very large number of amateur computer enthusiasts discovered the Internet during this period, either through university courses or through friends, and there are many user-initiated innovations that date to this period, ranging from games (e.g., MUDs, or Multi-User Dungeons) to programming and scripting languages (e.g., Perl, created by Larry Wall) to precursors of the World Wide Web (e.g., WAIS, Archie, and Gopher). During this period, the network was overseen and funded by the National Science Foundation, which invested heavily in improving the basic infrastructure of fiberoptic “backbones” in the United States in 1988. The oversight and management of the Internet was commercialized in 1995, with the backing of the presidential administration of Bill Clinton.
In 1993 the World Wide Web (originally designed by Tim Berners-Lee at CERN in Switzerland) and the graphical Mosaic Web Browser (created by the National Center for Supercomputing Applications at the University of Illinois) brought the Internet to a much larger audience. Between 1993 and 2000 the “dot-com” boom drove the transformation of the Internet from an underground research phenomena to a nearly ubiquitous and essential technology with far-reaching effects. Commercial investment in infrastructure and in “web presence” saw explosive growth; new modes of interaction and communication (e.g., e-mail, Internet messaging, and mailing lists) proliferated; Uniform Resource Locators (URLs, such as www.britannica.com) became a common (and highly valued) feature of advertisements and corporate identity; and artists, scientists, citizens, and others took up the challenge of both using and understanding the new medium.
PROTOCOLS AND THE INTERNET STANDARDS PROCESS
The core technical components of the Internet are standardized protocols, not hardware or software, strictly speaking—though obviously it would not have spread so extensively without the innovations in microelectronics, the continual enhancement of telecommunications infrastructures around the globe, and the growth in ownership and use of personal computers over the last twenty years. Protocols make the “inter” in the Internet possible by allowing a huge number of nonoverlapping and incompatible networks to become compatible and to route data across all of them.
The key protocols, known as TCP/IP, were designed in 1973 by Vint Cerf and Robert Kahn. Other key protocols, such as the Domain Name System (DNS) and User Datagram Protocol (UDP), came later. These protocols have to be implemented in software (such as in the UNIX operating system described above) to allow computers to interconnect. They are essentially standards with which hardware and software implementations must comply in order for any type of hardware or software to connect to the Internet and communicate with any other hardware and software that does the same. They can best be understood as a kind of technical Esperanto.
The Internet protocols differ from traditional standards because of the unconventional social process by which they are developed, validated, and improved. The Internet protocols are elaborated in a set of openly available documents known as Requests for Comments (RFCs), which are maintained by a loose federation of engineers called the Internet Engineering Task Force (IETF, the successor to the Network Working Group). The IETF is an organization open to individuals (unlike large standards organizations that typically accept only national or corporate representatives) that distributes RFCs free of charge and encourages members to implement protocols and to improve them based on their experiences and users’ responses. The improved protocol then may be released for further implementation.
This “positive feedback loop” differs from most “consensus-oriented” standardization processes (e.g., those of international organizations such as ISO, the International Organization for Standardization) that seek to achieve a final and complete state before encouraging implementations. The relative ease with which one piece of software can be replaced with another is a key reason for this difference. During the 1970s and 1980s this system served the Internet well, allowing it to develop quickly, according to the needs of its users. By the 1990s, however, the scale of the Internet made innovation a slower and more difficult procedure—a fact that is most clearly demonstrated by the comparatively glacial speed with which the next generation of the Internet protocol (known as IP Version 6) has been implemented.
Ultimately, the IETF style of standardization process has become a common cultural reference point of engineers and expert users of the Internet, and has been applied not only to the Internet, but also to the production of applications and tools that rely on the Internet. The result is a starkly different mode of innovation and sharing that is best exemplified by the growth and success of so-called “free software” or “open-source software.” Many of the core applications that are widely used on the Internet are developed in this fashion (famous examples include the Linux operating system kernel and the Apache Web Server).
CULTURAL, SOCIAL, AND ECONOMIC IMPLICATIONS OF THE INTERNET
As a result of the unusual development process and the nature of the protocols, it has been relatively easy for the Internet to advance around the globe and to connect heterogeneous equipment in diverse settings, wherever there are willing and enthusiastic users with sufficient technical know-how. The major impediment to doing so is the reliability (or mere existence) of preexisting infrastructural components such as working energy and telecommunications infrastructures. Between 1968 and 1993 this expansion was not conducted at a national or state level, but by individuals and organizations who saw local benefit in expanding access to the global network. If a university computer science department could afford to devote some resources to computers dedicated to routing traffic and connections, then all the researchers in a department could join the network without needing permission from any centralized state authority. It was not until the late 1990s that Internet governance became an issue that concerned governments and citizens around the world. In particular, the creation of the Internet Corporation for Assigned Names and Numbers (ICANN) has been the locus of fractious dispute, especially in international arenas. ICANN’s narrow role is to assign IP numbers (e.g., 192.168.0.1) and the names they map to (e.g., www.wikipedia.org), but it has been perceived, rightly or wrongly, as an instrument of U.S. control over the Internet.
With each expansion of the Internet, issues of privacy, security, and organizational (or national) authority have become more pressing. At its outset the Internet protocols sought to prioritize control within administrative boundaries, leaving rules governing use to the local network owners. Such a scheme obviated the need for a central authority that determined global rules about access, public/private boundaries, and priority of use. With the advent of widespread commercial access, however, such local control has been severely diluted, and the possibility for individual mischief (e.g., identity theft, spam, and other privacy violations) has increased with increasing accessibility.
On the one hand, increased commercial access means a decline in local organized authority over parts of the Internet in favor of control of large segments by Internet Service Providers (ISPs) and telecommunications/cable corporations. On the other hand, as the basic infrastructure of the Internet has spread, so have the practices and norms that were developed in concert with the technology—including everything from the proper way to configure a router, to norms of proper etiquette on mailing lists and for e-mail. Applications built on top of the Internet have often adopted such norms and modes of use, and promoted a culture of innovation, of “hacking” (someone who creates new software by employing a series of modifications that exploit or extend existing code or resources, with good or bad connotations depending on the context), and of communal sharing of software, protocols, and tools.
It is thus important to realize that although most users do not experience the Internet directly, the development of the particular forms of innovation and openness that characterize the Internet also characterize the more familiar applications built on top of it, due to the propagation of these norms and modes of engineering. There is often, therefore, a significant difference between innovations that owe their genesis to the Internet and those developed in the personal computer industry, the so-called “proprietary” software industry, and in distinct commercial network infrastructures (e.g., the SABRE system for airline reservations, or the MOST network for credit card transactions). The particularity of the Internet leads to different implications and potential impact on society than the impacts of “information technology” or computers more generally.
DIGITAL MUSIC, FILM, AND INTELLECTUAL PROPERTY
One of the most widely discussed and experienced implications of the Internet is the effect on the culture industries, especially music and film. As with previous media (e.g., video and audio cassette recorders), it is the intersection of technology and intellectual property that is responsible for the controversy. Largely due to its “openness,” the Internet creates the possibility for low-cost and extremely broad and fast distribution of cultural materials, from online books to digital music and film. At the same time, it also creates the possibility for broad and fast violation of intellectual property rights—rights that have been strengthened considerably by the copyright act of 1976 and the Digital Millennium Copyright Act (1998).
The result is a cultural battle over the meaning of “sharing” music and movies, and the degree to which such sharing is criminal. The debates have been polarized between a “war on piracy” on the one hand (with widely varying figures concerning the economic losses), and “consumer freedom” on the other—rights to copy, share, and trade purchased music. The cultural implication of this war is a tension among the entertainment industry, the artists and musicians, and the consumers of music and film. Because the openness of the Internet makes it easier than ever for artists to distribute their work, many see a potential for direct remuneration, and cheaper and more immediate access for consumers. The entertainment industry, by contrast, argues that it provides more services and quality—not to mention more funding and capital—and that it creates jobs and contributes to a growing economy. In both cases, the investments are protected primarily by the mechanism of intellectual property law, and are easily diluted by illicit copying and distribution. And yet, it is unclear where to draw a line between legitimate sharing (which might also be a form of marketing) and illegitimate sharing (“piracy,” according to the industry).
THE DIGITAL DIVIDE
A key question about the Internet is that of social equity and access. The term digital divide has been used primarily to indicate the differential in individual access to the Internet, or in computer literacy, between rich and poor, or between developed and developing nations. A great deal of research has gone into understanding inequality of access to the Internet, and estimates of both differential access and the rate of the spread of access have varied extremely widely, depending on methodology. It is, however, clear from the statistics that between 1996 and 2005 the rate of growth in usage has been consistently greater than 100 percent in almost all regions of the globe at some times, and in some places it has reached annual growth rates of 500 percent or more. Aside from the conclusion that the growth in access to the Internet has been fantastically rapid, there are few sure facts about differential access.
There are, however, a number of more refined questions that researchers have begun investigating: Is the quantity or rate of growth in access to the Internet larger or smaller than in the case of other media (e.g., television, print, and radio)? Are there significant differences within groups with access (e.g., class, race, or national differences in quality of access)? Does access actually enhance or change a person’s life chances or opportunities?
The implication of a digital divide (whether between nations and regions, or within them) primarily concerns the quality of information and the ability of individuals to use it to better their life chances. In local terms, this can affect development issues broadly (e.g., access to markets and government, democratic deliberation and participation, and access to education and employment opportunities); in global terms, differential access can affect the subjective understandings of issues ranging from religious intolerance to global warming and environmental issues to global geopolitics. Digital divides might also differ based on the political situation—such as in the case of the Chinese government’s attempt to censor access to politicized information, which in turn can affect the fate of cross-border investment and trade."
2,"3-Nanotechnology Research Paper
ABSTRACT
The miniaturization of science and engineering is just one aspect of the many ways that the rapidly expanding ﬁeld of nanotechnology promises to revolutionize the landscape of science, technology, and society. With potential applications stretching across the wide spectrum of research and development in consumer electronics and cosmetics, drug development and delivery in the pharmaceutical industry, medical technologies and therapeutics, energy production and storage, environmental engineering and remediation, industrial manufacturing, and textile production, nanoscience and nanotechnologies have demonstrated breathtaking potential. Some of its most ardent supporters project the future of this technology even more optimistically. Others disagree, suggesting that hype surrounding speculative nanotechnology is well beyond the plausible potential of the technology and seems more at home in science ﬁction novels and ﬁlms. This research paper explores the developing ﬁeld of nanotechnology and given its vast potential considers whether there are inherent concerns or dangers in the utilization of these technologies. Additional attention is given to ethical considerations and implications of these technologies, as well as to policy questions that are raised with respect to regulating nanotechnology for the public good.
INTRODUCTION
Few areas of research and development have captured the imagination with the potential for such broad impact and implications as has nanoscience and nanotechnology. Applications for nanotechnology cross such disparate ﬁelds of research, development, and production as materials science (with agricultural, industrial/manufacturing, and textile applications), energy production, environmental sciences, information and communication technologies, cosmetics, healthcare (with diagnostics, drug delivery, gene therapy, and potential contributions to regenerative medicine), and the possibility of futuristic applications with nanobots and nanotechnology assemblers. As of late 2014, an inventory by the Woodrow Wilson Center identiﬁed over 1,800 consumer products containing nanoscale materials (Project on Emerging Nanotechnologies 2014).
“Nanoscale technology” or “nanotechnology” generally refers to the products of science and engineering that seek to understand and control matter at the nanoscale level. While deﬁnitions of the terminology are not universally agreed upon, the U.S. National Nanotechnology Initiative (NNI n.d.) deﬁnes the “nanoscale” as the “dimensional range of approximately 1–100 nm” (a nanometer is one-billionth of a meter). Comparatively, a sheet of paper is roughly 100,000 nm thick. The ability to manipulate and control materials at the nanoscale level offers a variety of promising and wide-ranging applications. From enhancements to already existent consumer products to advances in industrial manufacturing to regenerative medicine and the possibility of futuristic applications, nanotechnology has the potential to revolutionize far-reaching aspects of human life. Strong proponents of the future of these technologies offer visions that seem to stretch the limits of credulity and posit applications that may seem more at home in science ﬁction novels and ﬁlms. Such proposals have led others to charge that this very promise has led to unreasonable research hype and speculations, and thus critics within the research and ethics communities increasingly are calling for more chastened projections of potential research deliverables, as well as what they claim are more “realistic” potential beneﬁts.
As an area of emerging technology, nanotechnology (at least in its formal use of nanoscience) and the corresponding ﬁeld of nanotechnology ethics (or nanoethics) are relatively recent developments of the 1990s with much of their growth occurring within the early years of the twenty-ﬁrst century. The purpose of this research paper is to explore the historical background leading to contemporary research, development, and product release of nanotechnologies; clarify key terminology; examine recent, near future, and speculative applications of these technologies; and conclude with a discussion of various ethical considerations raised by nanotechnologies.
HISTORICAL DEVELOPMENT
BACKGROUND
“Nanotechnology” was ﬁrst coined in 1974 by the Japanese researcher Norio Taniguchi “to mean precision machining with tolerances of a micrometer or less” (Voss 1999). While this marked the ﬁrst known usage of the term, the formal discipline of nanoscience is indebted to the work of Richard Feynman, a physicist who speculated in a 1959 presentation “There’s Plenty of Room at the Bottom.” In his talk, Feynman (1960) explored the possibility of printing the entire 24 volumes of the Encyclopedia Britannica on the head of a pin, thus requiring the equivalent of an electron microscope to read the nanometer-scale transcription. Feynman speculated on the ability to manipulate material at the atomic and molecular level to create what would later be referred to as nanoscale devices or nanomachines utilizing a “bottom-up approach” (O’Mathúna 2009). This “bottom-up approach” of nanoscale assembly is often contrasted with a “top-down approach” that seeks to apply larger-scale manufacturing techniques and principles at increasingly smaller scales (Navarro and Planell 2012; Mitchell et al. 2007). Feynman’s 1959 presentation advanced possibilities that would characterize both approaches.
Eric Drexler’s 1986 volume Engines of Creation was instrumental in introducing nanotechnology into public awareness and the ensuing policy discourse. In his work, Drexler revisited the two approaches to the miniaturization of research by contrasting “bulk technology”– similar to a “topdown approach” – with that of “molecular technology” – similar to a “bottom-up approach” and interchangeable with the term “nanotechnology” (Drexler 1990). In describing these two technologies, Drexler notes that previous technologies that moved “room-sized computers” to silicon chips relied upon the older model of bulk technology, while molecular technology will allow for the possibility of precision devices made of “nanocircuits and nanomachines” (Drexler 1990). Within the nanoscience and nanotechnology research community, Drexler’s promotion of nanobots and self-replicating nanoassemblers has had a polarizing effect resulting in a division between those who focus on the near-term potential of nanotechnology (based upon strides that have already been made and are perceived to be likely outcomes from current research capabilities) and those who take a more futuristic or speculative approach to nanotechnologies. Such speculative approaches often advance the idea of increasing convergence of emerging technologies to promote such possibilities as radical life extension, human enhancement and augmentation, cryonics, and attempts to guide the future of human evolution (O’Mathúna 2009).
One of the oldest known applications of nanotechnology dates back to the fourth-century Roman Lycurgus Cup made of dichroic glass. Housed in the British Museum, this cup contains metal nanoparticles in the glass which alter its color when held up to a light source (O’Mathúna 2009; Khan 2012). Little is known about how the glassmakers came to utilize these nanoparticles. Later applications throughout history included use of nanoparticles on glass windows (Ireland, mid-ﬁfth century), ceramics (Islamic world and later Europe, ninth to seventeenth centuries), and traditional medicines in South Asia. Furthermore, weapons metallurgy of the thirteenth to eighteenth centuries used carbon nanotubes and cementite nanowires in the making of “Damascus” saber blades (Khan 2012).
A major breakthrough in contemporary nanotechnology research occurred in 1985 with the creation of a new form of carbon known as the buckminsterfullerene or more commonly a “buckyball” (also bucky-ball) due to its resemblance to a soccer ball. The buckyball was followed in 1991 with the related development of carbon nanotubes. Due to their unique structure, carbon nanotubes were viewed as a major development for materials science, as they are approximately “60 times stronger than steel and capable of conducting electricity 1,000 times better than copper” (Khan 2012).
NANOTECHNOLOGY THEMES IN FICTION
Given the potential of nanotechnology, it is no surprise that it has also captured the attention of science ﬁction authors. The value of ﬁction and ﬁlm to philosophy, ethics, and medicine has been on the rise in the past decade with increased attention given to the contributions of the medical humanities and its role in education. Examples of nanotechnology in science ﬁction range from cautionary tales such as Michael Crichton’s 2002 novel Prey of an uncontrolled nanotech swarm to classic works such as Isaac Asimov’s Fantastic Voyage with its miniaturized machines (O’Mathúna 2009). Likewise, William Gibson’s The Peripheral (2014) explores two futures in which the technology of 3-D printing gives way to the proliferation of self-replicating nanoassemblers, demonstrating an interesting progression of possible technological evolution.
Perhaps the most direct connection between science ﬁction and bioethics is that nanotechnology and similar technologies offer “science ﬁction authors the opportunity to make predictions about what is to come and to issue warnings” (O’Mathúna 2009). Such “speculative ethics” may project and support either dystopian or utopian conceptions of the future. Nordmann (2007) and others counter that such speculative approaches treat imagined futures as if they already exist and thereby displace actual presenting issues. Balancing ethical reﬂection of existing nanotechnologies with anticipating futuristic applications of nanotechnologies is a delicate task. As with all rapidly evolving arenas of emerging technologies, merely setting aside the speculative dimension of potential applications seems to ignore the length of deliberation necessary to implement appropriate policy and regulatory regimes so as to avoid both conceptual and policy vacuums. Technology assessment and ethical analysis of emerging technologies such as nanotechnologies are perhaps necessarily speculative endeavors. However, an appropriate caution must be raised to prevent the conﬂation of hypotheticals with presenting technologies in such ethical discourse.
FUNDING AND INTERNATIONAL RESEARCH AND DEVELOPMENT
The U.S. government took an early lead in funding research and development of nanoscience and nanotechnology, launching what would eventually become the NNI in the late 1990s during the administration of President Bill Clinton. Recognizing the value of a coordinated federal effort for nanotechnology research and development, in 2003 President George W. Bush signed into law the 21st Century Nanotechnology Research and Development Act (NNI n.d.). Initial U.S. government funding of $464 million in 2001 quickly grew to nearly $2 billion in 2010, with 2015 budget allocations for the NNI set at more than $1.5 billion (NNI n.d.). In addition to the federal funding provided by the NNI, both state-level and private funding are substantial supplements to U.S. research initiatives in nanotechnology. Cumulative NNI investment from 2001 to 2015 is estimated at approximately $21 billion.
The European Commission has likewise taken “a leading and coordinating role in nanoscience and nanotechnology development in Europe,” as well as individual governmental support from several EU member states that invest directly in research and development such as the United Kingdom, Germany, France, and others (Malsch and Emond 2014). The European Commission in 2007 had allocated approximately EUR 600 million through its Seventh Framework Programme (FP7) for research in the areas of nanomedicine and nanomaterials (Navarro and Planell 2012). Similarly, signiﬁcant research initiatives in Australia, Japan, and South Korea as well as in numerous emerging economies such as Brazil, Argentina, Russia, India, and China demonstrate the global involvement of research investment in nanoscience and nanotechnology research, infrastructure, and development.
DEFINING TERMS
As noted earlier, “nanotechnology” refers to the products of science and engineering that seek to understand and control matter at the nanoscale level. The interest of studying and manipulating materials at the nanoscale level is that such materials exhibit novel properties that are distinct from larger scales of the same materials.
While the upper limit of 100 nanometers (nm) is commonly used, some have noted the lack of scientiﬁc evidence to justify such a demarcation in that some materials exhibit similar properties at larger scales and, thus, have proposed an alternative dimensional range of 1–1,000 nm (O’Mathúna 2009). The U.S. Food and Drug Administration (FDA) in a 2014 industry guidance suggests the consideration of those materials with dimensions outside of the nanoscale but which may demonstrate “novel properties and phenomena similar to those seen in materials with dimensions in the nanoscale range” and thus proposes an upper limit of 1 mm (1,000 nm) for regulation (FDA 2014). Others such as the European Commission have proposed more functional deﬁnitions that focus on controlling “the structure and behaviour of matter at the level of atoms and molecules” rather than emphasizing speciﬁc size ranges (European Commission 2014; Khan 2012). As O’Mathúna (2009) notes, part of the challenge to precisely deﬁning the boundaries of nanotechnology is that within the nanoscale range, “a unique combination of quantum and macroscale effects converge to give nanoparticles their unique and interesting properties,” particularly as they interact with cells and living tissues to “permeat[e] the impermeable.”
The UK’s Royal Society and Royal Academy of Engineering have emphasized the distinction between “nanoscience” and “nanotechnology.” The former is “the study of phenomena and manipulation of materials at atomic, molecular and macromolecular scales, where properties differ signiﬁcantly from those at a larger scale” (The Royal Society and The Royal Academy of Engineering 2004; ten Have 2007). The latter is “the design, characterization, production and application of structures, devices and systems by controlling shape and size at nanometer scale.” Furthermore, given the range of disciplines and applications covered by nanoscience and nanotechnology research, ten Have (2007) and others suggest that “nanotechnologies” is a more accurate term to designate the ﬁeld. With their potential impact, nanotechnologies have been deemed by many to be not only enabling technologies but also transformative or disruptive technologies, capable of altering a disciplinary landscape or aspect of technology and society (Jotterand 2008).
Nanotechnologies are available in a variety of applications, of which nanoparticles, carbon nanotubes, and nanobiosensors are some of the most common. As noted earlier, one of the oldest and most widely utilized nanotechnologies is nanoparticles. Some of the most commonly used types of nanoparticles are gold nanoparticles, magnetic nanoparticles, and a semiconductor nanocrystal referred to as a quantum dot (Navarro and Planell 2012). In contrast to their bulk material counterparts (generally those materials larger than 1 mm), nanoparticles exhibit novel properties (e.g., color, reactivity, conductivity, melting point, hardness, etc.) that introduce potential uses beyond those of their bulk counterparts. Nanoparticles are being utilized or researched for use in cosmetics, surface coatings for manufacturing and textiles, diagnostics, environmental remediation, energy production, as well as numerous medical applications.
One of the early developments in contemporary nanotechnology was the creation of carbon nanotubes (CNTs). In addition to their strength and conductivity, CNTs demonstrate an impressive ﬂexibility in the structures that may be formed and thus the diversity of types that may be created. CNTs have shown signiﬁcant potential for advances in materials science, electronics, energy production and storage, environmental remediation, and medical applications, particularly drug delivery and bone tissue engineering. Despite their promise, the unique structure of CNTs raises concerns about potential toxicity issues, particularly the potential for respiratory contamination (Malsch and Emond 2014; O’Mathúna 2009). Broader adoption of CNT technology will depend upon further research to examine under what conditions, if any, CNTs pose a serious risk to the health of humans and/or other living organisms.
Nanobiosensors are a nanoscale detection tool for biological or chemical materials. One category of nanobiosensors includes MEMS (microelectromechanical sensors), which have been proposed as a kind of smart dust for military and environmental applications, or even medical monitoring. MEMS also may refer to the broader category of nanotechnologies referred to as microelectromechanical systems, which include sensor technologies. Nanobiosensors or MEMS could be set to wirelessly transmit information allowing for the possibility of real-time detection or tracking capabilities. Other types of nanoscale sensors include nanoﬂuidic arrays and protein nanochips for analysis of chemical materials or even DNA.
MEMS and the broader category of nanobots (or nanorobotics) and nanites or nanoassemblers represent futuristic forms of nanotechnologies. Speculative uses envision nanobots injected into the bloodstream as medical diagnostics, releasing drugs on demand, and performing nanoscale surgeries to repair the body. Self-replicating nanoassemblers are posited as the future of materials science for construction and product manufacturing, disrupting contemporary models through the nanoscale equivalent of 3-D printing. At present, these potential applications remain the purview of speculative nanotechnologies, despite their popular adoption in many science ﬁction works.
CURRENT AND POTENTIAL APPLICATIONS
Given that nanotechnologies are so wide ranging in their impact and applications, what follows is merely a sampling of the various developments in recent years with relevant comments to areas of active inquiry.
INSTRUMENTATION
A number of the early advances due to nanoscience and nanotechnology were actually the result of the development of nanoscale instrumentation. New instruments capable of measurements at the subcellular level have led to expansion in the understanding of fundamental biological processes (Navarro and Planell 2012). Developments in instrumentation include the 1981 invention of the scanning tunneling microscope (STM). STM permitted images of individual atoms 5 nm in size and was the ﬁrst of several scanning probe microscopes signiﬁcantly to improve the power of instrumentation to study materials at the nanoscale level, leading to later developments such as the atomic force microscope (O’Mathúna 2009; ten Have 2007).
MEDICAL AND PHARMACEUTICAL
Nanodelivery systems that utilize encapsulation or coatings with additives allow for increased absorption of pharmaceutical drugs or nutrients such as vitamins, which permits lower dosages and potentially fewer side effects. Due to their ability to pass through cell membranes, carbon nanotubes in particular have demonstrated promise as drug delivery mechanisms (Navarro and Planell 2012; Malsch and Emond 2014). Carbon nanotubes also are generating research interest as potential scaffold material for tissue engineering (Malsch and Emond 2014).
One of the novel properties of nanoparticles, with various medical applications, involves electromagnetic spectrum reactions that result in the vibration or heating of nanomaterials. One such application would be a type of precision tumor surgery involving gold nanoparticles injected directly into a tumor or delivered as part of a drug dosage to enter cancer cells. Several studies have demonstrated the potential for such delivery mechanisms, and when coupled with low-energy laser pulses, the gold nanoparticles caused the cancer cells to explode while leaving surrounding cells unharmed (Evans 2008).
Beyond individual therapeutic interventions, much optimism surrounds the potential of nanotechnology to advance the ﬁeld of regenerative medicine. In the goal to repair, replace, or regenerate damaged tissues, William Haseltine and others argue that nanotechnology will play a pivotal role (Navarro and Planell 2012). The inclusion of nanotechnology is often seen as the ﬁnal phase of regenerative medicine, permitting nanoscale examination and analysis of biological structures as well as bottom-up construction of artiﬁcial organs and tissues. Research in this area is often classiﬁed as “nanobiotechnology” and deﬁned as a ﬁeld of inquiry that “applies the nanoscale principles and techniques to understand and transform biosystems (living or non-living) and which uses biological principles and materials to create new devices and systems integrated from the nanoscale” (Navarro and Planell 2012). In this respect, nanobiotechnology shares similarities in the application of engineering methodologies employed in genetic engineering (or, more recently, gene therapy) and synthetic biology.
AGRICULTURAL AND ENVIRONMENTAL
Agricultural and environmental applications hold the promise of promoting global equity both in food production, sanitation, and availability of clean water. Additives and engineered nanomaterial (ENM) coatings have the potential to serve as delivery mechanisms for fertilizers, pesticides, and veterinary medicine. Furthermore, such coatings could be applied on packing materials (e.g., silver nanoparticles for antimicrobial protection) to increase longevity of food storage and reduce spoilage (Malsch and Emond 2014). Other potential applications include nanobiosensors – sometimes referred to as “labon-a-chip” technology – to detect food spoilage for improved process monitoring or the presence of environmental contaminants. The combination of nanobiosensors with other technologies such as RFID (radio-frequency identiﬁcation technology) to tag materials or other data communication technologies would allow for such technologies to work in nanosensor networks. Such interaction between technologies would permit the nanobiosensors to report the quality of food products throughout the entire distribution process including at the checkout counter or even a refrigerator (Malsch and Emond 2014).
Another prospective application is environmental remediation to remove pollutants. One such example would be the utilization of various nanotechnologies to assist in water puriﬁcation (Evans 2008) or the removal of air pollutants such as carbon dioxide (Malsch and Emond 2014). Particularly promising for water puriﬁcation are nanoﬁltration membranes, nanomagnets, and magnetite nanoparticles (Evans 2008; see also Street et al. 2014; Malsch and Emond 2014). Other environmental applications could include nanobiosensors as air or water monitoring systems to identify contaminants.
ETHICAL DIMENSIONS
Like nanotechnology, nanoethics (or nanotechnology ethics) is a relatively recent development within the realm of applied ethics. Similar to other bioethical subspecialties (e.g., neuroethics, genetic ethics), scholars debate if nanoethics represents the development of a distinct subﬁeld of bioethical inquiry with unique considerations. Some have argued that it is merely a topical specialization that overlaps in its ethical considerations with several other emerging technologies and developments in biotechnology. Robert McGinn, for instance, has responded with skepticism that current evidence does not suggest that nanotechnology raises qualitatively new ethical issues and thus is best regarded as a “subﬁeld of bioethics” (Khan 2012). Similarly, ten Have (2007) and others have noted that only further developments in nanotechnologies will clarify whether nanoethics emerges as a unique “subdiscipline.”
HYPE AND SCIENTIFIC PROMISE
Particularly within Western countries, the current academic research environment has resulted in criticisms of ethics and research practices. One of these criticisms includes the increasing challenges presented by hype in individual research applications or budgetary proposals for entire ﬁelds of inquiry due to the increasingly competitive environment to procure grants and other funding. This activity may take multiple forms, such as exaggerating “a project’s feasibility, likely results or signiﬁcance” with regard to beneﬁts (McGinn 2010). Such hyping of research may also lead to media distortion in coverage of new technologies. While media distortion is not solely the result of hype within the research community, continued “researcher participation in or endorsement of media coverage of scientiﬁc or engineering developments that turns [sic] out to be distorted can dilute public trust and foster public misunderstanding of science and engineering” (McGinn 2010; see also Jotterand 2008; Cameron and Mitchell 2007). Such activities become counterproductive to active public engagement of such complex emerging technologies by impeding ethical considerations in the public deliberation process.
RISK, UNINTENDED CONSEQUENCES, AND THE PRECAUTIONARY PRINCIPLE
While the potential for nanotechnologies is vast, they pose threats similar to that of other realms of biotechnology and emerging technologies (e.g., gene therapy, genetically modiﬁed organisms, synthetic biology, and artiﬁcial life). With each of these ﬁelds, there is a threat potential for catastrophic consequences such as the mass destruction of nature and/or human life (Mitchell et al. 2007). Given the scale in which nanotechnology functions, disasters involving environmental contamination with an impact upon the water and food supply could be particularly damaging and may prove difﬁcult to resolve. While we should not equate science ﬁction with plausible outcomes, such scenarios (as, for instance, Neal Stephenson’s novel Diamond Age) regularly project futures in which environmental nanotechnology pollution has become the norm. Risk assessment for effects on environment, health, and safety (EHS) must be carefully considered and guarded against given the challenges for both detecting and removing nanotechnology materials (Khan 2012).
Potential futuristic applications of nanobots and nanoassemblers have given rise to concerns among even the most ardent supporters of nanotechnologies. The potential of an accident with self-replicating nanotechnologies could lead to something that “could easily be too tough, small, and rapidly spreading to stop” (Drexler 1990). Often referred to as the gray goo problem (or grey goo scenario), widespread environmental disasters resulting from an accident with nanotechnology have generally been “downplayed as an unlikely concern” (Mitchell et al. 2007). The potential for environmental contamination remains, however, and subsequent regulations and policies should be examined that promote due diligence on the part of individuals and corporations before releasing such materials publicly.
Such considerations may entail the need for strategies on containment, detection, and inactivation of nanotechnologies (Mitchell et al. 2007). At the very least, efforts must be made to facilitate open dialogue between governmental bodies, the general public, as well as those individuals and corporations pursuing release of nanotechnology. Such parties need to be actively engaged in discussions of “risk analysis, risk management, and acceptable options for risk transfer” (Cameron and Mitchell 2007). Nanotechnology presents particular challenges for constructing accurate threat matrices due to uncertainty regarding potential toxicity and pollution and for the near future will continue to present particular challenges for risk management and insurers. Clear priority must be given in nanoscience to study these aspects of potential risks, alongside traditional emphasis on discovery. Most models of risk analysis and risk management are based on “evolutionary developments” within a given ﬁeld. In emerging areas such as nanotechnology that mark “revolutionary” or transformative changes in which “potential for damage cannot be assessed,” one must carefully distinguish between those “potential risks related to events attributable to a cause” (i.e., real risks) as opposed to “those whose causality merely cannot be excluded” – so-called phantom risks (Cameron and Mitchell 2007; cf. Jotterand 2008).
Deb Newberry suggests several factors that must be assessed to determine potential harm “to humans, ﬂora, and fauna” (Khan 2012). Those factors include (1) Element Type: the primary compositional elements and their impact or toxicity on living organisms, (2) Object Size: the mass and/or volume of the material that may be present, (3) Total Number of Objects: total amount of the material present, (4) Object Shape: may determine the potential harm to certain organs even if the element type is compatible, (5) Time in System: length of interaction within a living system, (6) Life Cycle and Length of Exposure: life cycle of the material, (7) Method of Entry: how the material enters the life system (e.g., air, ﬂuid, injected, absorbed), and (8) Purity: residual chemicals or artifacts that remain on or in the material that result from the manufacturing process (Khan 2012). Each of these factors may assist in determining the potential harm or beneﬁt that may result from the presence of nanomaterial within a life system provided that the impact of such factors is established in the relevant research.
The importance of thorough knowledge of the effects of nanomaterials on living systems and the relative infancy of this ﬁeld demonstrates the need for caution. Furthermore, given the novel properties that exist at the nanoscale level, previous knowledge of bulk properties or the properties of microor macroscale materials may offer little guidance with respect to potential toxicity or harm (Malsch and Emond 2014). One example would be gold, which at the macro level (e.g., in jewelry) offers very low toxicity, good stability, low reactivity, and well-established properties. In contrast, the properties of gold nanoparticles are still being established, with variable melting temperature and higher reactivity, and thus may pose risks for human health (Khan 2012; Malsch and Emond 2014; O’Mathúna 2009). Given the size of nanoparticles, a potential health concern is the possibility that if they enter the human bloodstream, their size would enable them to cross the blood–brain barrier. This is not just an idle concern as buckyballs have been shown to cause damage in the brains of some aquatic animals (O’Mathúna 2009).
Unintended consequences are often examined with respect to short-term effects, but long-term consequences also may result that are not properly anticipated or thoroughly considered. One such example of an unintended consequence is that nanoparticles may exit the body of animals or humans as waste and be introduced into the environment indirectly. Environmental contamination could result both from direct release of nanoparticles having an unintended consequence or through indirect means such as being introduced as a waste by-product. Indirect means of contamination have important parallels with certain pharmaceutical drugs in which trace amounts have made their way into water supplies. Environmental public health is an emerging ﬁeld that is leading the way in studying the effects of these developments and may play an increasingly important role with the release of rising numbers of products involving nanomaterials. Of course, not all consequences are necessarily harmful, and an unintended consequence of a new material may be beneﬁcial.
In the midst of uncertain risk, the precautionary principle is typically invoked. In its common understanding, this principle “demands the proactive introduction of protective measures in the face of possible risks, which science at present (in the absence of knowledge) can neither conﬁrm nor deny” (Cameron and Mitchell 2007). Care must be taken in the invocation of the precautionary principle so as to avoid stiﬂing technological innovation. However, in the case of revolutionary technologies, the precautionary principle offers a conceptual framework to advance cautiously in their research, development, and commercialization until real risks can be distinguished from phantom risks and such real risks are analyzed and appropriately managed.
ROLE OF TECHNOLOGY ASSESSMENT
Beyon"
3,"4-Computer Forensics Research Paper
The 1s and 0s that make up digital data are incomprehensible to most people. Even though we have heard about them many times, we often have no idea how the 1s and 0s turn into useful data. Forensic analysts must make sense this data and present it to persuade others. This research paper explains how the binary data (1s and 0s) are exactly equal to a slightly easier way of showing the data: hexadecimal. Hexadecimal is commonly seen as the most fundamental representation of the data. Once expressed, the organization of the data becomes easier to understand. Common structures and methods of discovering and explaining other structures are explained and then shown in two examples: carving a lost or deleted file and finding hidden data in a common JPEG photograph. In the first example, a step-by-step set of commands is used to find and recover a deleted picture. The commands demonstrate the same steps used by automated forensic packages. The second example examines data embedded in a JPEG photograph byte by byte. Using publicly available resources, the structures of embedded data are explained. With this information, an analyst’s ability to present data exceeds the ability of commonly used forensic packages. Along with the examples is a discussion of how these operations are handled by automated forensic tools. The whole is brought together in a discussion of how these techniques can improve testimony and the investigative effectiveness of a forensic analyst.
INTRODUCTION
Digital information in its raw form is expressed as ones and zeros, or more correctly as on’s and off’s. Each single piece of information: one or zero is called a bit. This understanding is part of popular culture, but is essentially useless. Almost all meaningful information is gathered into bytes or “words.” With Intel processors, the standard word is 2 bytes or 16 bits. Even though the fundamentals of chip architecture and low-level file structure may also seem useless to the day-to-day business of an investigator or forensic analyst, the structure of data is critical to understanding digital evidence. While the majority of useful evidence comes from active user files, the majority of disk space is unallocated. This means that either direct interpretation of unallocated space forensic recovery or file carving is necessary to access that space. All three of these activities benefit from some knowledge of reading a hexadecimal presentation of the data. The term “hexadecimal presentation” is specifically chosen because it is merely another way to look at the data on the disk and in no way changes the content of the file.
FUNDAMENTALS OF DATA: AN EXPLANATION OF HEXADECIMAL PRESENTATION
Data “presentation” refers to the way data is displayed to a user. A native presentation is the way the file was intended to be displayed to a user. Hexadecimal presentation shows the data in a raw form but summarized into couplets of hexadecimal digits. Hexadecimal digits include the familiar 0–9 of decimal but also include A–F. Where the more familiar decimal repeats its digits every 10 counts, hexadecimal repeats every 16. Since these counting systems use the same characters, it is common to specify which system is being used. Hexadecimal numbers are often followed by “xh” to denote hexadecimal (e.g., 00xh); similarly, decimal notation can be noted with “xd” if it is intermixed with hexadecimal.
There is no mathematical difference between counting in hexadecimal (base 16), decimal (base 10), or even binary (base 2). Each notation can be converted to the other with absolutely no loss of information. For example, 255 (decimal) is exactly equal to FF (hexadecimal) which is exactly equal to 1111 1111 (binary). Internet addresses also demonstrate this fact. 192.168.0.1, a valid IP address, is expressed in decimal-dot notation. It can also be expressed in binary as 1100 0000. 1010 1000.0000 0000.0000 0001. However, it can most efficiently be expressed as C0.A8.0.1 in hexadecimal.
Hexadecimal notation is especially convenient for information based on 8-bit bytes. As can be seen in the IP address, two hexadecimal digits – a hexadecimal couplet – can completely express 8 bits. Rather than presenting eight digits in a row, the system uses two digits, making the information easier to interpret.
The underlying structure of a file is often based on a byte structure. Either an 8-bit word (byte) or some multiple of them is used to hold data. In Windows programming, a 16-bit word is called a WORD, a 32-bit word is called a DWORD, and a 64-bit word is called a QWORD. When reading outside resources about file structure, these terms are often used freely with the assumption that the reader understands them. Resources that do not deal specifically with the Windows Application Programming Interface (WinAPI) may use “word” in a different way.
Even when it only requires one or two digits to account for all possible values, a full byte or even a WORD or DWORD may be used to maintain byte alignment. This means that new fields within the data tend to start on the 0th, 4th, 8th, or 12th byte of a row of 16 bytes. Even when only single bytes are used, a full WORD or DWORD may be filled with padding to maintain byte alignment. Although any value can be used for padding, it is most common to find nulls (00xh). When working with tools that allow a user to change the number of columns displayed in hexadecimal, it can be useful to maintain the standard 16 byte width to allow easy recognition of byte aligned file structures.
BASIC FILE STRUCTURES
File types are structured according to the needs of their creators; thus, there are no universal rules about file structures. However, most file types use a header and body structure. Headers contain information about the file. A file signature value or “magic number” is often used to identify the file type. There is no guarantee that a signature value is unique to a given file type. The Web site wotsit.org maintains lists of file types by extension and gives signatures values when available. Other commonly found fields in the header include file version number, count of records contained in the file, logical size of the file, checksums of file contents, and many others. Header information usually contains only information necessary to interpret the file structure and identify the file type.
The body of the file contains the data used by the application interpreting the file. All of the possible data types cannot be listed here, but there are a few general types. Text-based file types tend to be presented in Unicode or ASCII equivalent. Most hex tools provide an ASCII interpretation to the right of the hex itself. Unicode corresponds to ASCII with Latin characters (the letters used in English). The only difference noticeable is that Unicode has columns of null (00xh) between the letters. Unicode needs the extra bits to encode accented Latin characters and non-Latin characters. However, in English they are not used so the bits are empty, thus null.
The body of a file may also contain blended text and non-text regularly interspersed. This is the hallmark of a file with discrete records. Each record can either follow a precisely established format or contain its own header information. The contents of the record may need to be interpreted or may be readable as text. Repeating record structures make it much more likely that a file fragment will provide useful information. For example, partial Web browser history files can be recovered and read from the hex without recovering an intact file. In many cases, the human eye can spot repeating patterns or legible fragments of files that cannot be identified by automated file carving software. A few regular expression searches (RegEx) and a little knowledge of file structure can produce accurate and precise results.
ENCAPSULATED FILE STRUCTURES: ENCRYPTION, COMPRESSION, AND ENCODING
Sometimes it is not immediately apparent that data is part of a known file type. This may be because the data is encapsulated or layered in other forms of processing. It is common practice in computer science to break a complex task into layers of activity. Each layer is responsible for a discrete task and either passes up or down to the next layer without interacting with the contents. In this way, multiple layers can be imposed on the data found on the disk. Processes that encrypt, compress, or merely convert data can make it impossible to directly analyze the data. The data on disk in no way resembles the original file data when viewed directly, but the original file data can be easily recalled by reversing the encapsulation or layering process. For useful analysis, the data must be converted to a readable format. Automated forensic packages can process this data back through the layers that were imposed on it or it can be done by isolating the file and processing it with a stand-alone tool.
ACCESSING THE DATA
There are many tools available that allow an analyst to access data with a hexadecimal presentation. Rather than try to exhaustively list them, an example of each type can serve to illustrate them. XXD is a utility found on Linux systems that will present a file in hexadecimal. It can also be used in Windows with the Cygwin utility. XXD is called from the command shell (e.g., Bash). Unless the file to be examined is small, it is best to send the output to “less” or a GREP search as follows: XXD/evidence/image.dd | grep ffd8ffe1. This line will search a forensic image for a JPEG signature characteristic of a JPEG picture with EXIF data embedded in it. WinHex is a full featured hex editor that is commercially available. Care should be taken when using hex editors to not write to the file being examined. The interface is familiar to Windows users. Familiar menus and dialog boxes allow the user to open a file in hex presentation. Additional dialogs allow a user to open a partition or physical disk the same way. FTK Imager is a commercial-grade imaging and data access tool provided at no cost by AccessData. Imager can open physical devices (disks, CDs, flash drives, etc.), partitions, or files. A file system hierarchy view similar to Windows Explorer allows a user to traverse a file structure and display data in hexadecimal or in its native presentation. FTK Imager is intentionally incapable of performing searches in the data.
AUTOMATED FORENSIC PACKAGES AND HEXADECIMAL-LEVEL ANALYSIS
Automated forensicpackages allow analysts to rapidly conduct searches across multiple data setsand access archive files (layered data) without intermediate steps. These toolsare not rivals or replacements for lower-level tools; quite the contrary, they includelow-level tools with other functions that add greater convenience for theanalyst. Automated tools can contextualize the data within the file system orwithin search results. Automated packages can also prevent human error found incomplex, repetitive tasks; however, they can mask unique aspects of the datathrough consistent presentation. For example, some forensic software takes a file that was never meant to be viewed by a user and presents it as a table of values or an html presentation (shows it as a Web page). While there is no “native” view of the file, hexadecimal presentation is the most accurate. Ifthe software cannot interpret a field of data, it may simply be skipped in the“user-friendly” presentation. If the user relies on the tool, then there may neverbe a chance to see the skipped data until an opposing expert brings it forward.Most forensic experienced users are aware of the warnings to “know” their toolsand the tool’s limitations, but without a firm basis in interpreting thehexadecimal data, they are left thinking that comparing the output of twoforensic tools works as validation; it does not.
Automated packages can also introduce a bias for intact data that can be cleanly presented by the package. There is no denying that automated packages are necessary to process the massive amounts of data found on even a moderately sized corporate system (very roughly 40 GB). It is self-evident that automated packages are needed for home systems that can run into the multiple terabytes. Automated packages tend toward a clean presentation of native views. With huge amounts of data available, there may be justification in searching visible user files first. As this practice is taught to new examiners and becomes ingrained in experienced examiners or, worst of all, becomes part of policy in a police department or forensics practice, it leads to a bias toward intact data of known types. It is easy to automate searches in EnCase™ with EnScripts™. If policy is in place to exclude unknown file types or search only intact data, large amounts of relevant data can be completely missed. It is easy to avoid this with the full text indexing and searches found in FTK: the Forensic Tool Kit™ by AccessData. However, users soon learn to ignore or even explicitly filter out results from unallocated space or files fragments in favor of those that can be cleanly presented. Users of either package can overcome these limitations, but there is a tendency to become complacent with the tool. By getting in the habit of examining “corrupt” files, unknown file types, and fragmentary data, a forensic examiner builds a mastery of forensic examination that goes beyond document review or tool dependence.
INTERPRETING FILE CONTENT
The two lines in Fig.1 are output from a hex editor. The first line identifies the column (inhexadecimal, there are 16xd columns). The second is the first 16 bytes of aJPEG photograph with EXIF data embedded. The JPEG signature is FF D8 FF E1xh(bytes 0–3). The JPEG type identifier is 45 78 69 66xh (bytes 6–9). This isfollowed by two null bytes: 00 00xh (bytes A–B). The next two bytes identifythe order of following information 4D 4Dxh (bytes C–D). The ASCIIinterpretation of the bytes (4D 4Dxh) as MM indicates that values should beread left to right as read by Motorola chips (called big endian byte order).The other possible value is “II,” indicating that values should be read rightto left as read by Intel chips (called little endian byte order). Much moreinformation can be found in the following lines like the make and model of thecamera used to create this photograph. The GPS coordinates are also recorded inthis particular photograph (not shown). Without an EXIF aware viewer, ananalyst would not know this data was present without viewing it in hexadecimal;the userfriendly presentation shows only the photograph.
 
There are thousands of file types in common use, it would be impossible to catalog them all in this research paper, but technical details can be found on the Web in a few common resources. Microsoft file formats can be found in Microsoft TechNet or in the Microsoft Developer Network (MSDN). To identify an unknown file signature or extension, wotsit.org maintains voluminous catalogs of file types. Details of open-source file types can often be found through their SourceForge projects. Some file types are maintained by work groups comprised of representatives from many companies, academic institutions, governmental bodies, etc. Web searching these resources or simply looking for them in the results of a Web search for the first 4–8 bytes, in hexadecimal, from an unknown file will help find authoritative and reliable sources for file formats.
RECOVERING AND INTERPRETING DATA
Files that have beendeleted or corrupted can be recovered by using the techniques described in thisresearch paper. File carving is the process of recovering a file or fragmentsof a file from unallocated space (where the file system has marked the space asunused). The easiest way to do this is to search for a header signature (FF D8FF E1xh for a JPEG picture) and copy all the data between it and the filefooter signature (FF D9xh for a JPEG) as seen below in the last line of a JPEGfile (Fig. 2).
 
RECOVERING A FILE FROM START TO FINISH
In this example, a JPEG photograph was copied to 128 MB flash drive formatted with the FAT32 file system. An image of the physical drive was created with the Linux/Unix utility “dd” using a SMART boot disk on a host system. SMART is a commercial forensic package created by ASR data, but any forensically sound Linux boot disk will work. The raw, bit-stream copy of the flash memory is analyzed using hexadecimal tools mentioned previously. The purpose of this example is to comment on various file structures along the way to recovering the file. A few pointers on the commands will help in understanding what is happening. All commands are issued in a terminal window in the Ubuntu operating system. Multiple commands can be strung together with a “pipe” character (|). Commands can also have options that change the way they act. Each of the following commands defines an operation in the recovery of a file. These same operations are used by automated packages. The automated packages do not actually implement the Linux/ Ubuntu commands, but they have the same functions.
The first step is to find the file to be recovered. As seen previously, the signature of a JPEG file is FFD8FFxh. Choosing to include the fourth couplet, either E0xh or E1xh, limits the number of files found. Removing it has other interesting effects as will be seen later. “xxd” converts a file from a binary stream to a hexadecimal stream. By refining it with command options, it can be used to extract specific information. “grep” is a Unix environment search tool that has been ported out to many other environments. By sending the output of an entire file in a hexadecimal stream to grep, an analyst can search that stream very efficiently. The image file is named “image_dd.001.”
UBUNTU Command: xxd -g 0 image_dd.001 | grep ffd8ff
Output:
0400400: ffd8ffe000104a464946000101010048…… JFIF…..H
By default xxd lists 16-byte lines with a hexadecimal line number. In this case, the line number is 0400400xh. It is not obvious, but that line number is a good sign for carving a complete file. Each sector is 512xd bytes. 512xd equals 200xh. Thus, any line number evenly divisible by 200xh is the first line of a sector. Most files start on sector boundaries, not in the middle of a sector. The first four bytes are ffd8ffe0xh. This is one of the possible JPEG signatures. These bytes are followed by the remaining 12 bytes in hexadecimal followed by the ASCII conversion of the data. One other consideration is that xxd gives output in a conveniently grouped byte pattern by placing a space between each hexadecimal couplet. “xxd –g 0” removes the grouping and makes the output searchable by grep.
It is necessary to convert this line number to decimal. In Windows, open calculator and choose scientific view. Enter 400400 in hexadecimal and convert to decimal. In a Linux command line, also use a calculator: bc. Echo directs output to stdout (the terminal or a pipe). “bc” is the binary calculator. The following command tells echo to repeat the information “ibase = 16; 400400” to bc. bc interprets the input base (ibase) as hexadecimal (16) and the number as 400400 with no other operations. The default base for bc is 10, so the output base stays 10. The output is the 400400xh converted to decimal. When using bc, remember the default input in hexadecimal is uppercase, but the default output of xxd is lowercase. This will be shown in the next example.
UBUNTU Command: echo “ibase ¼ 16; 400400” | bc
Output:
4195328
Once the header location is known, the footer signature location must be found to locate the end of the file. Line 400400xh was the first to contain a JPEG signature, so all previous lines can be skipped. Once again, xxd and grep can locate this value.
UBUNTU Command: xxd -s 4195328 -g image_dd.001 | grep ffd9
Output:
0406 fc0: 20 86 73 13 7a ee 7f ff d9 00 00 00 00 00 00 00.s.z………..
The line number is 0406fc0xh. The first nine bytes are part of the JPEG file. Bytes number eight and nine are the footer signature value (FF D9xh), marking the end of the file. The trailing nulls (00xh) are file slack space. It cannot be seen from the output above, but there are actually 55 bytes of file slack space before the end of the sector and in this case, the beginning of the next file. Those bytes could contain fragments of previous file(s) stored in that sector. In this case, they contain nulls because the media was securely deleted before use in this example.
Again, it is necessary to convert the line number to decimal, but also to add the 9 bytes found in that line. The direct output of the line number needs to be converted to uppercase or produced from xxd using the “-u” switch to make the hexadecimal digits uppercase. The default output of xxd is lowercase and bc requires uppercase. Since decimal and hexadecimal use the same characters for 0–9, it may be hard to tell that the additional nine bytes are added in hexadecimal.
UBUNTU Command: echo “ibase = 16; 406FC0 + 9” | bc
Output:
4222921
A final calculation will yield the file length. This can also be accomplished with a simple calculator.
UBUNTU Command: echo “4222921 – 4195328” | bc
Output:
27593
With all the file location information available, the file itself can be carved out of the image and exported to a file system. In this case, it is simply exported to the working directory containing the image, but the output can be directed to any mounted file system.
UBUNTU Command: dd if = image_dd.001 of = carved.jpg skip = 4195328 bs = 1 count =27593
Output:
27593 + 0 records in
27593 + 0 records out
27593 bytes (28 kB) copied, 0.515814 s, 53.5 kB/s
There is also a new file called “carved.jpg.” It can be viewed with the xview command or with the tools of the desktop environment. To be clear, there is no difference between the carved file and the original. This can be seen by viewing the JPEG file and seeing the picture. It can also be mathematically confirmed by running an MD5 checksum of the original file and the carved file. In this case, the MD5s match, confirming that not one bit is different.
INTERPRETING INFORMATION IN A JPEG FILE
Aside from the pictureitself, there is often other data embedded in a JPEG file. Two of the manytypes of JPEG file are discussed here. The first is the standard “JFIF” file.This is the type of file seen in the previous example. To comply with the JFIF standard,the first two bytes must be the image marker FF D8xh. The second two bytes arethe application marker FF E0xh. These bytes are the header signature value. Afive-byte identifier follows these: 4A 46 49 46 00xh; the identifier has theASCII values JFIF followed by a null terminator. This can be seen in (Fig. 3).
 
The second type of JPEG file contains additional information embedded within the file. EXIF data is often included with Tagged Image File Format (TIFF) files and JPEG/TIFF files. With photographs, it is most often used to capture camera settings, manufacturer, model, and other details relevant to photographers. As cellular phone cameras have become more common, the data has begun to include other elements like GPS coordinates. EXIF data also includes metadata describing the JPEG file. It is possible to get creation date for the image from the camera rather than from the file system containing the image. Much of this information is typically available through media management software, but automated forensic packages have been slow to update the types of EXIF data offered. This means that without exporting each JPEG file and viewing them in media management software (not forensically sound software), a forensic examiner may miss critical data like the GPS coordinates of where the picture was taken or embedded data possibly used on social media sites. As an alternative, the analyst can learn how to decipher the contents of that data directly.
In the followin gexample, the first 48 bytes of a JPEG picture are used to illustrate how EXIFinformation is stored in TIFF and JPEG files (Fig. 4) (Table 1).
  
The designers of theJPEG file, the Joint Photographic Experts Group, created the JPEG file to beversatile. A number of options were left open for future development orcustomization. Because of these fields could not be anticipated and listed inthe original file specification, some fields were reserved specifically todescribe subsequent field’s structures and locations. Image File Directories(IFDs) may either contain brief data (IFD resident data) or indicate where moreextensive data stores may be found by following a pointer to another locationin the file. The IFD tag is shown in bold in the example above. Each IDF is 12bytes. The table below explains the first two IFDs from the example above. Thefirst field of the IFD identifies it as tag number 271 (10 F in hexadecimalequals 271 in decimal). This value can be found in the TIFF standard on page 35and in a table on page 117. The TIFF standard document indicates that tag 271is the make of the camera or scanner. The second field in the first IFD is thefield type. According the TIFF standard on page 15, type 2 is 7-bit ASCII text.That means that the camera or scanner make will be reported in ASCII text. Thethird field of the first IFD is the length. Themake of the camera will be reported in 8 ASCII characters. The fourth and finalfield of the first IFD is either the data indicated (make of the camera) or apointer to the make of the camera. The last field has four bytes; if the datalength is greater than four, the value of this field is a pointer to where thedata can be found. Counting 158 bytes from the APPO section (not the beginningof the file), there are 8 ASCII bytes that identify the camera’s make:“HTC-8900.” This cannot be seen the sample; the sample data ends before thethird IFD’s field type is shown (Table 2).
 
The second IFD indicates the model of the camera or scanner. With the camera phone that created this photograph, the make and model are identical. The one difference that can be seen in the IFD is the pointer. The previous pointer was 158 bytes and the data occupied 8 bytes; the new pointer is 166 bytes (158 + 8 = 166). The third IFD is the orientation of the picture. The field type indicates that this is a WORD (2 bytes). The length of field indicates that only one unit is used to store the data. Since the field type is WORD and only one unit is used, the data for this IFD occupies one WORD or 2 bytes of data. The data or pointer field will contain data if it is less than four bytes. This IFD has resident data. Only the first two bytes are used, so the remaining two bytes are padded with a null (00xh).
The final IFD in the first group will indicate that there are no more IFDs with a pointer of 00 00 00 00xh or will give a pointer to the next IFD table. The tag type 34665 is reserved to indicate that the IFD does not contain displayable content but either a final entry or a pointer to the next set of entries. Tags higher than 32,768xd are considered private tags. They are typically used by organizations, hardware makers, or software makers for special data to be stored. There are no firm standards on where in the file an IFD can be stored. The presence of unique IFDs at unpredictable locations means that an analyst who is completely reliant on a given tool will likely miss data.
BOTH FINDING AND INTERPRETING DATA IN A JPEG
Another lesson presented by exploring the EXIF data is that pointers within a file indicate the location of data anywhere within the file. EXIF data can be placed anywhere within the Application Use area, as defined by the APPO marker and length. The JPEG file standards document, used to decipher the bytes found in the JPEG header, indicates that a JPEG is a wrapper for content data. Part of that data is one or more pictures, but part of it can be text or some other form of data. The file examined for EXIF data actually contains two images. A search for the image marker: FF D8xh shows that there can be three markers in a JPEG file. There is one marker at the beginning of the file, another at the beginning of a thumbnail, and a third at the beginning of the full-sized image. A fragment with a complete thumbnail may appear to be only a few lines of the greater image because the thumbnail does not display. The thumbnail may provide confirmation that the image was present on the system or it may show enough detail to prove useful. With out some knowledge of the underlying file structure, it is easy to ignore the image as being corrupt or incomplete.
FORENSIC EXAMINATION AND TESTIMONY
It is rare that a forensic examiner making conclusions supported by the data is challenged. The most common scenario in law enforcement forensics is the simple production of documents or images that speak for themselves. The challenge often lies in finding the data and backing the results with properly documented procedure, but not always. The most common scenario for civil forensic examiners is simple preservation of data in a forensically sound manner and production of documents for review by either an attorney or a content expert. Again, the challenge lies in producing a result in a consistent and searchable format with well-documented procedure, but not always. For this type of case, an automated forensic package with consistent results from a process that can be replicated is the tool of choice. They allow large volumes of data to be processed reliably.
For a subset of investigations in both civil and criminal forensics, the analyst must go deeper to understand what has happened to a particular file or in the system itself system. In the example above, the EXIF data contained in the picture includes GPS coordinates of where the picture was taken. The EXIF data in that picture also contains date and time stamps. Such times might differ from the file times that were created when the picture was downloaded from the camera. The EXIF data contains the make and model of the camera which can help match the camera to one seized at a crime scene. All of this information can help make a case, if the investigator is aware of it. In civil forensics, an “expert” may testify that all of a given type of file was produced to the opposing side during discovery. Without the ability to interpret and understand file types that are not known to the automated package used, the “expert” may miss an entire file type with crucial data. Almost worse, the “expert” may not warn the attorney of the presence of such information. An entire legal strategy may fail based on a key fact that was absent to the uninformed side.
Using the data from the JFIF and EXIF examples, it has been shown that files in unallocated space can be identified and recovered in a process known of file carving. By first locating the header signature of a file, then locating the footer signature, and copying all the data in between, an analyst can carve a file from unallocated space. This allows the analyst to demonstrate an understanding of the process used by automated tools that carve files. If segments of the file are stored in more than one location, a state called “fragmented,” an analyst can still derive useful information from EXIF data found in the initial fragment. Garfinkle (2007) published data that estimated 92 % of files found on an NTFS volume were intact or fragmented once. This means that most files can be found and carved intact. The remaining, fragmented, files can still yield useful results. If fragmentation is minimal, then file structure may indicate how to carve and combine the pieces.
CONCLUSION
Understanding the structures of digital files is more than an academic exercise. Analysts who limit themselves to the output of the tool do not bring any special skill to their analysis. Automated tools play an important role in rapidly processing large amounts of information, but they should not be allowed to set boundaries on the evidence. Restricting the data considered to easily displayed files places boundaries on the data. The inability to interpret file fragments places boundaries on the data. Even self-evident results can be successfully challenged by calling into question the process that produced them or the knowledge of the person who chose the process. Sometimes, an opposing expert can even pray on the ignorance of an analyst. The analyst either questions his or her own results or argues beyond his or her knowledge. The best defense to such challenges is a deep and thorough understanding of how the tools work and the structure of the data found as it sits on the disk."
4,"5-Neurotechnology Research Paper
ABSTRACT
Neuroscientiﬁc research has iteratively engaged ever more advanced technology and techniques to achieve an enhanced understanding and capability to access and affect the structure and functions of the brain. Neuroscience and neurotechnology should not be viewed as distinct entities, but rather as a reciprocal enterprise (i.e., what has been referred to as neuroscience and technology, neuroS/T) that is being rapidly and broadly translated for applications and use in medicine, public life, and international relations and global security. NeuroS/T research and its applications give rise to a number of neuroethical issues, questions, and problems. Three core issues and three major questions fostered by neuroS/T are presented, and approaches to addressing and resolving these issues are offered, with a view toward revisiting and perhaps revising extant ethical principles so as to develop a more cosmopolitan, yet community-sensitive and responsive neuroethics that is viable in international contexts.
INTRODUCTION
This essay presents current progress and developments in assessment and interventional neurotechnology. Neuroethical issues arising in and from neurotechnological research and its translation in medicine, public life, and national security are detailed, and key questions generated by these issues are posed. Neuroethics as a ﬁeld and set of practices is deﬁned, and approaches to addressing and resolving neuroethical issues and questions are discussed.
HISTORY AND DEVELOPMENT OF NEUROSCIENCE AND NEUROTECHNOLOGY
Building upon studies of the late nineteenth and twentieth centuries, the past 20 years of neuroscientiﬁc research has iteratively engaged ever more advanced technology and techniques to achieve an enhanced understanding and capability to access and affect the structure and functions of the brain. Fuelled by ongoing governmental and commercial support, contemporary brain science is advancing on an international and multidimensional scale through projects such as the European Union’s Human Brain Project and the Brain Research through Advancing Innovative Neurotechnologies (BRAIN) initiative in the USA. Neuroscience employs “tools-to-theory/ theory-to-tools” heuristics that results in the rapid development of new ideas, concepts, methods, devices, and abilities. Thus, neuroscience and neurotechnology should not be viewed as distinct entities but rather as a reciprocal enterprise (i.e., what has been referred to as neuroscience and technology, neuroS/T) that is being rapidly and broadly translated for applications and use in medicine, public life, and international relations and global security. Such growth is evidenced by an almost linear rise (of more than 70 %) in the number of research and clinical studies reported in the international peer-reviewed literature during the 10-year period from 2000 to 2010 (Giordano 2012a).
CURRENT NEUROS/T
In general, neuroS/T could be deﬁned as those methods and devices that are utilized to assess, access, and/or affect neural systems. These can be broadly categorized as assessment and interventional neuroS/T. Assessment neuroS/T includes different types of neuroimaging: basic and more advanced forms of encephalography, genomic/ genetic and proteomic methods, and neuropsychiatric biomarker assays. Interventional neuroS/T includes neuroand psychotropic agents and novel pharmaceutical methods; transcranial and in-dwelling neuromodulatory devices; peripheral neural stimulators; neural cell, tissue, and genetic implants and transplants; and neuraland brainmachine interfaced neuroprosthetic systems.
Certainly there is much that neuroS/T has achieved, but there is much that remains as yet unaccomplished, in part due to existing constraints and limitations of the tools and technologies themselves. Some of these constraints may be mitigated, if not overcome, through the process of advanced integrative scientiﬁc convergence (AISC) – a synthetic approach that explicitly seeks to foster innovative use of knowledge, skill, and tool sets in order to develop novel means of addressing and solving current impediments to scientiﬁc progress (Vaseashta 2012). This is crucial for progress because the current utility of neuroS/T can be limited by factors including (1) temporal and/or spatial constraints (e.g., as occurs in functional magnetic resonance imaging [fMRI] and forms of electroencephalography [EEG]), (2) difﬁculties of assimilating differing types and levels of data (e.g., from neuroimaging, neurogenetic, and neurological recording studies) to allow statistically relevant and valid comparative and/or normative inferences, (3) a lack of speciﬁcity of action and effect (e.g., as has been noted in transcranial electrical stimulation [tES]), and (4) the size and conﬁgurations of certain in-dwelling devices (e.g., as used in deep brain stimulation [DBS] and intraneural recording). Thus, AISC may generate new opportunities and trajectories for neuroS/T use in both research and practice.
NEUROETHICAL ISSUES
Like so many aspects of human endeavor, neuroS/T is developed and employed to advance knowledge and capability in order to fortify human survival, if not ﬂourishing. Given that ﬂourishing may be deﬁned as strivings for the good(s) of life, it then becomes important to address and examine if and how neuroS/T can, and should (or should not), be employed to obtain such goods. This situates any consideration of neuroS/T research and application squarely within the realm of neuroethics.
As a ﬁeld, neuroethics is devoted to two main tasks: (1) studies of the neurobiological substrates and processes operative in proto-moral, moral, and ethical cognition, emotions, and actions (i.e., the so-called neuroscience of ethics) and (2) the ethical issues fostered by neuroS/T research and uses in medicine, public life, national security, intelligence, and defense (i.e., the “ethics of neuroscience,” Roskies (2002)). Arguably, these are complementary in that any use of neuroS/T must ﬁrst be scrutinized and analyzed to evaluate the validity, viability, probity, problems and value of the techniques, and technologies employed.
In this light, current neuroethical issues arising in and from neuroS/T research and use include:
1.	The validity and relative value of using various forms of neuroS/T assessment (such as neuroimaging, neurogenetics, biomarker evaluation) to deﬁne, describe, and/or predict patterns, of thought, emotionality, and behaviors.
This incurs consequences in medicine and public health (as relates to predicting dispositions to potential neurological and/or psychiatric disorders), law (as regards using neuroS/T to deﬁne capability and culpability), and sociopolitical standards and actions (by using neurocentric criteria to establish norms and then basing social, economic, and politico-legal treatment of individuals and groups upon these deﬁnitions and distinctions).
NeuroS/T-based assessments have been shown to be useful in experimental, and certain clinical and social applications, by illustrating particular functional and dysfunctional parameters of brain structure and activity. However, these approaches also incur a number of potentially controversial issues and questions, based, at least in part, upon the capabilities and limitations of the techniques themselves. For example, neuroimaging technologies and techniques (e.g., positron emission tomography [PET], functional magnetic resonance imaging [fMRI], diffusion tensor imaging [DTI], and diffusion kurtosis imaging [DKI]) tend to have rather good spatial resolution, but less than optimal temporal resolution. In contrast, physiological measures (e.g., quantitative encephalography [qEEG] and magneto encephalography [MEG]) have very good temporal resolution, but generally restricted spatial precision (Glabus 2014). Genomic and genetic assessments may be viable in assessing certain predispositions to patterns of neural structure and function, but direct prediction of resulting phenotypes can be difﬁcult (if not impossible) given the diversity of interacting biological and environmental factors affecting physical expression (Potter 2010).
The utility and applicability of these approaches are credible if and only if the constraints and underlying assumptions (and mis-assumptions) are acknowledged and accounted for in any and all attempts to apply said techniques – especially in situations such as social norming and/or law that are beyond the research or restricted clinical realm. Can scanning the brain be used to “read minds”? Can genes and phenotypes be analyzed to predict future thoughts and actions? No. At least not to the extent depicted in ﬁctional accounts that tend to prompt public fears (Giordano et al. 2014). Are current efforts in neuroS/T on a path to such possibilities? Perhaps, and this fosters something of a paradox: given (a) trends in, and relative speed of, neuroS/T advancement, and (b) recent events of social violence, there is a pervasive – if not increasingly strong – call from the public (as well as certain professional and government sectors) to “do something” and employ imaging and genetic neuroS/T to deﬁne, describe, and predict who will be most likely to commit acts of aggression or violence.
At the same time, there is an equally strong fear that such neuroS/T will be used to probe consciousness and usurp personal privacy and individual autonomy (Giordano et al. 2014). How can a balance be struck between calls for protection and equivocal calls for privacy? Of concern is that various groups, if not society at large, might lose sight of the real capabilities and limitations of such neuroS/T and imprudently employ them in ways beyond their actual current technological capacities. Moreover, the growing availability of, and reliance upon, neuroS/T for assessment might then instigate a more widespread use of neuroS/T for cognitive, emotional, and/or behavioral control (Wurzman and Giordano 2014).
1.	The potential use and misuse of interventional neuroS/T (such as novel drugs, tES, transcranial magnetic stimulation – TMS, DBS, and gene and tissue transplants and implants) to affect fundamental aspects of personality or “the self” as deﬁned in ﬁrst-, second-, or third-person perspectives.
Might neuroS/T be used to change subjective experience by altering mental representations of past and present, so as to affect thought, feelings, and actions in the future? Recent work in tES, TMS, DBS, and optogenetics is impressive and can be seen as taking ardent steps toward altering cognitive functions, emotional reactivity, and behavior. These studies are indeed important, but here too it is equally important to be cautious in parsing fact from hype.
Altering brain activity to change cognitive and emotional states is not a new approach, even in the brain sciences. What is new is the speciﬁcity and precision enabled by state-of-theart neuroS/T. For example, the use of selective pharmacological agents, tES or TMS, and DBS is all currently possible. There is building motivation and impetus to use these forms of neuroS/T to augment cognitive, emotional, and decisional abilities, in this way affect performance in learning and memory and acquisition of knowledge and skill, and alter “talent,” “sensitivity,” and perhaps “morality” – however deﬁned (Savulescu and Bostrom 2009). But what contexts of use will be deemed appropriate, and what – and whose – criteria will provide the metrics of need or value for such use? On the one hand, these applications of neuroS/T can be seen as providing new means – and levels – of individual and/or group improvement. On the other, these same approaches could be leveraged to enforce medicolegal and political standards and norms. In both scenarios, economics will likely inﬂuence how – and to whom – various techniques and technologies would be provided.
This harkens a bit of sci-ﬁ (e.g., the Hollywood ﬁlms Terminal Man, A Clockwork Orange, Eternal Sunshine of the Spotless Mind, and Limitless) and is reﬂective of public sentiment, at least to some extent. Without doubt, there are situations that might justify the use of neuroS/T to alter if not eradicate memories (e.g., traumatic events, profoundly sad or disturbing experiences, etc.), change cognitions and emotions (e.g., depressive disorder, post-traumatic stress syndrome, and other anxiety disorders), and alter behavior (e.g., ﬂorid psychopathy and overt interpersonal aggression and violence), and such use could be seen in a positive, therapeutic light. However, this same capability could also be used in ways that might – and perhaps should – be viewed as far more controversial, such as in interrogations, intelligence operations, and even certain contexts of what is construed to be “public safety” (Giordano et al. 2014).
But assume that the use of neuroS/T could be conﬁned to the clinical milieu; it is still important to consider why, when, in whom, and how particular techniques and technologies will be used, the consequences, and continuity of care. Each and all of these contingencies are vital to obtaining informed consent. As well, it is questionable how “informed” consent can really be, given the nascence of this neuroS/T (Giordano 2015).
Yet, this need not impede further studies and use in practice; as a matter of fact, the only way to fortify determinations of beneﬁts, burdens, risks, and harms is through ongoing research. And animal research alone will be insufﬁcient, given that most of the contestable issues regarding the use of neuroS/T center upon the assessment and control of human mental function. Of course, many potential side and adverse effects will be reduced through the careful design and conduct of any such studies. Still, the fact that this research investigates and employs heretofore unknown interactions between neuroS/T and the brain creates strong probability that unanticipated outcomes can and will occur, and thus, it will be important to address and manage these unintended effects. While this may be relatively easy once a standard of care has been established within single payer and nationally subsidized healthcare systems, it could prove to be problematic within multiple party payer systems (such as that of the USA), and this might necessitate revision of medical economics to accommodate the trend and trajectories of cutting-edge neuroS/T and its translations into clinical contexts (Gunduz et al. 2014).
It is unrealistic to expect that the use of neuroS/ T will be limited to the clinical arena, and at present EEG-based neurofeedback, tES and TMS are being used in extra-clinical settings to affect vigilance, information acquisition and retrieval, arousal/relaxation, and emotionality. What type or extent of interventions constitute treatment (of “abnormalities,” e.g., if said norms are neuroscientiﬁcally deﬁned) versus enhancements (and how far can and should brain functions and human performance be enhanced)? Might there be some “middle ground”– such as socially sanctioned and justiﬁable “enablement” (a term developed by our group) – in which key individuals in select professions (e.g., peace ofﬁcers, ﬁre ﬁghters, physicians, airline pilots, soldiers, etc.) might receive neuroS/T augmentation of those capabilities that are deemed crucial to upholding their social status as protectors of the polis? What professions might be appropriate? And what happens if these individuals can no longer perform such duties or are no longer in such jobs?
Should they then be “disenabled,” or will their altered capabilities render them misﬁt for other aspects of society? How will these “disenabled” individuals be cared for? Who will decide? Who will pay?
1.	Commercialization and global leveraging of neuroS/T.
Recent reports estimate that neuroS/T generates over $150 billion/year in revenues (NeuroInsights 2015). The market pull of neuroS/T that may be publically viewed or construed (via advertising) to be of high value (e.g., for cognitive, emotional, or behavioral alteration) may prompt further penetrance into the consumer sphere with increased potential for inapt use. This could foster issues of commercial liability and indemniﬁcation, which might easily establish default reliance upon Caveat Emptor as an undergirding ethico-legal precept. Moreover, current predictions posit a greater than 60 % increase in neuroS/T research, development, and translational application(s) within the next 10 years, with Asian and South American efforts becoming equal to, if not surpassing, US and European output (NeuroInsights 2015). This establishes brain science as a major economic factor and force affecting power distributions upon the world stage of the forthcoming decade. Therefore, it will become important – and necessary – to consider and acknowledge the needs and values of other, non-Western nations and groups when examining and articulating neuroethical issues and their possible resolution(s).
Such leveraging of brain science is not limited to the portfolios of international economics; it is also crucial to consider the use of neuroS/T in military operations. The formal deﬁnition of a weapon as “a means of contending against another” would warrant more accurate insight to the ways that neuroS/T might be employed to foster improved perception and understanding of factors that spawn cognitive and emotional precipitants of aggression and violence. In an optimistic, but nonetheless pragmatic view, neuroS/T could be employed in such ways to defer conﬂict. But human history and contemporary events prompt reexamination of the viability of weaponizable neuroS/T to incur sickness and death and to negatively impact local, national, and international public health (Wurzman and Giordano 2014).
NEUROETHICAL QUESTIONS
From these issues arise three major – and interrelated – neuroethical questions and debates: First is what we – as individuals, communities, organizations, nations, and perhaps even a species – will do with the information and capability conferred by neuroS/T and what we will do about the capability and information we lack (and by extension, whether we will be insightful and exercise sufﬁcient judgment to know the difference). In other words, will we be (a) sufﬁciently pragmatic in our assessment of neuroS/T to recognize the actual strengths and limitations of these approaches and (b) sufﬁciently prudent in the ways we use and limit the outcomes and products of these approaches to leverage medical, social, economic, legal, and even political effects and power?
Second is whether neuroS/T developed and articulated under current funding initiatives will be translated into clinical care and/or publicly viable – and sound – use within a reasonable window of time or whether these approaches are so nascent as to remain ineffectual in enabling brain science as a broadly applied public good. This is a double-edged sword; while there is a need for concern about not translating neuroS/T research into clinical care, it is equally important to be concerned about exaggerated claims of beneﬁt, as well as underestimations of potentially adverse effects that prompt over expedience in moving novel neuroS/T to the clinical forefront.
Third is how assessment and interventional neuroS/T can, should, and/or should not be used to deﬁne, predict, and change the cognitive, emotional, and behavioral states, conditions, and manifestations that are associated with various brain structures and functions. Of course, this incurs a host of derivative issues, questions, and problems, including if and how neuroS/T-based or neuroS/T-derived information might be used to establish neurocentric criteria for normality and abnormality; what thresholds (of both neuroS/T accuracy and neurobiological function) will be employed to initiate medical, socio-legal, and/or political actions; and if and how various types of neuroS/T will be utilized to execute such actions (by changing thoughts, emotions, beliefs, and actions).
NEUROETHICS IN PRACTICE
A simple precautionary stance is untenable when addressing these questions for two principal reasons: First is the momentum of progress of neuroS/T is such that it will be very difﬁcult to slow or stop. Second is that neuroS/T research, applications, and market viability have become global in articulation and effect, making precautions somewhat relative and proscriptions problematic (Giordano 2012b; 2015; Lanzilao et al. 2013). In other words, neuroS/T research and its application are happening and will continue to happen. However, a “wait until it happens” attitude toward addressing neuroethico legal and social issues created by neuroS/T is cavalier, if not dangerous. Going forward it will be essential to assume more of a preparatory posture, wherein the neuroethical tasks will be to (1) realistically assess the neuroS/T that is available at present and in the near future; (2) model trajectories for use, misuse, and effect; and taking these together (3) develop guidelines and approaches that will be necessary to identify, address, and either prevent problems or resolve them at their inception or early in their development (Giordano 2012b).
To establish insights to those ways that neuroS/ T will generate neuroethico-legal and social issues writ large (i.e., on a community, public, and/or national/international scale), it is necessary to appraise what areas and types of neuroS/T are being subsidized, fortiﬁed, and advanced and to gauge how such technological developments will affect local, regional, national, and global societies, economics, and politics (Swetnam et al. 2013). Determining neuroethical issues, questions, and problems writ somewhat smaller (i.e., on the individual and deﬁned group level) requires a methodological approach to posing key questions framed within deﬁned contingencies and contexts (Giordano 2015).
Developing neuroethical guidelines and providing neuroethical recommendations for policies will require taking sociocultural contingencies and exigencies of various stake and shareholders in neuroS/T into accord. Extant ethical concepts will need to be revisited, and may need to be revised, and in some cases embellished or replaced in light of new and deepening understanding of the brain, new capabilities of neuroS/T, and widening use of these techniques and technologies by individuals and groups with differing needs and values (Lanzilao et al. 2013).
CONCLUSION
As a wok in progress, the ﬁeld of neuroethics will require dedicated efforts toward formation of working groups, ongoing discourse, formulation of methods and protocols, and establishment of standards and guidelines. The proactive work of groups such as the Nufﬁeld Council of Bioethics (in the UK), Presidential Commission for the Study of Bioethical Issues, and Defense Advanced Research Projects Agency’s (DARPA) Neuroscience Ethics’ Legal and Social Issues Advisory Panel (in the USA) is notable and encouraging in its explicit dedication to these issues, both in acknowledgment of neuroS/T progress to date and in advance of major incentivized programs in brain research.
Further efforts along these lines will be vital. Without doubt, neuroS/T can and will affect if not change the human predicament of suffering and pain, will affect the human condition, and may alter the human being – and conceptualizations of what it means to be human or a person. That neuroS/T becomes a salient and powerful reality –and force – that will affect the human prospect of the twenty-ﬁrst century society is inevitable. How this force is to be realized remains to be determined and remains subject to our insight, pragmatism, and prudence."
5,"6-Agriculture and Food Technology Research Paper Topics
1. ACTIVATED CARBON
Activated carbon is made from any substance with a high carbon content, and activation refers to the development of the property of adsorption. Activated carbon is important in purification processes, in which molecules of various contaminants are concentrated on and adhere to the solid surface of the carbon. Through physical adsorption, activated carbon removes taste and odor-causing organic compounds, volatile organic compounds, and many organic compounds that do not undergo biological degradation from the atmosphere and from water, including potable supplies, process streams, and waste streams. The action can be compared to precipitation. Activated carbon is generally nonpolar, and because of this it adsorbs other nonpolar, mainly organic, substances. Extensive porosity (pore volume) and large available internal surface area of the pores are responsible for adsorption. Activated carbon also found wide application in the pharmaceutical, alcoholic beverage, and electroplating industries; in the removal of pesticides and waste of pesticide manufacture; for treatment of wastewater from petroleum refineries and textile factories; and for remediation of polluted groundwater. Although activated carbons are manufactured for specific uses, it is difficult to characterize them quantitatively. As a result, laboratory trials and pilot plant experiments on a specific waste type normally precede installation of activated carbon facilities.
2. BIOLOGICAL PEST CONTROL
Insect outbreaks have plagued crop production throughout human history, but the growth of commercial agriculture since the middle of the nineteenth century has increased their acuteness and brought forth the need to devise efficient methods of insect control. Methods such as the spraying of insecticides, the application of cultural methods, the breeding of insect-resistant plants, and the use of biological control have increasingly been used in the twentieth century. Traditionally limited to checking the populations of insect pests through the release of predatory or parasitic insects, biological control now refers to the regulation of agricultural or forest pests (especially insects, weeds and mammals) using living organisms. It also includes other methods such as the spraying of microbial insecticides, the release of pathogenic microorganisms (fungi, bacteria or viruses), the release of male insects sterilized by radiation, the combination of control methods in integrated pest management programs, and the insertion of toxic genes into plants through genetic engineering techniques. Biological control is also directed against invasive foreign species that threaten ecological biodiversity and landscape esthetics in nonagricultural environments.
3. CROP PROTECTION AND SPRAYING
Humans have controlled agricultural pests, both plants and insects, that infest crops with a variety of biological and technological methods. Modern humans developed spraying pest management techniques that were based on practical solutions to combat fungi, weeds, and insects. Ancient peoples introduced ants to orchards and fields so they could consume caterpillars preying on plants. Chinese, Sumerian, and other early farmers used chemicals such as sulfur, arsenic, and mercury as rudimentary herbicides and insecticides. These chemicals were usually applied to or dusted over roots, stems, or leaves. Seeds were often treated before being sowed. As early as 200 BC, Cato the Censor promoted application of antipest oil sprays to protect plants in the Roman Republic. The nineteenth century potato famine and other catastrophic destruction of economically significant crops including vineyard grapes emphasized the need to improve crop protection measures. People gradually combined technological advances with biological control methods to initiate modern agricultural spraying in the late nineteenth century. Such crop protection technology was crucial in the twentieth century when large-scale commercial agriculture dominated farming to meet global demands for food. Individual farms consisted of hundreds to thousands of acres cultivated in only one or two crop types. As a result, spraying was considered essential to prevent devastating economic losses from pest damage associated with specific crops or locales.
4. DAIRY FARMING
Throughout the world, especially in the Northern Hemisphere, milk, cheese, butter, ice cream, and other dairy products, have been central elements of food production. Over the centuries improvements in cattle breeding and nutrition, as well as new dairy techniques, led to the increased production of dairy goods. Hand-operated churns and separators were used to make butter and cream, and those close to a barnyard had access to fresh milk. By the late nineteenth century, new science and technology had begun to transform dairy production, particularly in the U.S. and Europe. Rail transportation and iced and refrigerated boxcars made it easier to transport milk to more distant markets. Successful machinery for separating milk from cream came from the DeLaval Corporation in 1879, and the Babcock butterfat tester appeared in 1890. The first practical automated milking machines and commercial pasteurization machines were in use in the decades before 1900. Louis Pasteur’s contribution to the dairy industry— discovering the sterilization process for milk— was substantial. By heating milk, pasteurization destroys bacteria that may be harmful to humans. The pasteurization process also increases the shelf life of the product by eliminating enzymes and bacteria that cause milk to spoil. Milk is pasteurized via the ‘‘batch’’ method, in which a jacketed vat is surrounded by heated coils. The vat is agitated while heated, which adds qualities to the product that also make it useful for making ice cream. With the ‘‘continuous’’ method of pasteurization, time and energy are conserved by continuously processing milk as a high temperature using a steel-plated heat exchanger, heated by steam or hot water. Ultra-high temperature pasteurization was first used in 1948.
5. FARMING AND AGRICULTURAL METHODS
Agriculture experienced a transformation in the twentieth century that was vital in increasing food and fiber production for a rising global population. This expansion of production was due to mechanization, the application of science and technology, and the expansion of irrigation. Yet these changes also resulted in the decimation of traditional agricultural systems and an increased reliance on capital, chemicals, water, exploitative labor conditions, and the tides of global marketing. A sign of the transformation of agriculture in the twentieth century was the shift from China and India as countries often devastated by famine to societies that became exporters of food toward the end of the century. As the world’s technological leader, the U.S. was at the vanguard of agricultural change, and Americans in the twentieth century experienced the cheapest food in the history of modern civilization, as witnessed by the epidemic of obesity that emerged in the 1990s. Unfortunately, this abundance sometimes led to overproduction, surplus, and economic crisis on the American farm, which one historian has labeled ‘‘the dread of plenty.’’
6. FARMING AND GROWTH PROMOTION
Early in the twentieth century, most farmers fed livestock simple mixtures of grains, perhaps supplemented with various plant or animal byproducts and salt. A smaller group of scientific agriculturalists fed relatively balanced rations that included proteins, carbohydrates, minerals, and fats. Questions remained, however, concerning the ideal ratio of these components, the digestibility of various feeds, the relationship between protein and energy, and more. The discoveries of various vitamins in the early twentieth century offered clear evidence that proteins, carbohydrates, and fats did not supply all the needs of a growing animal. Additional research demonstrated that trace minerals like iron, copper, calcium, zinc, and manganese are essential tools that build hemoglobin, limit disease, and speed animal growth. Industrially produced nonprotein nitrogenous compounds, especially urea, have also become important feed additives. The rapid expansion of soybean production, especially after 1930, brought additional sources of proteins and amino acids within the reach of many farmers. Meanwhile, wartime and postwar food demands, as well as a substantial interest in the finding industrial uses for farm byproducts, led to the use of wide variety of supplements—oyster shells, molasses, fish parts, alfalfa, cod liver oil, ground phosphates, and more.
7. FARMING MECHANIZATION
Mechanization of agriculture in the twentieth century helped to dramatically increase global production of food and fiber to feed and clothe a burgeoning world population. Among the significant developments in agricultural mechanization in the twentieth century were the introduction of the tractor, various mechanical harvesters and pickers, and labor-saving technologies associated with internal combustion engines, electric motors, and hydraulics. While mechanization increased output and relieved some of the drudgery and hard work of rural life, it also created unintended consequences for rural societies and the natural environment. By decreasing the need for labor, mechanization helped accelerate the population migration from rural to urban areas. For example, in 1790, 90 percent of Americans worked in agriculture, yet by 2000 only about 3 percent of the American workforce was rural. Blessed with great expanses of land and limited labor, technologically inclined Americans dominated the mechanization of agriculture during the twentieth century. Due to mechanization, irrigation, and science, the average American farmer in 1940 fed an estimated ten people, and by 2000 the number was over 100 people. Yet even as mechanization increased the speed of planting and harvesting, reduced labor costs, and increased profits, mechanization also created widespread technological unemployment in the countryside and resulted in huge losses in the rural population.
8. FERTILIZERS
As the twentieth century opened, fertilizers were a prominent concern for farmers, industrialists, scientists, and political leaders. In 1898, British scientist William Crookes delivered a powerful and widely reported speech that warned of a looming ‘‘famine’’ of nitrogenous fertilizers. According to Crookes, rising populations, increased demand for soil-depleting grain products, and the looming exhaustion of sodium nitrate beds in Chile threatened Britain and ‘‘all civilized nations’’ with imminent mass starvation and collapse. Yet Crookes also predicted that chemists would manage to discover new artificial fertilizers to replace natural and organic supplies, a prophecy that turned out to encapsulate the actual history of fertilizers in the twentieth century. In addition to obvious links to increased agricultural production, the modern fertilizer industry has been linked with a number of concerns beyond the farm. For example, the short-lived phosphate boom on the Pacific island of Nauru offers a telling case study of the social consequences and environmental devastation than can accompany extractive industries. Further, much of the nitrogen applied to soils does not reach farm plants; nitrates can infiltrate water supplies in ways that directly threaten human health, or indirectly do so by fostering the growth of bacteria that can choke off natural nutrient cycles. To combat such threats, the European Union Common Agricultural Policy includes restrictions on nitrogen applications, and several nations now offer tax incentives to farmers who employ alternative agricultural schemes. Nevertheless, the rapidly growing global population and its demand for inexpensive food means that artificial fertilizer inputs are likely to continue to increase.
9. FISH FARMING
Controlled production, management, and harvesting of herbivorous and carnivorous fish has benefited from technology designed specifically for aquaculture. For centuries, humans have cultivated fish for dietary and economic benefits. Captive fish farming initially sustained local populations by supplementing wild fish harvests. Since the 1970s, aquaculture became a significant form of commercialized farming because wild fish populations declined due to overfishing and habitat deterioration. Growing human populations increased demand for reliable, consistent sources of fish suitable for consumption available throughout the year. Fish farming technology can be problematic. If genetically engineered fish escape and mate with wild fish, the offspring might be unable to survive. Cultivated fish live in crowded tanks that sometimes cause suffocation, diseases, and immense amounts of waste and pollutants. Antibiotic use can sometimes result in resistant microorganisms. Coastal fish farms, especially those for shrimp, can be environmentally damaging if adjacent forests are razed.
10. FOODS ADDITIVES AND SUBSTITUTES
Advances in food and agricultural technology have improved food safety and availability. Food technology includes techniques to preserve food and develop new products. Substances to preserve and enhance the appeal of foods are called food additives, and colorings fit into this category of additives that are intentionally included in a processed food. All coloring agents must be proven to be safe and their use in terms of permitted quantity, type of food that can have enhanced coloring, and final level is carefully controlled. Fat substitutes on the other hand are technically known as replacers in that they replace the saturated and/or unsaturated fats that would normally be found in processed food as an ingredient or that would be added in formulation of a processed food. Usually the purpose is to improve the perceived health benefit of the particular food substance. Technically speaking, substitutes are not additives but their efficacy and safety must be demonstrated.
11. FOOD PREPARATION AND COOKING
Twentieth century technological developments for preparing and cooking food consisted of both objects and techniques. Food engineers’ primary objectives were to make kitchens more convenient and to reduce time and labor needed to produce meals. A variety of electric appliances were invented or their designs improved to supplement hand tools such as peelers, egg beaters, and grinders. By the close of the twentieth century, technological advancements transformed kitchens, the nucleus of many homes, into sophisticated centers of microchip-controlled devices. Cooking underwent a transition from being performed mainly for subsistence to often being an enjoyable hobby for many people. Kitchen technology altered people’s lives. The nineteenth-century Industrial Revolution had initiated the mechanization of homes. Cooks began to use precise measurements and temperatures to cook. Many people eagerly added gadgets to their kitchens, ranging from warming plates and toasters to tabletop cookers. Some architects designed kitchens with built-in cabinets, shelves, and convenient outlets to encourage appliance use. Because they usually cooked, women were the most directly affected by mechanical kitchen innovations. Their domestic roles were redefined as cooking required less time and was often accommodated by such amenities as built-in sinks and dishwashers. Ironically, machines often resulted in women receiving more demands to cook for events and activities because people no longer considered cooking to be an overwhelming chore.
12. FOOD PRESERVATION BY COOLING AND FREEZING
People have long recognized the benefits of cooling and freezing perishable foods to preserve them and prevent spoilage and deterioration. These cold storage techniques, which impede bacterial activity, are popular means to protect food and enhance food safety and hygiene. The food industry has benefited from chilled food technology advancements during the twentieth century based on earlier observations. For several centuries, humans realized that evaporating salt water removed heat from substances. As a result, food was cooled by placing it in brine. Cold storage in ice- or snow-packed spaces such as cellars and ice houses foreshadowed the invention of refrigerators and freezers. Before mechanical refrigeration became consistent, freezing was the preferred food preservation technique because ice inhibited microorganisms. Freezing technology advanced to preserve food more efficiently with several processes. Blast freezing uses high-velocity air to freeze food for several hours in a tunnel. Refrigerated plates press and freeze food for thirty to ninety minutes in plate freezing. Belt freezing quickly freezes food in five minutes with air forced through a mesh belt. Cryogenic freezing involves liquid nitrogen or Freon absorbing food heat during several seconds of immersion.
13. FOOD PRESERVATION BY FREEZE DRYING, IRRADIATION, AND VACUUM PACKING
Humans have used processes associated with freeze-drying for centuries by placing foods at cooler high altitudes with low atmospheric pressure where water content is naturally vaporized. Also called lyophilization, freeze-drying involves moisture being removed from objects through sublimation. Modern freeze-drying techniques dehydrate frozen foods in vacuum chambers, which apply low pressure and cause vaporization. Irradiation is less successful than freeze-drying. Prior to irradiation, millions of people worldwide became ill annually due to contaminated foods with several thousand being hospitalized or dying due to food-borne pathogens. By exposing food to an electron beam, irradiation enhances food safety. Irradiated human and animal feed, especially grain, can be transported over distances and stored for a long duration without spoiling or posing contamination hazards. The radura is the international food packaging symbol for irradiation. Vacuum-packing food technologies involve a process that removes empty spaces around foods being packaged. Vacuum technology uses environments artificially modified to have atmospheric pressures that are lower than natural conditions. Vacuum packing extends the shelf life of food. The U.K. Advisory Committee on the Microbiological Safety of Foods warned that anaerobic pathogens such as C. botulinum can grow in vacuum-packed foods. Because vacuum packing often results in rubbery sliced cheese, some manufacturers use the modified atmosphere packaging (MAP) system, which utilizes gases to fill spaces so that cheese can mature to become tastier inside packaging.
14. IRRIGATION SYSTEMS
Since the onset of human civilization, the manipulation of water through irrigation systems has allowed for the creation of agricultural bounty and the presence of ornamental landscaping, often in the most arid regions of the planet. These systems have undergone a widespread transformation during the twentieth century with the introduction of massive dams, canals, aqueducts, and new water delivery technology. In 1900 there were approximately 480,000 square kilometers of land under irrigation; by 2000 that total had surged to 2,710,000 square kilometers, with India and China as the world leaders in irrigated acreage. Globally, the agriculture industry uses about 69 percent of the available fresh water supplies, producing 40 percent of the world’s food on just about 18 percent of the world’s cropland. (It takes 1000 tons of water to produce 1 ton of grain.) New technologies to monitor evaporation, plant transpiration, and soil moisture levels have helped increase the efficiency of irrigation systems. The US is the world leader in irrigation technology, exporting upward of $800 million of irrigation equipment to the rest of the world each year, with the sales of drip irrigation equipment increasing 15 to 20 percent per annum in the 1990s. Golf course and landscape irrigation are also an increasing part of the irrigation technology market. Intense competition for water from cities and for environmental restoration projects might mean a reduction in irrigated agriculture in future years. At the same time, salinization of fields, infiltration of aquifers by sea water, and depleted water availability could lead to a reduction in land under irrigation worldwide.
15. NITROGEN FIXATION
In 1898, the British scientist William Crookes in his presidential address to the British Association for the Advancement of Science warned of an impending fertilizer crisis. The answer lay in the fixation of atmospheric nitrogen. Around 1900, industrial fixation with calcium carbide to produce cyanamide, the process of the German chemists Nikodemus Caro and Adolf Frank, was introduced. This process relied on inexpensive hydroelectricity, which is why the American Cyanamid Company was set up at Ontario, Canada, in 1907 to exploit the power of Niagara Falls. Electrochemical fixing of nitrogen as its monoxide was first realized in Norway, with the electric arc process of Kristian Birkeland and Samuel Eyde in 1903. The nitrogen monoxide formed nitrogen dioxide, which reacted with water to give nitric acid, which was then converted into the fertilizer calcium nitrate. The yield was low, and as with the Caro–Frank process, the method could be worked commercially only because of the availability of hydroelectricity.
16. PESTICIDES
A pesticide is any chemical designed to kill pests and includes the categories of herbicide, insecticide, fungicide, avicide, and rodenticide. Individuals, governments, and private organizations used pesticides in the twentieth century, but chemical control has been especially widespread in agriculture as farmers around the world attempted to reduce crop and livestock losses due to pest infestations, thereby maximizing returns on their investment in seed, fuel, labor, machinery expenses, animals, and land. Until the twentieth century, cultural pest control practices were more popular than chemicals. Cultural methods meant that farmers killed pests by destroying infested plant material in the fields, trapping, practicing crop rotation, cultivating, drying harvested crops, planting different crop varieties, and numerous other techniques. In the twentieth century, new chemical formulations and application equipment were the products of the growth in large-scale agriculture that simultaneously enabled that growth. Large scale and specialized farming provided ideal feeding grounds for harmful insects. Notable early efforts in insect control began in the orchards and vineyards of California. Without annual crop rotations, growers needed additional insect control techniques to prevent build-ups of pest populations. As the scale of fruit and nut production increased in the early decades of the century, so too did the insect problem.
17. PROCESSED AND FAST FOOD
Convenience, uniformity, predictability, affordability, and accessibility characterized twentieth-century processed and fast foods. Technology made mass-produced fast food possible by automating agricultural production and food processing. Globally, fast food provided a service for busy people who lacked time to buy groceries and cook their meals or could not afford the costs and time associated with eating traditional restaurant fare. As early as the nineteenth century, some cafeterias and restaurants, foreshadowing fast-food franchises, offered patrons self-service opportunities to select cooked and raw foods, such as meats and salads, from displays. Many modern cafeterias are affiliated with schools, businesses, and clubs to provide quick, cheap meals, often using processed foods and condiments, for students, employees, and members. Food-processing technology is designed primarily to standardize the food industry and produce food that is more flavorful and palatable for consumers and manageable and inexpensive for restaurant personnel. Food technologists develop better devices to improve the processing of food from slaughter or harvesting to presentation to diners. They are concerned with making food edible while extending the time period it can be consumed. Flavor, texture, and temperature retention of these foods when they are prepared for consumers are also sought in these processes. Microwave and radio frequency ovens process food quickly, consistently, and affordably. Microwaves are used to precook meats before they are frozen for later frying in fast-food restaurants. Nitrogen-based freezing systems have proven useful to process seafood, particularly shrimp. Mechanical and cryogenic systems also are used. The dehydrating and sterilizing of foods remove contaminants and make them easier to package. Heating and thawing eliminate bacteria to meet health codes. These processes are limited by associated expenses and occasional damage to foods. Processing techniques have been adapted to produce a greater variety of products from basic foods and have been automated to make production and packaging, such as mixing and bottling, efficient enough to meet consumer demand.
18. SYNTHETIC FOODS, MYCOPROTEIN AND HYDROGENATED FATS
Food technologists developed synthetic foods to meet specific nutritional and cultural demands. Also referred to as artificial foods, synthetic foods are meat-free and are designed to provide essential fiber and nutrients such as proteins found in meats while having low saturated fat and lacking animal fat and cholesterol. These foodstuffs are manufactured completely from organic material. They have been manipulated to be tasty, nutritionally sound with major vitamins and minerals, have appealing textures, and safe for consumption. Synthetic foods offer people healthy dietary choices, variety, and convenience. Mycoprotein is created from Fusarium venenatum (also known as Fusarium graminearum), a small edible fungi related to mushrooms and truffles that was initially found in the soil of a pasture outside Marlow in Buckinghamshire, England. Concerned about possible food shortages such as those experienced in World War II Europe; as global populations swelled postwar, scientists began investigating possible applications for this organism as a widely available, affordable protein source. Scientists at one of Britain’s leading food manufacturers, Rank Hovis McDougall, focused on mycoprotein from 1964. At first, they were unable to cultivate fungus to produce mycoprotein in sufficient quantities for the envisioned scale of food production. Food technologists devoted several years to establishing procedures for growing desired amounts of mycoprotein. They chose a fermentation process involving microorganisms, somewhat like those historically used to create yogurt, wine, and beer. Food technologists create hydrogenated fats by processing vegetable oils, consisting of glycerides and fatty acids, with chemicals to achieve certain degrees of hardening. Partial hydrogenation stiffens oils, while full hydrogenation converts liquid oils into solid fat. The hydrogenation process involves moving hydrogen gas through heated oils in vats containing metals, usually copper, nickel, or zinc. When the metal reacts to the gas, it acts as a catalyst to relocate hydrogen molecules in the oil to create different, stiffer molecular shapes. This chemical reaction creates trans fats. Saturation of fats in these synthetic molecules increases according to the degree of hydrogenation achieved.
19. TRANSPORTATION OF FOODSTUFFS
Twentieth century foodstuffs were transported by land on vehicles and trains, by air on cargo planes, and by water on ships or barges. Based on innovations used in previous centuries, engineers developed agricultural technology such as refrigerated containers to ship perishable goods to distant markets. Technological advancements enabled food transportation to occur between countries and continents. International agreements outlined acceptable transportation modes and methods for shipping perishables. Such long-distance food transportation allowed people in different regions of the world to gain access to foodstuffs previously unavailable and incorporate new products they liked into their diets. Refrigerated trailers dominate road food transportation methods. This transportation mode minimizes food vulnerability to shipment damage from being harvested to placement on grocery shelves. Refrigerated transport enables fresh produce from milder climates to be shipped out-of-season to colder locations. Refrigeration is achieved by mechanical or cryogenic refrigeration or by packing or covering foods in ice. Ventilation keeps produce cool by absorbing heat created by food respiration and transferred through the walls and floor from the external air beneath and around the shipping trailer. Food technologists design packaging materials for food transportation. Most produce is shipped in corrugated and fiberboard cardboard boxes that are sometimes coated with wax. Wooden and wire-bound crates are also used in addition to bushel hampers and bins. Mesh plastic, burlap, and paper bags hold produce. Meat is often vacuum packed on plastic trays that are placed in wooden lugs. Foods are occasionally wrapped in plastic liners or packed in ice to withstand damage in transit and limit evaporation.
 AGRICULTURE AND FOOD TECHNOLOGY
In late-twentieth century Western societies, food was available in abundance. Shops and supermarkets offered a wide choice in products and brands. The fast-food industry had outlets in every neighborhood and village. For those in search of something more exclusive, there were smart restaurants and classy catering services. People chose what they ate and drank with little awareness of the sources or processes involved as long as the food was tasty, nutritious, safe, and sufficient for everyone. These conditions have not always been met over the last century when food shortages caused by economic crises, drought, or armed conflicts and war, occurred in various places. During the second half of the twentieth century, food deficiency was a feature of countries outside the Western world, especially in Africa. The twentieth century also witnessed a different sort of food crisis in the form of a widespread concern over the quality and safety of food that mainly resulted from major changes in production processes, products, composition, or preferences.
Technology plays a key role in both types of crises, as both cause and cure, and it is the character of technological development in food and agriculture that will be discussed. The first section examines the roots of technological developments of modern times. The second is an overview of three patterns of agricultural technology. The final two sections cover developments according to geographical differences.
Before we can assess technological developments in agriculture and food, we must define the terms and concepts. A very broad description of agriculture is the manipulation of plants and animals in a way that is functional to a wide range of societal needs. Manipulation hints at technology in a broad sense; covering knowledge, skills, and tools applied for production and consumption of (parts or extractions of) plants and animals. Societal needs include the basic human need for food. Many agricultural products are food products or end up as such. However, crops such as rubber or flax and animals raised for their skin are only a few examples of agricultural products that do not end up in the food chain. Conversely, not all food stems from agricultural production. Some food is collected directly from natural sources, like fish, and there are borderline cases such as beekeeping. Some food products and many food ingredients are artificially made through complicated biochemical processes. This relates to a narrow segment of technology, namely science-based food technology.
Both broad and narrow descriptions of agriculture are relevant to consider. In sugar production for example, from the cultivation of cane or beets to the extraction of sugar crystals, both traditional and science-based technologies are applied. Moreover, chemical research and development resulted in sugar replacements such as saccharin and aspartame. Consequently, a randomly chosen soft drink might consist of only water, artificial sweeteners, artificial colorings and flavorings, and although no agriculture is needed to produce such products, there is still a relationship to it. One can imagine that a structural replacement of sugar by artificial sweeteners will affect world sugar prices and therewith the income of cane and beet sugar producers. S"
6,"7-Energy and Power Technology Research Paper Topics
1. BIOMASS POWER GENERATION
Biomass, or biofuels, are essentially clean fuels in that they contain no sulfur and the burning of them does not increase the long-term carbon dioxide (CO2) levels in the atmosphere, since they are the product of recent photosynthesis (note that peat is not a biofuel in this sense). This is by no means an unimportant attribute when seen in the context of the growing awareness across the globe of the pollution and environmental problems caused by current energy production methods, and the demand for renewable energy technologies.
Biomass can be used to provide heat, make fuels, and generate electricity. The major sources of biomass include:
Standing forests,Wood-bark and logging residues,Crop residues,Short rotation coppice timber or plants,Wood-bark mill residues,Manures from confined livestock,Agricultural process residues,Seaweed,Freshwater weed,Algae
A few facts and figures might help to put the land-based biomass sources in perspective. The first three of the above list produce in the U.S. approximately the equivalent of 4 million barrels of oil per day in usable form. If all crop residues were collected and utilized to the full, almost 10 percent of the total U.S. energy consumption could be provided for. Although the other land-based sources of biomass are perhaps not on the same scale as this, the combined resource represents a huge untapped reservoir of potential energy. An interesting point to note is that current practices in forestry and food crop farming are aimed directly at optimizing the production of specific parts of a plant. Since biomass used for energy would make use of the whole plant, some significant advantage might be gained by growing specifically adapted crops designed to maximize the energy yield rate. It is from this origin that the energy farm concept is born.
2. EARLY FUSION NUCLEAR REACTORS
The production of nuclear energy through the fusion of two light chemical elements is better known as a controlled thermonuclear reaction (CTR). In the 1950s, explosive or uncontrolled thermonuclear reaction was achieved with the manufacture of hydrogen bombs, but CTR was never successfully accomplished.
In order to reach the fusion point, a gaseous mixture containing deuterium and tritium should be heated to 100,000,000C and hold that temperature for enough time to activate a self-sustaining reaction. At elevated temperatures, a gaseous mixture becomes plasma, a state in which electrons and ions are no longer physically bonded. (The term plasma was first used in 1922 by the American physical chemist Irving Langmuir because the properties of a super-heated gas reminded him of blood plasma.)
Heating and confinement of plasma are the two main features of any fusion reactor. Plasma must avoid any contact with the walls of the vessel containing it in order to avoid the loss of temperature and subsequent instability that makes a controlled thermonuclear reaction impossible to achieve. Early designs of fusion reactors focused on confinement of plasma using magnetic fields.
3. ELECTRICAL POWER DISTRIBUTION
While the first commercial power station in San Francisco in 1879 was used for arc lighting (using a spark jumping a gap as the source of light) for street lamps, these had limited application. Edison’s carbon filament lamp was the stimulus for the spread of electric lighting. A few of Edison’s buildings and some private residences had their own generators, but Edison also recognized there was a need for a generating and distribution system. Edison’s distribution system was first demonstrated in London, with a temporary installation running cables under the Holburn Viaduct in early 1882 that provided power for the surrounding district. The first permanent central electric generating station was Edison’s Pearl Street Station in New York that went into operation in September 1882 and provided electricity (with a meter) to 85 customers in a 1 square mile (2.6 square kilometers) area. The Pearl Street Station used direct current (DC). In DC systems, the current flows in one direction, with a constant voltage. The dissipation of energy limits the size of DC systems and requires the source of electric generation to be close to the customer. Alternating current (AC) systems, in which the current changes direction (in today’s public electricity supply, 50 or 60 times per second), overcame this limitation.
4. ELECTRICITY GENERATION AND THE ENVIRONMENT
Fossil fuel thermal generating technologies were a mainstay of both twentieth century electricity generation and environmental attention. While concern with declining urban air quality, initially at the center of this attention, dated back to the nineteenth century, it was the substantial post- World War II rise in electricity consumption that resulted in the later prominence of these concerns. The impacts of fossil fuel extraction and transportation were also a source of significant twentieth century environmental attention, but concern over atmospheric emissions dominated. Although initial concern focused on particulate emissions, attention shifted to acidic emissions from the 1970s onward, and the final decade of the century was dominated by concern with the impact of fossil fuel emissions on climate. This later concern with fossil fuel greenhouse gas emissions, primarily thermally produced carbon dioxide (CO2) but also fugitive emissions (i.e., not caught by a capture system) such as methane from coal seams and from gas extraction and distribution systems, reinforced an increasing emphasis on alternative generating technologies. Some of these, notably macro-hydro and nuclear fission, were significant twentieth century technologies in their own right, and their environmental impacts are briefly discussed below. However, as the twentieth century closed this emphasis was increasingly turning to renewable energy technologies and the potential for significant further efficiencies in both electricity generation and consumption, including the drive to ‘‘decarbonize’’ electricity generation by turning away from fossil fuel technologies.
5. FAST BREEDERS NUCLEAR REACTORS
The idea of a fast breeder reactor (FBR) was first conceived in 1946 by the Canadian physicist Walter H. Zinn at the Argonne National Laboratory in the U.S. On the basis of wartime developments in nuclear reactor research, Zinn thought a combination of two options within reactor technologies was feasible: fast neutron nuclear fission and the breeding principle. Fast reactors produce nuclear fission with fast neutrons rather than thermal neutrons. Fast neutrons prompt critical reactions with a large energy release in a short time and without a moderator operating in the core. Breeder reactors have a core of fissile material (i.e., uranium-235 or plutonium-239) produced in nuclear reactors or through chemical separation, and a blanket of fertile material (i.e., uranium-238), the treated radioactive mineral. Once in operation, breeder reactors incinerate the fissile material in the core and emit neutrons as fission products. Thus the blanket is neutron bombarded, the fertile material is irradiated, and afterward transformed into fissile material (i.e., plutonium-239) by neutron capture and following decay. In optimal operation conditions, the fissile material produced through breeding equals the fissile material incinerated in the core, so that the reactor perpetuates indefinitely the production of its fuel.
6. FOSSIL FUEL POWER STATIONS
Until the last third of the twentieth century, fossil fuels—coal, oil, natural gas—were the primary source of energy in the industrialized world. Large thermal power stations supplied from fossil fuel resources have capacities ranging up to 4000 to 5000 megawatts. Gas-fired combined-cycle power stations tend to be somewhat smaller, perhaps no larger than 1000 to 1500 megawatts in capacity.
Concerns in the 1970s over degradation of urban air quality due to particulate emissions and acid rain from sulfur dioxide emissions from fossil fuel power stations were joined from the 1990s by an awareness of the potential global warming effect of greenhouse gases such as carbon dioxide (CO2), produced from the combustion of fossil fuels (see Electricity Generation and the Environment). However, despite a move towards carbon-free electricity generation, for example from nuclear power stations and wind and solar plants, fossil fuels remain the most significant source of electrical energy generation.
7. FUEL CELLS
The principle of the fuel cell (FC) is similar to that of the electrical storage battery. However, whereas the battery has a fixed stock of chemical reactants and can ‘‘run down,’’ the fuel cell is continuously supplied (from a separate tank) with a stream of oxidizer and fuel from which it generates electricity. Electrolysis—in which passage of an electrical current through water decomposes it into its constituents, H2 and O2—was still novel in 1839 when a young Welsh lawyer–scientist, William Grove, demonstrated that it could be made to run in reverse. That is, if the H2 and O2 were not driven off but rather allowed to recombine in the presence of the electrodes, the result was water— and electrical current.
Over the 20th century FCs moved from laboratory curiosity to practical application in limited roles and quantities. It is very possible that the twenty-first century will see them assume a major or even dominant position as power sources in a broad array of applications. Obstacles are largely economic and the outcome will be influenced by success in development of competing systems as well as FCs themselves.
8. GAS TURBINES
During the 20th century, the gas turbine was developed to fit many applications on land, sea and in the air. From early beginnings, the gas turbine came alongside, competed with, and often replaced the existing technologies of steam, water, and reciprocating internal combustion engines. Initial problems stemmed from a lack of knowledge and the techniques; the fundamentals were well enough understood, but what were lacking were the design techniques. Materials also held up developments; but after extensive experimentation, successful turbine designs were being constructed in the first ten years of the 20th century.
The gas turbine has the advantage over traditional engines in that its combustion process is continuous and thus the equipment is less subject to cyclic heat stresses and its power is less limited— power is limited by combustion knock in spark ignition engines, but in diesel engines it is only limited by structural strength and maximum working pressures in the fuel injection systems. It also has fewer moving parts, so wear and tear is lessened. Despite these differences, the gas turbine still has the basic four functions of the four-stroke cycle but operates continuously: air is admitted, compressed, heated by burning fuel so that it expands and does work, and then the spent gases are expelled. However, unlike an ordinary engine, each of these processes takes place in a separate part of the engine and happens continuously; the oil engine has all processes within the cylinder and they follow on from each other.
9. GAS TURBINES IN LAND VEHICLES
The gas turbine has found widespread use in the aviation, marine, and stationary power areas. However, the gas turbine has only seen limited use in land transportation.
As various companies began to experiment with gas turbines in the 1920s and 1930s some gave thought to using the turbine as a source of motive power for land vehicles. The turbine promised much higher power-to-weight ratios than conventional reciprocating engines and also had the capability of using cheaper fuels such as industrial heating oil, diesel fuel, and even powered coal. As with gas turbines in aviation, most development has occurred since World War II.
10. HYDROELECTRIC POWER GENERATION
It is estimated that about 50 percent of the economically exploitable hydroelectric resources, not including tidal resources, of North America and Western Europe have already been developed. Worldwide, however, the proportion is less than 15 percent.
The size of hydroelectric power plants covers an extremely wide range, from small plants of a few megawatts to large schemes such as Kariba in Zimbabwe, which comprises eight 125 megawatt generating sets. More recently, power stations such as Itaipu on the Parana River between Brazil and Paraguay in South America were built with a capacity of 12,600 megawatts, comprising eighteen generating sets each having a rated discharge of approximately 700 cubic meters per second.
Hydroelectric power has traditionally been regarded as an attractive option for power generation since fuel costs are zero; operating and maintenance costs are low; and plants have a long life—an economic life of 30 to 50 years for mechanical and electrical plant and 60 to 100 years for civil works is not unusual.
11. LARGE SCALE ELECTRICAL ENERGY GENERATION AND SUPPLY
Public supply of electricity at the close of the nineteenth century was typically confined to the larger towns and cities where either a local entrepreneur, or a far-sighted municipality, established relatively small generating stations to supply local lighting loads. Many of these local power stations employed reciprocating engines to drive direct current (DC) dynamos. Overhead circuits generally carried the power no more than a kilometer or two to local businesses or the larger households in the district. Sometimes, where water-powered mills had existed previously, hydroelectric generators were established to supply the electricity consumers. As more and more people began to appreciate the convenience of electrical power and, moreover, could afford to pay for it, demand on local supplies increased and larger power stations began to be established. The invention of the electrical transformer to step-up the voltage at the generating station and step it down again to a safe level for use by the consumers, meant that higher speed alternators, often driven by steam turbines, could be employed to produce the power. High-voltage distribution reduced the losses in the circuits between the generating stations and the loads.
12. LATER FUSION NUCLEAR REACTORS
In the early 1950s, the Soviet physicists Andrei Sakharov and Igor Tamm proposed a reactor that generated both internal plasma and external toroidal magnetic fields. This concept was adopted by their colleague Lev Artsimovich in his T-3 reactor, the first ‘‘tokamak’’ (the Russian acronym for toroidal chamber and magnetic coil), unveiled in 1968. The tokamak magnetic field is thus the combination of two magnetic fields: the stronger horizontal, toroidal field interacts with the weaker vertical, poloidal plasma field to produce a helical magnetic field. In confining its plasma for 0.01 to 0.02 seconds and heating it to 10,000,000C, the T- 3 produced results that suggested fusion energy was feasible.
The tokamak reactor subsequently became the standard tool for fusion research. The energy crisis of the 1970s resulted in state support for major projects in a number of industrialized countries including France, Japan, the U.K., and the U.S. The largest and most notable were the American Tokamak Fusion Test Reactor (TFTR), approved by the Atomic Energy Commission in 1974 and completed in 1982 at Princeton University, and the British–European Joint European Torus (JET), which began operations in 1983 in Culham, Oxfordshire, U.K. Other important tokamaks include Japan’s JT-60 and General Atomics’ DIII-D.
13. POWER GENERATION AND RECYCLING
Recovering energy from wastes from municipal or industrial sources can turn the problem of waste disposal into an opportunity for generating income from heat and power sales. The safe and cost-effective disposal of these wastes is becoming increasingly important worldwide, especially with the demand for higher environmental standards of waste disposal and the pressure on municipalities to minimize the quantities of waste generated that must be disposed.
14. PRIMARY AND SECONDARY BATTERIES
The battery is a device that converts chemical energy into electrical energy and generally consists of two or more connected cells. A cell consists of two electrodes, one positive and one negative, and an electrolyte that works chemically on the electrodes by functioning as a conductor transferring electrons between the electrodes.
Primary cells, most often ‘‘dry cells,’’ are exhausted (i.e., one or both of the electrodes are consumed) when they convert the chemical energy into electrical energy. These battery types are widely used in flashlights and similar devices. They generally contain carbon and zinc electrodes and an electrolyte solution of ammonium chloride and zinc chloride. Another form of primary cell, often called the mercury battery, has zinc and mercuric oxide electrodes and an electrolyte of potassium hydroxide. The mercury battery is suitable for use in electronic wristwatches and similar devices.
Secondary cells convert chemical energy into electrical energy through a chemical reaction that is essentially reversible. In ‘‘charging,’’ the cell is forced to operate in reverse of its discharging operation by pushing a current through in the opposite direction of the one normal in discharge. Energy is thus ‘‘stored’’ in these cells as chemical, not electrical, energy. They may be ‘‘recharged’’ by an electrical current passing through them in the opposite direction of their discharge. Secondary, or storage, cells are generally wet cells, which use a liquid electrolyte.
15. SOLAR POWER GENERATION
The emergence of solar power generation is part of the overall movement toward renewable energy production. Interest in this type of energy production grew in the early 1970s with an increased public awareness of the negative impact of technological developments on the environment. The use of solar power, of course, was not new. Heat produced by the sun was used for all sorts of purposes from the early history of humankind. In the search for renewable energy sources, the direct use of the sun’s heat has continued in the use of solar panels. In these panels, heat from the sun is absorbed by water flowing in pipes, and the hot water can then be used for heating purposes. In the twentieth century, two types of thermal solar energy systems developed: (1) active systems that used pumps or fans to transport the heat; and (2) passive systems that use natural heat transfer processes. In 1948 a school in Tucson, Arizona, with a passive solar energy system was built by Arthur Brown. In 1976 the Aspen-Pitkin County airport was opened as the first large commercial building in the U.S. that used a passive solar energy system for heating. However, the original idea of using passive solar energy goes back to ancient times. Archeologists have found houses with passive solar energy systems dating back to the fifth century AD.
16. STEAM TURBINES
The first steam turbine, of which there is any record, was made by Hero of Alexandria more than 2000 years ago. This simply demonstrated that a jet of steam, impinging on a paddle wheel, could convert heat energy into mechanical energy. In the late nineteenth century significant improvements in the efficiency of conversion were made by, among others, Sir Charles Parsons on Tyneside, U.K. and Charles G. Curtis in the U.S.
Early steam engines up to that time had involved very high rotational speed, which was difficult to utilize for many purposes unless speed-reducing gearboxes were employed. Parsons had deduced that moderate surface velocities and speeds of rotation were essential if the ‘‘turbine motor’’ was to receive general acceptance as a prime mover. His early designs arranged to divide the fall in pressure of the steam into small fractional expansions over a large number of turbine wheels in series so that the velocity of the steam over each wheel was not excessive.
At the close of the 19th century, many local power stations employed reciprocating steam engines to drive electric generators. Steam turbines had the advantage over reciprocating steam engines, which were based on the movement of a piston in a cylinder, of being lighter and more efficient. The Curtis multiple-stage steam turbine (patented in 1896, sold rights to General Electric in 1901) occupied a smaller space and cost much less than contemporary reciprocating steam engine-driven generators of the same output. The Curtis turbine was also shorter than the Parsons turbine, and was thus less susceptible to distortion of the central shaft.
The work that Curtis, Parsons, and others carried out in the development of steam turbines allowed large central power stations to be developed, providing electricity for the growing demand during the early 1900s. Early machines at the beginning of the 20th century
17. THERMAL GRAPHITE MODERATED NUCLEAR REACTORS
In a nuclear reactor, an element low on the atomic scale such as carbon or hydrogen is used to absorb kinetic energy to slow down naturally emitted neutrons from the radioactive fuel. In most power reactors, refined but unenriched natural uranium (238U or uranium-238) is the preferred fuel over 99 percent of the time. When the neutrons move more slowly or at a ‘‘moderated’’ speed, the chances of collision between the neutrons and other uranium nuclei, leading to fission and a chain reaction, are increased. Reactor designs are often named for the type of moderator used.
The first reactors, including the experimental pile built in 1942 at Chicago during World War II and the early production reactors built in 1943 at Hanford in Washington state, used graphite as a moderator. Later reactors used water, heavy water, sodium, or other materials as moderators. In the U.S., almost all power reactors and all submarine and ship propulsion reactors relied on pressurized water systems or boiling water systems, first installed in the late 1950s. Acronyms for all these systems have become conventional, with the most common being the boiling water reactor (BWR), pressurized water reactor (PWR), and light water-cooled graphite-moderated reactor (LWGR).
Accidents involving graphite reactors are particularly dangerous, since graphite is flammable. A release of radioactivity in 1957 at the British Windscale Reactor near Sellafield, Cumbria, a graphite production reactor, was not immediately disclosed. Even accidents with water-cooled reactors, such as that at Three Mile Island in Pennsylvania on March 28, 1979, cause national and international concern. However, far more serious was the Chernobyl fire of April 26 1986, in a 1000-megawatt rated RBMK graphite-moderated reactor. That fire spread radioactive contamination across not only the Ukraine but also much of eastern and northern Europe as well. As at Windscale, details of the Chernobyl accident were temporarily suppressed. Gas-cooled graphite reactors are prevented from burning by the fact that they are cooled with carbon dioxide. However, if oxygen-containing air leaks into the system and the cooling system fails, the graphite can ignite.
18. THERMAL WATER MODERATED NUCLEAR REACTORS
Nuclear reactors are usually classified by their coolant and their moderators. The moderator is a material, low in the atomic scale, whose atomic nucleus has the effect of slowing down or moderating the speed of fast neutrons emitted during nuclear fission. By slowing the speed of neutrons, the moderator increases the chance of collision of neutrons with the nuclei of fissionable nuclear fuel atoms. The original reactor designed by Enrico Fermi during the Manhattan Project at Chicago, known as Chicago Pile One, or CP-1, was a graphite-moderated, air-cooled reactor. Many British and French nuclear reactors for the generation of electrical power use carbon in the form of graphite, and they are cooled with carbon dioxide gas. These types are known as Magnox reactors. However, the common designs for power generation developed in the U.S. used water both as coolant and as a moderator.
Water-cooled reactors fall into two large families. Heavy water reactors contain water in which the hydrogen atom is replaced with the hydrogen isotope deuterium. This type of reactor is manufactured for export by Canada. The pressurized heavy water reactor (PHWR) has been exported and installed in India, Romania, and elsewhere. The U.S. built five heavy water reactors at Savannah River, South Carolina, in the 1950s to serve as production reactors for the manufacture of plutonium and tritium for nuclear weapons. By the late 1980s, all the Savannah River production reactors had been closed. After some experimentation with graphite-moderated gas-cooled designs and with heavy-water moderation during the 1950s, the U.S. followed the ‘‘light water’’ path.
19. WIND POWER GENERATION
Wind is essentially the movement of substantial air masses from regions of high pressure to regions of low pressure induced by the differential heating of the Earth’s surface. This simplistic view belies the complexity of atmospheric weather systems but serves to indicate the origin of climatic airflow.
The first attempts to harness wind power for electricity production date back to the 1930s. In Germany, Honnef planned a monstrous five turbine, 20 megawatt (MW), wind tower, several hundred meters high, a far cry from the sleek aerospace wind turbine generators (WTGs) of today. The design of a large scale WTG is limited to one of two formats realistically held to have good prospects. First, the horizontal axis type descended from those encountered by Don Quixote and common until recently in the flat lands of Europe; and second, the vertical axis machines of which the Darrieus rotor is perhaps the most common. Of the two, horizontal axis machines predominate, although the vertical axis type has many positive attributes, not the least of these being simplicity."
7,"8-ENERGY AND POWER TECHNOLOGY
 At the close of the 20th century, electricity was so commonplace that it would be difficult to imagine an existence without light, heat, and music bowing to our command at the flick of a switch. Children who could barely stretch high enough to toggle a light switch now have dominion over phenomena that less than a century ago would have been considered inconceivable. The temptations of such power have proved hard to resist.
The repercussions of the rapacious appetite for control of energy among Western industrial nations have not been confined to the lot of the individual, however. As in previous eras, when the control of mechanical or biological power carried financial, geographical, and social significance, the use and abuse of electrical energy now additionally carries environmental, political, and moral implications. Developments in energy and power in the twentieth century must therefore be considered within these broader thematic areas as the generation and consumption of energy are inextricably linked with practically the whole spectrum of human existence.
At the beginning of the 20th century, despite the fact that many components of modern electronics such as the battery had already been invented 100 years earlier, body power was still the norm, especially in rural areas. Horses, carriages, tow paths, water mills, and the like were the standard means of transport and power for a large proportion of the population, despite the growth of electricity and the 130 supply companies that were operating by 1896 in Britain. Even in urban settings, only lighting and telegraphy were advanced to the stage where the benefits were generally enjoyed as a result of Thomas Edison’s invention of the light bulb in 1879 and Alexander Graham Bell’s first telephone transmission in 1878.
By 1900 in Britain the main features of an electricity supply industry had been established. The system was based on the generation of high-voltage alternating current (AC), with transformers stepping down voltages for local use. However, one obstacle that the industry had to overcome was the lack of standardization across local areas. In some parts, direct current (DC) equipment was still installed, and local voltage levels and frequencies varied considerably. Despite problems posed by these variations, at the start of the century most of the appliances that are now taken for granted had appeared. Space heaters, cookers, and lighting equipment were not yet in every home, but the very speed at which their use was adopted was testament to the flexibility and popularity of electricity. In 1918 electric washing machines became available, and in 1919 the first refrigerator appeared in Britain. They had already been introduced for domestic use in the U.S. in 1913. Electricity had been firmly accepted as the energy of the future. Demand from the residential sector started to boom and spurred further research. Most importantly, perhaps, by the 1920s in Britain the domestic immersion heater began to take over the duties of coal. The use of electric trolleys and trains, which had been running since the end of the nineteenth century, also continued to expand, and underground travel developed swiftly. Electricity also made advances in communications possible, from the telegraph and the telephone, to the broadcasting boom of the 1920s. In 1928 the construction of a British national grid system began, and it took less than ten years before the system was in operation. This alacrity is partly to be explained by the influence of World War I. The war’s heavy demands on manufacturing acted as a great incentive for the rapidly evolving electricity industry, particularly with regard to improving the efficiency of supply. Thereafter, the rebuilding and expansion of industry across the industrialized world began. In Russia, Lenin was moved to state, ‘‘Communism equals Soviet power plus electrification,’’ as part of the propaganda for industrialization. Electricity took over the driving of fans, elevators, and cranes, driving coal-mining equipment, for example, and rolling mills in steel factories. The use of individual electric motors allowed astonishing advances in speed control, precision, and productivity of machine tools.
With World War II came devastation. Power stations and fuel supplies were inevitably considered as strategic targets for the bombers during the destructive aerial attacks by both the Axis powers and the Allies. By 1946, the estimated deficiency of generating capacity in Europe was 10,000 megawatts. According to anecdotal evidence, the victory bells in Paris were only able to ring out in 1945 because of electricity transmitted from Germany, where more industrial capacity of all kinds, including power stations, had survived. Whatever the truth of this may be, the security of electricity supply quickly became an issue of undisputed importance throughout Europe, and the fuels used in electrical generation were valuable resources indeed.
COAL
At the start of the twentieth century, there was a new worldwide optimism about coal as a resource that seemed to be available in almost unlimited amounts. Coal consumption levels rose steeply both in the U.S. and Europe, to reach a peak around 1914 and the outbreak of World War I. Between the world wars, consumption quantities remained almost static, particularly in the U.S., as other fuel types started to dominate the market. Reasons for this slow-down include the rising popularity of the four-stroke ‘‘Otto’’ cycle engine that is widely used in transportation even today as well as the commercialization of the diesel engine. These two technologies pushed fuel sources swiftly from solid to liquid fuels.
NUCLEAR POWER
Nuclear fission was discovered in the 1930s. Considerable research occurred in those early years, particularly in the U.S., the U.K., France, Canada, and the former Soviet Union, in the design and construction of commercial nuclear power stations. In the early 1940s, U.S. intelligence regarding Germany’s promising nuclear research activities dramatically hastened the U.S. resolve to build a nuclear weapon. The Manhattan Project was established for this purpose in August 1942. In July 1945, Manhattan Project scientists tested the first nuclear device in Alamagordo, New Mexico, using plutonium produced from a uranium and graphite-pile reactor in Richland, Washington. A month later a highly enriched uranium nuclear bomb was dropped on the Japanese city of Hiroshima, and a plutonium nuclear bomb was dropped on Nagasaki, effectively ending World War II.
The nuclear power industry suffered some notable disasters during its years of technological development. In 1979, the Three Mile Island Unit 2 (TMI-2) nuclear power plant in Pennsylvania suffered damage due to mechanical or electrical failure of parts of the cooling system. Just seven years later, on the opposite side of the Iron Curtain near an obscure city on the Pripiat River in northcentral Ukraine, another disaster occurred. This accident became a metaphor not only for the horror of uncontrolled nuclear power but also for the collapsing Soviet system and its disregard for the safety and welfare of workers. On April 26, 1986, the No. 4 reactor at Chernobyl exploded and released 30 to 40 times the radioactivity of the atomic bombs dropped on Hiroshima and Nagasaki. The Western world first learned of history’s worst nuclear accident from Sweden where abnormal radiation levels, the result of deposits carried by prevailing winds, were registered.
Ranking as one of the greatest industrial accidents of all time, the Chernobyl disaster and its impact on the course of Soviet events can scarcely be exaggerated. No-one can predict what will finally be the exact number of human victims. Thirty-one lives were lost immediately. Hundreds of thousands of Ukrainians, Russians, and Belo Russians had to abandon entire cities and settlements within the 30 kilometer zone of extreme contamination. Estimates vary, but it is likely that over 15 years after the event, some 3 million people, more than 2 million in Belarus alone, continued to live in contaminated areas.
OIL
Often accused of being one of the two great evils in the energy sector along with nuclear power, the oil industry grew over the course of the twentieth century to acquire significance and influence previously unimagined for any industrial sector. As the century opened, the U.S. was the largest oil producer in the world, but the discovery and exploitation of reserves in the Middle East, South America, and Mexico soon shifted the balance of the market away from the U.S., which by 1950 produced less than half the world’s oil. This trend continued and by the year 2000, oil production was almost equally divided between OPEC (Organization of Petroleum-Exporting Countries) and non-OPEC countries. Even in the early years of the century, the geographical spread of supply and demand quickly created the need for a system of distribution of unprecedented scale. The distances and quantities involved led to the construction of pipelines and huge ocean-going ships and tanker trucks. The capital intensive nature of these infrastructure projects, as well as the costs of exploration and exploitation of oil fields, concentrated control of resources in the hands of a few companies with vast coffers. As the reserves from easily exploitable sites dwindled, the pockets even of governments were insufficiently deep to invest in new drilling projects, and Royal Dutch Shell, Standard Oil, British Petroleum, and others were born.
Concern about fossil fuel depletion began to be voiced around the world in the 1960s, but the issue created headlines on the international political circuit in 1970 following the publication of the Club of Rome’s report ‘‘Limits to Growth.’’ This document warned of the impending exhaustion of the world’s 550 billion barrels of oil reserves. ‘‘We could use up all of the proven reserves of oil in the entire world by the end of the next decade,’’ said U.S. President Jimmy Carter. And although between 1970 and 1990 the world did indeed use 600 billion barrels of oil, and according to the Club of Rome reserves should have dwindled to less than zero by then, in fact, the unexploited reserves in 1990 amounted to 900 billion barrels not including tar shale.
HYDROELECTRIC POWER
Not a recent development by any stretch of the imagination, hydroelectric power was used extensively at the start of the twentieth century for mechanical work in mills and has a pedigree stretching back to ancient Egyptian times. Indeed, water power produces 24 percent of the world’s electricity and supplies more than 1 billion people with power. At the end of the twentieth century, hydroelectric power plants generally ranged in size from several hundred kilowatts to many hundreds of megawatts, but a few mammoth plants supplied up to 10,000 megawatts and electricity to millions of people. These leviathans, or ‘‘temples of modern India,’’ as India’s first prime minister Jawaharlal Nehru declared, were also the cause of massive discontent from social and environmental standpoints. The displacement of local indigenous populations and failure to deliver promised benefits were just two of the many complaints. By comparison, and despite hydroelectric power’s renewable credentials, the use of conventional fossil fuel technologies such as natural gas remained relatively uncontroversial.
COAL-GAS TECHNOLOGY
A derivative of coal as its name implies, coal-gas is produced through the carbonization of coal and has played a not insignificant role in the development of power and energy in the twentieth century. It was an important and well-established industry product as the century opened, although electricity had already started to make inroads into some of the markets that coal-gas served. Coal-gas enjoyed widespread use in domestic heating and cooking and some industrial facilities, but despite the invention of the Welsbach Mantle in 1885, electricity soon started to dominate the lighting market. The Ruhrgebeit in Germany was the most active coal-gas producing area in the world. It was here that the Lurgi process, in which low-grade brown coal is gasified by a mixture of superheated steam and oxygen at high pressure, flourished for many years. However, as the coal supplies necessary for the process became increasingly expensive, and as oil fractions with similar properties became available, the coal-gas industry swiftly declined. In fact, when the coal industry seemed to have reached a pinnacle, another rival industry—natural gas—was being born.
NATURAL GAS
The American gas industry developed along different lines from the European market. Each started from a different basis at the dawn of the twentieth century. The U.S. had been quick to adopt the production of coal-gas, which was used for lighting as early as 1816. After the discovery of fields of largely compatible natural gas in relatively shallow sites when searching for oil reserves, the natural gas industry expanded swiftly. Large-scale transmission mechanisms were developed with alacrity, and one noteworthy example of this came from the Trans-Continental Gas Pipeline Corporation, which completed a link from fields in Texas and Louisiana to the demand-intensive area around New York in 1951. By contrast, in Europe the exploitation of natural gas began in earnest in the years following World War II. In the Soviet Union, for example, the rich fields around Baku in Azerbaijan were connected to both their Eastern Bloc allies by 1971 and also to West Germany and Italy by over 680,000 kilometers of pipelines.
In Western Europe developments on the geopolitical level benefited Britain, which officially acquired the mineral rights for the western section of the North Sea in 1964. Just one year later, the West Sole field was discovered. Britain had already imported some natural gas from the U.S., and within 12 years had switched almost entirely from manufactured coal-gas to natural gas. This conversion was no simple operation. The differing properties of manufactured and natural gas meant that domestic and industrial appliances numbering in the tens of millions had to be altered. The British conversion scheme, which lasted ten years, is estimated to have cost £1000 million. Other similar conversion programs were carried out in Holland, Hungary, and even in the Far East.
In October 1973, panic gripped the U.S. The crude-oil rich Middle Eastern countries had cut off exports of petroleum to Western nations as punishment for their involvement in recent Arab–Israeli conflicts. Although the oil embargo would not ordinarily have made a tremendous impact on the U.S., panicking investors and oil companies caused a gigantic surge in oil prices.
There were more oil scares throughout the next two decades. When the Shah of Iran was deposed during a revolution, petroleum exports were diminished to virtually negligible levels, causing crude oil prices to soar once again. Iraq’s invasion of Kuwait in the 1990s also inflated oil prices, albeit for only a short time. These events highlighted the world’s dependence on Middle Eastern oil and raised political awareness about the security of oil supplies.
The ‘‘dash for gas’’ in the U.K.—the rapid switch from coal to gas as the dominant source of power generation fuel—was no doubt partly instigated by the discovery of home reserves there. Worldwide the new application of an old technology, combined cycle gas turbines, or CCGTs, played a significant role. During the last decades of the twentieth century, the gas turbine emerged as the world’s dominant technology for electricity generation. Gas turbine power plants thrived in countries as diverse as the U.S., Thailand, Spain, and Argentina. In the U.K., the changeover began in the late 1980s and resulted in the closure of many coal mines and coal-fired power stations. As electricity industries were privatized and liberalized, the CCGT in particular became more and more attractive because of its low capital cost, high thermal efficiency, and relatively low environmental impact. Indeed, this technology contributed to the trend identified by Cesare Marchetti, which depicts the chronological shift of the world’s sources of primary power from wood to coal to oil to gas during the last century and a half. Each of these fuels is successively richer in hydrogen and poorer in carbon than its predecessor, supporting the hypothesis that we are progressing toward a pure hydrogen economy.
DISTRIBUTED GENERATION
Embedded or distributed generation refers to power plants that feed electricity into a local distribution network. By saving transmission and distribution losses, it is generally considered to be an environmentally and socially beneficial option compared with centralized generation. Technologies that contributed to the expansion of this mode of generation include wind turbines, which developed to the point where their cost of generation rivaled that of central power stations, photovoltaic cells, and combined heat and power units. These industries expanded massively in the latter years of the century, particularly in Europe where regulatory measures gave impetus and a degree of commercial security to the fledgling industries.
HYDROGEN
Many industries worldwide began producing hydrogen, hydrogen-powered vehicles, hydrogen fuel cells, and other hydrogen products toward the end of the twentieth century. Hydrogen is intrinsically ‘‘cleaner’’ than any other fuel used to date because combustion of hydrogen with oxygen produces energy with only water, no greenhouse gases or particulate exhaust fumes, as a byproduct. At the close of the twentieth century, however, although prototypes and demonstration projects abounded, commercial competitiveness with conventional fuels was still only a distant prospect.
From almost wholly somatic sources of power in 1900, energy and power developed at an astonishing pace through the century. As the century closed, despite support for ‘‘green’’ power, particularly in developed nations, the worldwide generation of energy was still dominated by fossil fuels. Nevertheless, unprecedented changes seemed possible, driven for the first time by environmental and social concerns rather than technological possibilities or purely commercial considerations. Awareness of energy-related carbon emissions issues addressed by the Kyoto protocol raised questions concerning the institutional arrangements on both national and international levels, and their capacity for action in responding to public demand. After a century of development, a wide variety of institutional and regulatory regimes evolved around electricity supply. These most often took the form of a franchised, regulated monopoly within clearly defined administrative boundaries, in a functional symbiosis with government. However, each has the same basic technical model at its heart; Large, central generators produce AC electricity, and deliver it to consumers over a network. The continuing stable operation of this system on which many millions of people rely, once considered the responsibility of central governments, is changing. The increasing shift toward liberalization and internationalization is moving responsibility for energy supplies away from state-owned organizations, a trend compounded by the environmental and institutional implications of renewable energy technologies."
8,"9-Computer Technology Research Paper Topics
1. ANALOG COMPUTERS
Paralleling the split between analog and digital computers, in the 1950s the term analog computer was a posteriori projected onto pre-existing classes of mechanical, electrical, and electromechanical computing artifacts, subsuming them under the same category. The concept of analog, like the technical demarcation between analog and digital computer, was absent from the vocabulary of those classifying artifacts for the 1914 Edinburgh Exhibition, the first world’s fair emphasizing computing technology, and this leaves us with an invaluable index of the impressive number of classes of computing artifacts amassed during the few centuries of capitalist modernity. True, from the debate between ‘‘smooth’’ and ‘‘lumpy’’ artificial lines of computing (1910s) to the differentiation between ‘‘continuous’’ and ‘‘cyclic’’ computers (1940s), the subsequent analog–digital split became possible by the multitudinous accumulation of attempts to decontextualize the computer from its socio-historical use alternately to define the ideal computer technically. The fact is, however, that influential classifications of computing technology from the previous decades never provided an encompassing demarcation compared to the analog– digital distinction used since the 1950s. Historians of the digital computer find that the experience of working with software was much closer to art than science, a process that was resistant to mass production; historians of the analog computer find this to have been typical of working with the analog computer throughout all its aspects. The historiography of the progress of digital computing invites us to turn to the software crisis, which perhaps not accidentally, surfaced when the crisis caused by the analog ended. Noticeably, it was not until the process of computing with a digital electronic computer became sufficiently visual by the addition of a special interface—to substitute for the loss of visualization that was previously provided by the analog computer—that the analog computer finally disappeared.
2. ARTIFICIAL INTELLIGENCE
Artificial intelligence (AI) is the field of software engineering that builds computer systems and occasionally robots to perform tasks that require intelligence. The term ‘‘artificial intelligence’’ was coined by John McCarthy in 1958, then a graduate student at Princeton, at a summer workshop held at Dartmouth in 1956. This two-month workshop marks the official birth of AI, which brought together young researchers who would nurture the field as it grew over the next several decades: Marvin Minsky, Claude Shannon, Arthur Samuel, Ray Solomonoff, Oliver Selfridge, Allen Newell, and Herbert Simon. It would be difficult to argue that the technologies derived from AI research had a profound effect on our way of life by the beginning of the 21st century. However, AI technologies have been successfully applied in many industrial settings, medicine and health care, and video games. Programming techniques developed in AI research were incorporated into more widespread programming practices, such as high-level programming languages and time-sharing operating systems. While AI did not succeed in constructing a computer which displays the general mental capabilities of a typical human, such as the HAL computer in Arthur C. Clarke and Stanley Kubrick’s film 2001: A Space Odyssey, it has produced programs that perform some apparently intelligent tasks, often at a much greater level of skill and reliability than humans. More than this, AI has provided a powerful and defining image of what computer technology might someday be capable of achieving.
3. COMPUTER AND VIDEO GAMES
Interactive computer and video games were first developed in laboratories as the late-night amusements of computer programmers or independent projects of television engineers. Their formats include computer software; networked, multiplayer games on time-shared systems or servers; arcade consoles; home consoles connected to television sets; and handheld game machines. The first experimental projects grew out of early work in computer graphics, artificial intelligence, television technology, hardware and software interface development, computer-aided education, and microelectronics. Important examples were Willy Higinbotham’s oscilloscope-based ‘‘Tennis for Two’’ at the Brookhaven National Laboratory (1958); ‘‘Spacewar!,’’ by Steve Russell, Alan Kotok, J. Martin Graetz and others at the Massachusetts Institute of Technology (1962); Ralph Baer’s television-based tennis game for Sanders Associates (1966); several networked games from the PLATO (Programmed Logic for Automatic Teaching Operations) Project at the University of Illinois during the early 1970s; and ‘‘Adventure,’’ by Will Crowther of Bolt, Beranek & Newman (1972), extended by Don Woods at Stanford University’s Artificial Intelligence Laboratory (1976). The main lines of development during the 1970s and early 1980s were home video consoles, coin-operated arcade games, and computer software.
4. COMPUTER DISPLAYS
The display is an essential part of any general-purpose computer. Its function is to act as an output device to communicate data to humans using the highest bandwidth input system that humans possess—the eyes. Much of the development of computer displays has been about trying to get closer to the limits of human visual perception in terms of color and spatial resolution. Mainframe and minicomputers used ‘‘terminals’’ to display the output. These were fed data from the host computer and processed the data to create screen images using a graphics processor. The display was typically integrated with a keyboard system and some communication hardware as a terminal or video display unit (VDU) following the basic model used for teletypes. Personal computers (PCs) in the late 1970s and early 1980s changed this model by integrating the graphics controller into the computer chassis itself. Early PC displays typically displayed only monochrome text and communicated in character codes such as ASCII. Line-scanning frequencies were typically from 15 to 20 kilohertz—similar to television. CRT displays rapidly developed after the introduction of video graphics array (VGA) technology (640 by 480 pixels in16 colors) in the mid-1980s and scan frequencies rose to 60 kilohertz or more for mainstream displays; 100 kilohertz or more for high-end displays. These displays were capable of displaying formats up to 2048 by 1536 pixels with high color depths. Because the human eye is very quick to respond to visual stimulation, developments in display technology have tended to track the development of semiconductor technology that allows the rapid manipulation of the stored image.
5. COMPUTER MEMORY FOR PERSONAL COMPUTERS
During the second half of the twentieth century, the two primary methods used for the long-term storage of digital information were magnetic and optical recording. These methods were selected primarily on the basis of cost. Compared to core or transistorized random-access memory (RAM), storage costs for magnetic and optical media were several orders of magnitude cheaper per bit of information and were not volatile; that is, the information did not vanish when electrical power was turned off. However, access to information stored on magnetic and optical recorders was much slower compared to RAM memory. As a result, computer designers used a mix of both types of memory to accomplish computational tasks. Designers of magnetic and optical storage systems have sought meanwhile to increase the speed of access to stored information to increase the overall performance of computer systems, since most digital information is stored magnetically or optically for reasons of cost.
6. COMPUTER MODELING
Computer simulation models have transformed the natural, engineering, and social sciences, becoming crucial tools for disciplines as diverse as ecology, epidemiology, economics, urban planning, aerospace engineering, meteorology, and military operations. Computer models help researchers study systems of extreme complexity, predict the behavior of natural phenomena, and examine the effects of human interventions in natural processes. Engineers use models to design everything from jets and nuclear-waste repositories to diapers and golf clubs. Models enable astrophysicists to simulate supernovas, biochemists to replicate protein folding, geologists to predict volcanic eruptions, and physiologists to identify populations at risk of lead poisoning. Clearly, computer models provide a powerful means of solving problems, both theoretical and applied.
7. COMPUTER NETWORKS
Computers and computer networks have changed the way we do almost everything—the way we teach, learn, do research, access or share information, communicate with each other, and even the way we entertain ourselves. A computer network, in simple terms, consists of two or more computing devices (often called nodes) interconnected by means of some medium capable of transmitting data that allows the computers to communicate with each other in order to provide a variety of services to users.
8. COMPUTER SCIENCE
Computer science occupies a unique position among the scientific and technical disciplines. It revolves around a specific artifact—the electronic digital computer—that touches upon a broad and diverse set of fields in its design, operation, and application. As a result, computer science represents a synthesis and extension of many different areas of mathematics, science, engineering, and business.
9. COMPUTER-AIDED CONTROL TECHNOLOGY
The story of computer-aided control technology is inextricably entwined with the modern history of automation. Automation in the first half of the twentieth century involved (often analog) processes for continuous automatic measurement and control of hardware by hydraulic, mechanical, or electromechanical means. These processes facilitated the development and refinement of battlefield fire-control systems, feedback amplifiers for use in telephony, electrical grid simulators, numerically controlled milling machines, and dozens of other innovations.
10. COMPUTER-AIDED DESIGN AND MANUFACTURE
Computer-aided design and manufacture, known by the acronym CAD/CAM, is a process for manufacturing mechanical components, wherein computers are used to link the information needed in and produced by the design process to the information needed to control the machine tools that produce the parts. However, CAD/CAM actually constitutes two separate technologies that developed along similar, but unrelated, lines until they were combined in the 1970s.
11. COMPUTER-USER INTERFACE
A computer interface is the point of contact between a person and an electronic computer. Today’s interfaces include a keyboard, mouse, and display screen. Computer user interfaces developed through three distinct stages, which can be identified as batch processing, interactive computing, and the graphical user interface (GUI). Today’s graphical interfaces support additional multimedia features, such as streaming audio and video. In GUI design, every new software feature introduces more icons into the process of computer– user interaction. Presently, the large vocabulary of icons used in GUI design is difficult for users to remember, which creates a complexity problem. As GUIs become more complex, interface designers are adding voice recognition and intelligent agent technologies to make computer user interfaces even easier to operate.
12. EARLY COMPUTER MEMORY
Mechanisms to store information were present in early mechanical calculating machines, going back to Charles Babbage’s analytical engine proposed in the 1830s. It introduced the concept of the ‘‘store’’ and, if ever built, would have held 1000 numbers of up to 50 decimal digits. However, the move toward base-2 or binary computing in the 1930s brought about a new paradigm in technology—the digital computer, whose most elementary component was an on–off switch. Information on a digital system is represented using a combination of on and off signals, stored as binary digits (shortened to bits): zeros and ones. Text characters, symbols, or numerical values can all be coded as bits, so that information stored in digital memory is just zeros and ones, regardless of the storage medium. The history of computer memory is closely linked to the history of computers but a distinction should be made between primary (or main) and secondary memory. Computers only need operate on one segment of data at a time, and with memory being a scarce resource, the rest of the data set could be stored in less expensive and more abundant secondary memory.
13. EARLY DIGITAL COMPUTERS
Digital computers were a marked departure from the electrical and mechanical calculating and computing machines in wide use from the early twentieth century. The innovation was of information being represented using only two states (on or off), which came to be known as ‘‘digital.’’ Binary (base 2) arithmetic and logic provided the tools for these machines to perform useful functions. George Boole’s binary system of algebra allowed any mathematical equation to be represented by simply true or false logic statements. By using only two states, engineering was also greatly simplified, and universality and accuracy increased. Further developments from the early purpose-built machines, to ones that were programmable accompanied by many key technological developments, resulted in the well-known success and proliferation of the digital computer.
14. ELECTRONIC CONTROL TECHNOLOGY
The advancement of electrical engineering in the twentieth century made a fundamental change in control technology. New electronic devices including vacuum tubes (valves) and transistors were used to replace electromechanical elements in conventional controllers and to develop new types of controllers. In these practices, engineers discovered basic principles of control theory that could be further applied to design electronic control systems.
15. ENCRYPTION AND CODE BREAKING
The word cryptography comes from the Greek words for ‘‘hidden’’ (kryptos) and ‘‘to write’’ (graphein)—literally, the science of ‘‘hidden writing.’’ In the twentieth century, cryptography became fundamental to information technology (IT) security generally. Before the invention of the digital computer at mid-century, national governments across the world relied on mechanical and electromechanical cryptanalytic devices to protect their own national secrets and communications, as well as to expose enemy secrets. Code breaking played an important role in both World Wars I and II, and the successful exploits of Polish and British cryptographers and signals intelligence experts in breaking the code of the German Enigma ciphering machine (which had a range of possible transformations between a message and its code of approximately 150 trillion (or 150 million million million) are well documented.
16. ERROR CHECKING AND CORRECTION
In telecommunications, whether transmission of data or voice signals is over copper, fiber-optic, or wireless links, information coded in the signal transmitted must be decoded by the receiver from a background of noise. Signal errors can be introduced, for example from physical defects in the transmission medium (semiconductor crystal defects, dust or scratches on magnetic memory, bubbles in optical fibers), from electromagnetic interference (natural or manmade) or cosmic rays, or from cross-talk (unwanted coupling) between channels. In digital signal transmission, data is transmitted as ‘‘bits’’ (ones or zeros, corresponding to on or off in electronic circuits). Random bit errors occur singly and in no relation to each other. Burst error is a large, sustained error or loss of data, perhaps caused by transmission problems in the connecting cables, or sudden noise. Analog to digital conversion can also introduce sampling errors.
17. GLOBAL POSITIONING SYSTEM (GPS)
The NAVSTAR (NAVigation System Timing And Ranging) Global Positioning System (GPS) provides an unlimited number of military and civilian users worldwide with continuous, highly accurate data on their position in four dimensions— latitude, longitude, altitude, and time— through all weather conditions. It includes space, control, and user segments (Figure 6). A constellation of 24 satellites in 10,900 nautical miles, nearly circular orbits—six orbital planes, equally spaced 60 degrees apart, inclined approximately 55 degrees relative to the equator, and each with four equidistant satellites—transmits microwave signals in two different L-band frequencies. From any point on earth, between five and eight satellites are ‘‘visible’’ to the user. Synchronized, extremely precise atomic clocks—rubidium and cesium— aboard the satellites render the constellation semiautonomous by alleviating the need to continuously control the satellites from the ground. The control segment consists of a master facility at Schriever Air Force Base, Colorado, and a global network of automated stations. It passively tracks the entire constellation and, via an S-band uplink, periodically sends updated orbital and clock data to each satellite to ensure that navigation signals received by users remain accurate. Finally, GPS users—on land, at sea, in the air or space—rely on commercially produced receivers to convert satellite signals into position, time, and velocity estimates.
18. GYROCOMPASS AND INERTIAL GUIDANCE
Before the twentieth century, navigation at sea employed two complementary methods, astronomical and dead reckoning. The former involved direct measurements of celestial phenomena to ascertain position, while the latter required continuous monitoring of a ship’s course, speed, and distance run. New navigational technology was required not only for iron ships in which traditional compasses required correction, but for aircraft and submarines in which magnetic compasses cannot be used. Owing to their rapid motion, aircraft presented challenges for near instantaneous navigation data collection and reduction. Electronics furnished the exploitation of radio and the adaptation of a gyroscope to direction finding through the invention of the nonmagnetic gyrocompass.
Although the Cold War arms race after World War II led to the development of inertial navigation, German manufacture of the V-2 rocket under the direction of Wernher von Braun during the war involved a proto-inertial system, a two-gimballed gyro with an integrator to determine speed. Inertial guidance combines a gyrocompass with accelerometers installed along orthogonal axes, devices that record all accelerations of the vehicle in which inertial guidance has been installed. With this system, if the initial position of the vehicle is known, then the vehicle’s position at any moment is known because integrators record all directions and accelerations and calculate speeds and distance run. Inertial guidance devices can subtract accelerations due to gravity or other motions of the vehicle. Because inertial guidance does not depend on an outside reference, it is the ultimate dead reckoning system, ideal for the nuclear submarines for which they were invented and for ballistic missiles. Their self-contained nature makes them resistant to electronic countermeasures. Inertial systems were first installed in commercial aircraft during the 1960s. The expense of manufacturing inertial guidance mechanisms (and their necessary management by computer) has limited their application largely to military and some commercial purposes. Inertial systems accumulate errors, so their use at sea (except for submarines) has been as an adjunct to other navigational methods, unlike aircraft applications. Only the development of the global positioning system (GPS) at the end of the century promised to render all previous navigational technologies obsolete. Nevertheless, a range of technologies, some dating to the beginning of the century, remain in use in a variety of commercial and leisure applications.
19. HYBRID COMPUTERS
Following the emergence of the analog–digital demarcation in the late 1940s—and the ensuing battle between a speedy analog versus the accurate digital—the term ‘‘hybrid computer’’ surfaced in the early 1960s. The assumptions held by the adherents of the digital computer—regarding the dynamic mechanization of computational labor to accompany the equally dynamic increase in computational work—was becoming a universal ideology. From this perspective, the digital computer justly appeared to be technically superior. In introducing the digital computer to social realities, however, extensive interaction with the experienced analog computer adherents proved indispensable, especially given that the digital proponents’ expectation of progress by employing the available and inexpensive hardware was stymied by the lack of inexpensive software. From this perspective—as historiographically unwanted it may be by those who agree with the essentialist conception of the analog–digital demarcation—the history of the hybrid computer suggests that the computer as we now know it was brought about by linking the analog and the digital, not by separating them. Placing the ideal analog and the ideal digital at the two poles, all computing techniques that combined some features of both fell beneath ‘‘hybrid computation’’; the designators ‘‘balanced’’ or ‘‘true’’ were preserved for those built with appreciable amounts of both. True hybrids fell into the middle spectrum that included: pure analog computers, analog computers using digital-type numerical analysis techniques, analog computers programmed with the aid of digital computers, analog computers using digital control and logic, analog computers using digital subunits, analog computers using digital computers as peripheral equipment, balanced hybrid computer systems, digital computers using analog subroutines, digital computers with analog arithmetic elements, digital computers designed to permit analog-type programming, digital computers with analog-oriented compilers and interpreters, and pure digital computers.
20. INFORMATION THEORY
Information theory, also known originally as the mathematical theory of communication, was first explicitly formulated during the mid-twentieth century. Almost immediately it became a foundation; first, for the more systematic design and utilization of numerous telecommunication and information technologies; and second, for resolving a paradox in thermodynamics. Finally, information theory has contributed to new interpretations of a wide range of biological and cultural phenomena, from organic physiology and genetics to cognitive behavior, human language, economics, and political decision making. Reflecting the symbiosis between theory and practice typical of twentieth century technology, technical issues in early telegraphy and telephony gave rise to a proto-information theory developed by Harry Nyquist at Bell Labs in 1924 and Ralph Hartley, also at Bell Labs, in 1928. This theory in turn contributed to advances in telecommunications, which stimulated the development of information theory per se by Claude Shannon and Warren Weaver, in their book The Mathematical Theory of Communication published in 1949. As articulated by Claude Shannon, a Bell Labs researcher, the technical concept of information is defined by the probability of a specific message or signal being picked out from a number of possibilities and transmitted from A to B. Information in this sense is mathematically quantifiable. The amount of information, I, conveyed by signal, S, is inversely related to its probability, P. That is, the more improbable a message, the more information it contains. To facilitate the mathematical analysis of messages, the measure is conveniently defined as I ¼ log2 1/P(S), and is named a binary digit or ‘‘bit’’ for short. Thus in the simplest case of a two-state signal (1 or 0, corresponding to on or off in electronic circuits), with equal probability for each state, the transmission of either state as the code for a message would convey one bit of information. The theory of information opened up by this conceptual analysis has become the basis for constructing and analyzing digital computational devices and a whole range of information technologies (i.e., technologies including telecommunications and data processing), from telephones to computer networks.
21. INTERNET
The Internet is a global computer network of networks whose origins are found in U.S. military efforts. In response to Sputnik and the emerging space race, the Advanced Research Projects Agency (ARPA) was formed in 1958 as an agency of the Pentagon. The researchers at ARPA were given a generous mandate to develop innovative technologies such as communications.
In 1962, psychologist J.C.R. Licklider from the Massachusetts Institute of Technology’s Lincoln Laboratory joined ARPA to take charge of the Information Processing Techniques Office (IPTO). In 1963 Licklider wrote a memo proposing an interactive network allowing people to communicate via computer. This project did not materialize. In 1966, Bob Taylor, then head of the IPTO, noted that he needed three different computer terminals to connect to three different machines in different locations around the nation. Taylor also recognized that universities working with IPTO needed more computing resources. Instead of the government buying machines for each university, why not share machines? Taylor revitalized Licklider’s idea, securing $1 million in funding, and hired 29-yearold Larry Roberts to direct the creation of ARPAnet.
In 1974, Robert Kahn and Vincent Cerf proposed the first internet-working protocol, a way for datagrams (packets) to be communicated between disparate networks, and they called it an ‘‘internet.’’ Their efforts created transmission control protocol/internet protocol (TCP/IP). In 1982, TCP/IP replaced NCP on ARPAnet. Other networks adopted TCP/IP and it became the dominant standard for all networking by the late 1990s.
In 1981 the U.S. National Science Foundation (NSF) created Computer Science Network (CSNET) to provide universities that did not have access to ARPAnet with their own network. In 1986, the NSF sponsored the NSFNET ‘‘backbone’’ to connect five supercomputing centers. The backbone also connected ARPAnet and CSNET together, and the idea of a network of networks became firmly entrenched. The open technical architecture of the Internet allowed numerous innovations to be grafted easily onto the whole. When ARPAnet was dismantled in 1990, the Internet was thriving at universities and technology- oriented companies. The NSF backbone was dismantled in 1995 when the NSF realized that commercial entities could keep the Internet running and growing on their own, without government subsidy. Commercial network providers worked through the Commercial Internet Exchange to manage network traffic.
22. MAINFRAME COMPUTERS
The term ‘‘computer’’ currently refers to a general-purpose, digital, electronic, stored-program calculating machine. The term ‘‘mainframe’’ refers to a large, expensive, multiuser computer, able to handle a wide range of applications. The term was derived from the main frame or cabinet in which the central processing unit (CPU) and main memory of a computer were kept separate from those cabinets that held peripheral devices used for input and output.
Computers are generally classified as supercomputers, mainframes, minicomputers, or microcomputers. This classification is based on factors such as processing capability, cost, and applications, with supercomputers the fastest and most expensive. All computers were called mainframes until the 1960s, including the first supercomputer, the naval ordnance research calculator (NORC), offered by International Business Machines (IBM) in 1954. In 1960, Digital Equipment Corporation (DEC) shipped the PDP-1, a computer that was much smaller and cheaper than a mainframe.
Mainframes once each filled a large room, cost millions of dollars, and needed a full maintenance staff, partly in order to repair the damage caused by the heat generated by their vacuum tubes. These machines were characterized by proprietary operating systems and connections through dumb terminals that had no local processing capabilities. As personal computers developed and began to approach mainframes in speed and processing power, however, mainframes have evolved to support a client/server relationship, and to interconnect with open standard-based systems. They have become particularly useful for systems that require reliability, security, and centralized control. Their ability to process large amounts of data quickly make them particularly valuable for storage area networks (SANs). Mainframes today contain multiple CPUs, providing additional speed through multiprocessing operations. They support many hundreds of simultaneously executing programs, as well as numerous input and output processors for multiplexing devices, such as video display terminals and disk drives. Many legacy systems, large applications that have been developed, tested, and used over time, are still running on mainframes.
23. MINERAL PROSPECTING
Twentieth century mineral prospecting draws upon the accumulated knowledge of previous exploration and mining activities, advancing technology, expanding knowledge of geologic processes and deposit models, and mining and processing capabilities to determine where and how to look for minerals of interest. Geologic models have been developed for a wide variety of deposit types; the prospector compares geologic characteristics of potential exploration areas with those of deposit models to determine which areas have similar characteristics and are suitable prospecting locations. Mineral prospecting programs are often team efforts, integrating general and site-specific knowledge of geochemistry, geology, geophysics, and remote sensing to ‘‘discover’’ hidden mineral deposits and ‘‘measure’’ their economic potential with increasing accuracy and reduced environmental disturbance. Once a likely target zone has been identified, multiple exploration tools are used in a coordinated program to characterize the deposit and its economic potential.
24. PACKET SWITCHING
Historically the first communications networks were telegraphic—the electrical telegraph replacing the mechanical semaphore stations in the mid-nineteenth century. Telegraph networks were largely eclipsed by the advent of the voice (telephone) network, which first appeared in the late nineteenth century, and provided the immediacy of voice conversation. The Public Switched Telephone Network allows a subscriber to dial a connection to another subscriber, with the connection being a series of telephone lines connected together through switches at the telephone exchanges along the route. This technique is known as circuit switching, as a circuit is set up between the subscribers, and is held until the call is cleared.
One of the disadvantages of circuit switching is the fact that the capacity of the link is often significantly underused due to silences in the conversation, but the spare capacity cannot be shared with other traffic. Another disadvantage is the time it takes to establish the connection before the conversation can begin. One could liken this to sending a railway engine from London to Edinburgh to set the points before returning to pick up the carriages. What is required is a compromise between the immediacy of conversation on an established circuit-switched connection, with the ad hoc delivery of a store-and-forward message system. This is what packet switching is designed to provide.
25. PERSONAL COMPUTERS
A personal computer, or PC, is designed for personal use. Its central processing unit (CPU) runs single-user systems and application software, processes input from the user, sending output to a variety of peripheral devices. Programs and data are stored in memory and attached storage devices. Personal computers are generally single-user desktop machines, but the term has been applied to any computer that ‘‘stands alone’’ for a single user, including portable computers.
The technology that enabled the construction of personal computers was the microprocessor, a programmable integrated circuit (or ‘‘chip’’) that acts as the CPU. Intel introduced the first microprocessor in 1971, the 4-bit 4004, which it called a ‘‘microprogrammable computer on a chip.’’ The 4004 was originally developed as a general-purpose chip for a programmable calculator, but Intel introduc"
9,"10-Chemical Technology Research Paper Topics
1. BIOPOLYMERS
Biopolymers are natural polymers, long-chained molecules (macromolecules) consisting mostly of a repeated composition of building blocks or monomers that are formed and utilized by living organisms. Each group of biopolymers is composed of different building blocks, for example chains of sugar molecules form starch (a polysaccharide), chains of amino acids form proteins and peptides, and chains of nucleic acid form DNA and RNA (polynucleotides). Biopolymers can form gels, fibers, coatings, and films depending on the specific polymer, and serve a variety of critical functions for cells and organisms. Advances in metabolic engineering, environmental considerations about renewable polymers from nonpetroleum feedstocks, and the expansion in molecular biology and protein engineering tools in general are taking biopolymer synthesis and production in new directions. The opportunity to enhance, alter, or direct the structural features of biopolymers through genetic manipulation, physiological controls, or enzymatic processes provides new routes to novel polymers with specialty functions. The use of biopolymers in commodity and specialty materials, as well as biomedical applications, can be expected to continue to increase with respect to petrochemical-derived materials. The benefit in tailoring structural features is a plus for generating higher performance properties or more specialized functional performance. Biosynthesis and disposal of biopolymers can be considered within a renewable resource loop, reducing environmental burdens associated with synthetic polymers derived from petrochemicals that often require hundreds of years to degrade. In addition, biopolymers can often be produced from low cost agricultural feedstocks versus petroleum supplies and thereby generate value-added products.
2. BORANES
Boranes are chemical compounds of boron and hydrogen. During the 1950s, the U.S. government sponsored a major secretive effort to produce rocket and aircraft fuels based on boron hydrides. Much of the information initially available to the U.S. effort was contained in a book written by the German chemist Alfred Stock in 1933. When burned in air, the energy released by various boron hydride compounds, as measured by their heat of combustion, is 20 to 55 percent greater than the energy released by petroleum-based jet fuels. It was expected that this greater energy content of the boron fuels would translate into equivalent higher payloads or ranges for rockets and aircraft. All of the boron fuel manufacturing processes started with the production of diborane, a compound composed of two boron atoms linked to six hydrogen atoms. Initially, this was produced by reacting lithium hydride with boron trifluoride or boron trichloride in diethyl ether as a solvent. This entailed a need to recover and recycle the expensive lithium. A later process produced diborane by reacting sodium borohydride with boron trichloride in the presence of catalytic amounts of aluminum chloride, using a solvent called diglyme.
3. CHEMICAL PROCESS ENGINEERING
The chemical industry expanded dramatically during the twentieth century to become a highly integrated and increasingly influential contributor to the international economy. Its products seeded and fertilized the growth of other new technologies, particularly in the textiles, explosives, transport, and pharmaceutical industries. The industry also became a major supporter of industrial research, especially in the U.S. and Germany. The production of chemicals during the century can be described as a history of products, processes and professions.
The larger scale changes in chemical production can be better understood in terms of processes rather than as discrete products. Indeed, Hardie and Pratt (1966) describe the history of the chemical industry in terms of the history of its processes; that is, the succession of actions that transform raw materials into a new chemical product. Such conversion may involve chemical reactions (e.g., the production of soda alkali and sulfuric acid in the LeBlanc process); physical change (e.g., oxidation by roasting, or distillation by boiling and condensation); or physical manipulation (e.g., by grinding, mixing and extruding).
4. CHEMICAL WARFARE
Popular fiction forecast the use of poison gas in warfare from the 1890s. While an effort was made to ban the wartime use of gas at The Hague International Peace Conference in 1899, military strategists and tacticians dismissed chemical weapons as a fanciful notion. The stalemate of World War I changed this mindset. Under Fritz Haber, a chemist at the Kaiser Wilhelm Institute, Germany’s chemical industry began making gas weapons. Compressed chlorine gas in 5730 cylinders was released against French Algerian and Canadian troops at Ypres, Belgium, on April 22, 1915. The gas attack resulted in approximately 3000 casualties, including some 800 deaths. Within months the British and French developed both gas agents of their own and protective gear, ensuring that chemical warfare would become a regular feature of the war.
A variety of lethal and nonlethal chemical agents were developed in World War I. Lethal agents included the asphyxiating gases such as chlorine, phosgene, and diphosgene that drown their victims in mucous, choking off the supply of oxygen from the lungs. A second type were blood gases like hydrogen cyanide, which block the body’s ability to absorb oxygen from red corpuscles. Incapacitating gases included lachrymatorics (tear gases) and vesicants (blistering gases). The most notorious of these is mustard gas (Bis-[2- chloroethyl] sulphide), a blistering agent that produces horrible burns on the exposed skin and destroys mucous tissue and also persists on the soil for as long as 48 hours after its initial dispersion.
5. CHROMATOGRAPHY
The natural world is one of complex mixtures, often with up to a 100,000 (e.g., proteins in the human body) or 1,000,000 (e.g., petroleum) components. Separation methods necessary to cope with these, and with the simpler but still challenging mixtures encountered, for example in pharmaceutical analysis, are based on chromatography and electrophoresis, and underpin research, development, and quality control in numerous industries and in environmental, food, and forensic analysis. In chromatography, a sample is dissolved in a mobile phase (initially this was a liquid), which is then passed through a stationary phase (which is either a liquid or a solid) held in a small diameter tube—the ‘‘column.’’ According to the differing relative solubilities in the two phases, mixture components travel through the column at different rates and become separated before emerging and detected by the measurement of some chemical or physical property. The sample size can be as small as one picogram (10–12 g), but tens of grams can be handled in preparative separations.
6. COATINGS, PIGMENTS, AND PAINTS
The development and application of pigments, paints, and coatings have been an integral part of human development from Paleolithic cave paintings, the art of early civilizations, and protection of buildings from rain. During the twentieth century, understanding of chemicals and the manufacturing need for high-quality decorative and protective coatings drove rapid progression in paint technology. All paints employ the same basic ingredients: pigment to provide color; a medium to bind or suspend the pigment, including emulsions such as resins or oils; and a solvent carrier, which acts to wet the surface to ensure adhesion and thins the resin to make it easy to apply. Early pigments such as those used in Minoan frescoes and Anasazi rock art and the use of henna as body paint originated primarily from natural sources. Clays, mineral pigments such as iron or chromium oxide, vegetable dyes, animal sources such as shells or urine, as well as precious metals and gems gave rise to a broad selection of pigments that were often unique to one region (e.g., lapis lazuli) and thus highly prized as trade items. But value wasn’t limited merely to pigments. Preservation of painted surfaces required protective resins. The most successful early coating was lacquer, used in China since at least the 1300 BC. Lacquer was processed—using a highly guarded secret formula—from resin from the lacquer tree (Rhus vernicflua). Shellac, produced from the gum secreted by an insect native to India and southern Asia, makes a varnish when mixed with acetone or alcohol and was used from the eighteenth century. Natural plant resins dissolved in oil or solvent such as turpentine were used in the nineteenth century— evaporation of the solvent leaves a lacquer coating.
7. COMBINATORIAL CHEMISTRY
Combinatorial chemistry is a term created about 1990 to describe the rapid generation of multitudes of chemical structures with the main focus on discovering new drugs. In combinatorial chemistry the chemist should perform at least one step of the synthesis in combinatorial fashion. In the classical chemical synthesis, one synthetic vessel (flask, reactor) is used to perform chemical reaction designed to create one chemical entity. Combinatorial techniques utilize the fact that several operations of the synthesis can be performed simultaneously. Historically, the first papers bringing the world’s attention to combinatorial chemistry were published in 1991, but none of these papers used the term combinatorial chemistry. Interestingly, they were not the first papers describing the techniques for preparation of compound mixtures for biological evaluation. Previously, H. Mario Geysen’s lab had prepared mixtures of peptides for identification of antibody ligands in 1986. Other laboratories heavily engaged in synthesizing multitudes or mixtures of peptides were Richard A. Houghten’s laboratory in San Diego and A ´ rpa´d Furka’s laboratory in Budapest. The recollections of the authors of these historical papers were published in the journal dedicated to combinatorial chemistry, Journal of Combinatorial Chemistry.
8. CRACKING
Three major innovations emerged to meet the dramatically higher needs for quantity and better quality motor fuel in the twentieth century: thermal cracking, tetra ethyl lead, and catalytic cracking (including the fluid method). William M. Burton of Standard Oil, Indiana, in 1911–1913 raised gasoline fractions from petroleum distillation from 15 to 40 percent by increasing both temperatures and pressures. Benjamin Stillman Jr. of Yale University had discovered in 1855 that high temperatures could transform or ‘‘crack’’ heavy petroleum fractions into lighter or volatile components, but at atmospheric pressure over half of the raw material was lost to vaporization before the cracking temperature (about 360C) was reached. English scientists James Dewar and Boverton Redwood discovered and patented in 1889 a process using higher pressure to restrain more of the heavier fractions and increase the volatile components. Burton and his team, not aware of this patent at the start of their work, similarly used higher pressure to improve his gasoline yields. Their process by 1913 employed a drum 9 by 2.5 meters diameter over a furnace and a long runback pipe of 300 millimeter diameter that carried vapors to condensing coils and simultaneously allowed heavier fractions to drip back into the drum for more cracking. A tank connected to the condensing coils separated gasoline from the uncondensed hydrocarbon vapors. They used a comparatively low (5.1 atmospheres) pressure because they relied on riveted plates. Burton’s team improved the batch process in the next three years with false bottom plates in the still to improve cleaning time of coke deposits, and a bubble tower to improve fractionation of cracked vapors and increase gasoline yield. Refinery manager Edgar M. Clark took cracking to the next step by using tubes of about 100 millimeters diameter as the primary contact with the furnace, which increased cracking time and allowed pressures of about 6.8 atmospheres, and decreased maintenance time because of reduced coke deposition and fuel costs.
9. DETERGENTS
Detergents are cleaning agents used to remove foreign matter in suspension from soiled surfaces including human skin, textiles, hard surfaces in the home and metals in engineering. Technically called surfactants, detergents form a surface layer between immiscible phases, facilitating wetting by reducing surface tension. Detergent molecules have two parts, one of which is water-soluble (lyophobic) and the other oil or fat-soluble (lyophilic). They are adsorbed onto surfaces where they remove dirt by suspending it in foam. Some also act as biocides, but no single product does these things equally well. Special detergents also have many uses in industry and engineering. In the oil industry for example, surfactants are sometimes used to promote oil flow in porous rocks, or to flush out oil left behind by a water flood. In this case a band of detergent is put down before the water to create a low surface tension and thus allow the oil-bearing rock to be scrubbed clean. The water is often made viscous by adding a polymer to prevent it breaking through the surfactant layer. Detergents are also widely used in industrial flotation processes to separate lighter particles from a mixture with heavier materials.
10. DYES
The decade commencing in 1900 marked the end of half a century of remarkable inventiveness in synthetic dyestuffs that had started with William Perkin’s 1856 discovery of the aniline dye known as mauve. The products, derived mainly from coal tar hydrocarbons, included azo dyes, those containing the atomic grouping – N = N -, and artificial alizarin (1869–1870) and indigo (1897). By 1900, through intensive research and development, control of patents, and aggressive marketing, the industry was dominated by German manufacturers, such as BASF of Ludwigshafen, and Bayer, of Leverkusen. A new range of dyes based on anthraquinone (from which alizarin and congeners were made), and generally known as vat dyes, were the first major innovations in the twentieth century. Anthraquinone was obtained by oxidation of the three-ring aromatic hydrocarbon anthracene. Vat dyes are generally applied in reduced, soluble form; they then reoxidize to the original pigment and are extremely stable.
11. ELECTROCHEMISTRY
Electrochemistry deals with the relationship between chemical change and electricity. Under normal conditions, a chemical reaction is accompanied by the liberation or absorption of heat and not of any other form of energy. However, there are many so-called electrochemical reactions that when allowed to proceed in contact with two electronic conductors joined by conducting wires, will generate electrical energy in this external circuit. Current between the electrodes (usually metallic plates or rods) is carried by electrons, while in the electrolyte, a nonmetallic ionic compound either in the molten condition or in solution in water or other solvents, ions carry the current. Conversely, the energy of an electric current can be used to bring about many chemical reactions that do not occur spontaneously. The process in which electrical energy is directly converted into chemical energy is called electrolysis. The products of an electrolytic process have a tendency to react spontaneously with one another, reproducing the substances that were reactants and were therefore consumed during the electrolysis. If this reverse reaction is allowed to occur under proper conditions, a large proportion of the electrical energy used in the electrolysis can be regenerated. This possibility is used in accumulators or storage cells, sets of which are known as storage batteries.
12. ELECTROPHORESIS
Electrophoresis is a separation technique that involves the migration of charged colloidal particles in a liquid under the influence of an applied electric field. The word is derived from electro, referring to the energy of electricity, and phoresis, from the Greek verb phoros, meaning ‘‘to carry across.’’ Electrophoresis has many applications in analytical chemistry, particularly biochemistry. It is one of the staple tools in molecular biology and it is of critical value in many aspects of genetic manipulation, including DNA studies, and in forensic chemistry. Swedish biochemist Arne Tiselius carried out studies on proteins and colloids in the 1920s, and in 1930 introduced electrophoresis as a new technique for separating proteins in solution on the basis of their electrical charge. Tiselius was awarded the 1948 Nobel Prize in chemistry for this work, and the technique became a common tool in the 1940s and 1950s. Biological molecules such as amino acids, peptides, proteins, nucleotides, and nucleic acids, possess ionizable groups. At any given pH (concentration of hydrogen ions), these molecules exist in solution as electrically charged species either as cations (positive, or þ) or anions (negative, or ). Depending on the nature of the net charge, the charged particles will migrate either to the cathode or to the anode. For example, proteins in an electric field separate according to size, shape, and charge with charges contributed by the side chains of the amino acids composing the proteins. The charge of the protein depends on the hydrogen ion content of the surrounding buffer with a high ionic strength resulting in a greater charge.
13. ENVIRONMENTAL MONITORING
As a dynamic system, the environment is changing continually, with feedback from both natural (climatic or biogeochemical) and anthropogenic (human activities) sources. Assessing the rate and magnitude of environmental processes is difficult, especially as data collection over time is limited to the last century, or even the last few decades. Since the 1980s, environmental monitoring programs have been developed as a response to concerns that environmental impact or sustainability of policy initiatives could not be evaluated adequately. So-called State of the Environment Reports date from this period. Many are concerned with the state of national, regional, or local environments (land, rivers, or seas); others focus on environments at particular risk, mostly due to human impact and pollution (e.g., environmental contaminants in marine or terrestrial wildlife), or those where environmental quality is significant in the context of human health (e.g., urban air quality; water resources, fisheries). Many monitoring programs include information collected remotely by satellites (see Satellites, Environmental Sensing), but this entry focuses on technologies and policies for in situ monitoring. Adequate and sustained monitoring and its evaluation provides early warning of possible environmental degradation. Such information is important for the prediction of change that may be generated in the wake of a development project, such as dam construction or deforestation programs. In this context—as an element of an environmental impact assessment—monitored data are valuable for an evaluation of sustainability.
14. EXPLOSIVES
All chemical explosives obtain their energy from the almost instantaneous transformation from an inherently unstable chemical compound into more stable molecules. The breakthrough from the 2000- year old ‘‘black powder’’ to the high explosive of today was achieved with the discovery of the molecular explosive nitroglycerine, produced by nitrating glycerin with a mixture of strong nitric and sulfuric acids. Nitroglycerin, because of its extreme sensitivity and instability, remained a laboratory curiosity until Alfred Nobel solved the problem of how to safely and reliably initiate it with the discovery of the detonator in 1863, a discovery that has been hailed as key to both the principle and practice of explosives. Apart from the detonator, Nobel’s major contribution was the invention of dynamite in 1865. This invention tamed nitroglycerine by simply mixing it with an absorbent material called kieselguhr (diatomous earth) as 75 percent nitroglycerin and 25 percent kieselguhr. These two inventions were the basis for the twentieth century explosives industry. Explosives are ideally suited to provide high energy in airless conditions. For that reason explosives have played and will continue to play a vital role in the exploration of space.
15. FEEDSTOCKS
The word feedstock refers to the raw material consumed by the organic chemical industry. Sometimes, feedstock is given a more restricted meaning than raw material and thus applied to naphtha or ethylene, but not petroleum. The inorganic chemical industry also consumes raw materials, but the feedstock tends to be specific to the process in question, such as sulfur in the case of sulfuric acid. The development and growth of new feedstocks has driven the evolution of the organic chemical industry over the last two centuries. To a large extent, the history of this industry is the history of its feedstocks. Until the nineteenth century, the only significant raw material for the nascent organic chemical industry was fermentation- based ethanol (ethyl alcohol). Gradually, the products of wood distillation also became important, only to be overshadowed after 1860 by the coal-tar industry. As the organic chemical industry expanded in both size and scope between 1880 and 1930, the need for new feedstocks became urgent. The competition between coal and petroleum was resolved in favor of the latter in the late 1950s. The petrochemical industry has been phenomenally successful, underwriting the postwar boom in organic chemicals and plastics and weathering the oil crises of the 1970s with minimal damage. Its long-term sustainability remains an issue, and increasing attention is being paid to renewable feedstocks.
16. GREEN CHEMISTRY
The term ‘‘green chemistry,’’ coined in 1991 by Paul T. Anastas, is defined as ‘‘the design of chemical products and processes that reduce or eliminate the use and generation of hazardous substances.’’ This voluntary, nonregulatory approach to the protection of human health and the environment was a significant departure from the traditional methods previously used. While historically people tried to minimize exposure to chemicals, green chemistry emphasizes the design and creation of chemicals so that they do not possess intrinsic hazard. Within the definition of green chemistry, the word chemical refers to all materials and matter. Therefore, the application of green chemistry can affect all types of products, as well as the processes to make or use these products. Green chemistry has been applied to a wide range of industrial and consumer goods, including paints and dyes, fertilizers, pesticides, plastics, medicines, electronics, dry cleaning, energy generation, and water purification.
17. INDUSTRIAL GASES
While the gases that are now commonly referred to as ‘‘industrial,’’ namely oxygen, hydrogen, carbon dioxide, and nitrogen, were not fully understood until the nineteenth century, scientists in the twentieth century moved rapidly to utilize the knowledge. Driven largely by the demands of manufacturing industries in North America and Western Europe, rapid improvements in the technology of production and storage of industrial gases drove what has become a multibillion dollar business, valued at $34 billion in 2000. At the start of the twenty-first century, industrial gases underpin nearly every aspect of the global economy, from agriculture, welding, metal manufacturing and processing, refrigerants, enhanced oil recovery, food and beverage processing, electronic component manufacturing, to rocket propulsion. Oxygen for metal manufacturing is the largest volume market, with chemical processing and electronics using significant volumes of hydrogen and lower volumes of specialty gases such as argon. One of the byproducts of the expansive use of industrial gas is an increase in undesirable environmental pollutants—contributing to the ‘‘greenhouse effect’’ and an overabundance of nitrates from agriculture application. Subsequently, government controls worldwide have led the gas industry to revamp some of its distribution and application. Hydrogen is likely to be the gas of the future, employed in ‘‘green’’ fuel cell technology, and glass and steel manufacturers are reducing nitrous dioxide emissions by mixing oxygen with coal.
18. ISOTOPIC ANALYSIS
Beyond the analysis of the chemical elements in a sample of matter, it is possible to determine the isotopic content of the individual chemical elements. The chemical analysis of a substance generally takes its isotopic composition to be a ‘‘standard’’ that represents terrestrial composition, because for most purposes the isotopic ratios are more or less fixed, allowing chemical weight to be a useful laboratory parameter for most elements. Deviations from the standard composition occur because of differences in: 1. Nuclear synthesis 2. Radioactive decay 3. Geological, biological, and artificial fractionation 4. Exposure to various sources of radiation Applications of isotopic analysis make use of these sources of variation. Our knowledge results from analytical techniques developed during the twentieth century that allow precisions of 10–5 and that have led to significant improvements in the understanding of the Earth and solar system and even of archaeology.
19. NITROGEN FIXATION
In 1898, the British scientist William Crookes in his presidential address to the British Association for the Advancement of Science warned of an impending fertilizer crisis. The answer lay in the fixation of atmospheric nitrogen. Around 1900, industrial fixation with calcium carbide to produce cyanamide, the process of the German chemists Nikodemus Caro and Adolf Frank, was introduced. This process relied on inexpensive hydroelectricity, which is why the American Cyanamid Company was set up at Ontario, Canada, in 1907 to exploit the power of Niagara Falls. Electrochemical fixing of nitrogen as its monoxide was first realized in Norway, with the electric arc process of Kristian Birkeland and Samuel Eyde in 1903. The nitrogen monoxide formed nitrogen dioxide, which reacted with water to give nitric acid, which was then converted into the fertilizer calcium nitrate. The yield was low, and as with the Caro–Frank process, the method could be worked commercially only because of the availability of hydroelectricity.
In Germany, BASF of Ludwigshafen was interested in diversification into nitrogen fixation. From 1908, the company funded research into nitrogen fixation by Fritz Haber at the Karlsruhe Technische Hochschule. Haber specialized in the physical chemistry of gas reactions and drew on earlier studies started in 1903 on the catalytic formation of ammonia from its elements, nitrogen and hydrogen. He attacked the problem with high pressures, catalysts, and elevated temperatures. Even under optimum conditions the yield was low, around 5 percent, but Haber arranged for unreacted hydrogen and nitrogen to be recirculated. Though exothermic, the reaction was carried out at 600C in order to increase the rate. The preferred catalyst was either osmium or uranium. The main part of the apparatus was the furnace (later known as a converter) in which the gases were preheated by the outgoing reaction mixture. At a pressure of 200 atmospheres the gases were forced to react in the presence of the catalyst. Cooling moved the equilibrium in the direction of producing ammonia, which was liquefied and separated from unreacted hydrogen and nitrogen.
20. OIL FROM COAL PROCESS
The twentieth-century coal-to-petroleum or synthetic fuel industry evolved in three stages: 1. Invention and early development of Bergius coal liquefaction (hydrogenation) and Fischer–Tropsch (F–T) gas synthesis from 1910 to 1926. 2. Germany’s industrialization of the Bergius and F–T processes from 1927 to 1945. 3. Global transfer of the German technology to Britain, France, Japan, Canada, the U.S., and other nations from the 1930s to the 1990s. Petroleum had become essential to the economies of industrialized nations by the 1920s. The mass production of automobiles, the introduction of airplanes and petroleum-powered ships, and the recognition of petroleum’s high energy content compared to wood and coal, required a shift from solid to liquid fuels as a major energy source. Industrialized nations responded in different ways. Germany, Britain, Canada, France, Japan, Italy, and other nations, having little or no domestic petroleum, continued to import it. Germany, Japan, and Italy also acquired by force the petroleum resources of other nations during their 1930s–1940s World War II occupations in Europe and the Far East. In addition to sources of naturally occurring petroleum, Germany, Britain, France, and Canada in the 1920s–1940s synthesized petroleum from their domestic coal or bitumen resources, and during the 1930s–1940s war years Germany and Japan synthesized petroleum from the coal resources they seized from occupied nations. A much more favorable energy situation existed in the U.S., and it experienced few problems in making an energy shift from solid to liquid fuels because it possessed large resources of both petroleum and coal.
21. RADIOACTIVE DATING
There are natural radioactive isotopes that have half-lives comparable to the age of the earth, the most familiar being uranium-235 and -238(235U, 238U) and thorium-232 (232Th). The decay of these isotopes proceeds sequentially through intermediate products having much shorter half-lives to the stable isotopes of lead, 207Pb, 206Pb, and 208Pb, respectively. There are 14 other isotopes scattered through the periodic table that have half-lives spanning times from 1015 to 109 years. All can be used for dating geologic materials. There are other isotopes, termed cosmogenic isotopes, that are produced by cosmic rays and that have half-lives that are useful for measuring periods of historical interest. The best known of these is carbon-14 (14C), which has important uses in archaeology because of its half-life of 5760 years. Procedures for measuring isotope composition have the disadvantage of destroying the extracted portion of the sample used. The phenomena that allow nondestructive analysis rely on exciting optical radiation or x-radiation from the atoms of the sample, most commonly induced by electron bombardment. In these methods, isotopic effects are too small to be accurately observed.
22. REPPE CHEMISTRY
Reppe chemistry refers to a group of high-pressure reactions used in industry to make various organic chemicals. Most of these reactions were based on acetylene, and as acetylene declined in popularity (because of the cheapness of ethylene) in the 1960s, most of the Reppe reactions have also lost their significance. However, the formation of butanediol from acetylene has survived and is now one of the few acetylene-based processes used in the chemical industry. Walter Reppe who was working for the German firm of IG Farben at Ludwigshafen on the Rhine, discovered in 1932 that alcohols could be added to acetylene under considerable pressure to form the corresponding vinyl ether. The dangers of using acetylene were well known, and the success of this reaction was unexpected. The vinyl ethers were used as the starting point for the production of polyvinyl ethers, which were considered to be possible alternatives to polyvinyl chloride (PVC). Subsequently they turned out to have only limited applications, for instance, as a synthetic substitute for chewing gum.
23. SOLVENTS
Solvents are the hidden element in a broad range of technological activities, including chemical processes, paint, dry cleaning, and metal degreasing. They are used to dissolve organic compounds (water is the usual solvent for inorganic compounds) to enable reactions or polymerization, spreading or ease of use, or to extract compounds from a matrix such as plant material. Dry cleaning is a specialized form of the last group, as it removes fatty substances that adhere dirt to clothing. Many organic compounds either react with or do not dissolve in water, hence the need to find suitable organic chemicals to act as the solvent. It is hard to find compounds that dissolve a wide range of substances, but are also relatively cheap, nonflammable and nontoxic. In practice, the solvents used represent a compromise, often an unsatisfactory one.
The first solvent to be readily available was ethanol (ethyl alcohol) made by fermentation and distillation since the late Middle Ages. Amyl alcohol (fusel oil), a byproduct of ethanol manufacture, also became a popular solvent. Oil of turpenti"
10,"11-Military Technology Research Paper Topics
1. AIRCRAFT CARRIERS
Three nations built fleets of aircraft carriers— Britain, Japan and the United States—and each contributed to carrier design trends. Experiments began before World War I when, in November 1910, Eugene Ely flew a Curtiss biplane from a specially built forward deck of the cruiser USS Birmingham moored off Hampton Roads, Virginia. Two months later he accomplished the more difficult task of landing on a deck built over the stern of the cruiser Pennsylvania. Sandbags were used to anchor ropes stretched across the deck to help stop the airplane, which trailed a crude hook to catch the ropes.
The Enterprise, America’s first atomic-powered carrier, entered service in 1961 with a range of 320,000 kilometers, or capable of four years’ cruising. She was similar to the Forrestal carriers except for her small square island structure that originally featured ‘‘billboard’’ radar installations. Despite a huge cost increase (about 70 percent more than the Forrestals), she became the prototype for the ultimate Nimitz class of nuclear carriers that began to enter fleet service in the mid-1970s. Displacing nearly 95,000 tons, each had a crew of some 6,500 men. Driven by concerns about the growing expense of building and operating the huge American fleet carriers and their vulnerability, research into smaller carrier designs continued.
2. AIR-TO-AIR MISSILES
Interest in air-to-air missiles (AAMs, also known as air intercept missiles or AIMs) was initially prompted by the need to defend against heavy bombers in World War II. Unguided rockets were deployed for the purpose during the war, but the firing aircraft had to get dangerously close, and even so the rockets’ probability of approaching within killing range of their targets was poor. Nazi Germany developed two types of rocket-propelled missiles employing command guidance and produced some examples, but neither saw service use.
3. AIR-TO-SURFACE MISSILES
Precision attack of ground targets was envisioned as a major mission of air forces from their first conception, even before the advent of practicable airplanes. Until the 1970s most air forces believed that this could be best accomplished through exact aiming of cannon, unguided rockets, or freelyfalling bombs, at least for most targets. But although impressive results were sometimes achieved through these methods in tests and exercises, combat performance was generally disappointing, with average miss distances on the order of scores, hundreds, or even thousands of meters.
4. BATTLESHIPS
The battleship dates back to the final decade of the 19th century when the term came into general use in English for the most powerfully armed and armored surface warships. Material improvements allowed the construction of ships with high freeboard and good sea keeping capable of effectively fighting similar ships at sea, like the line of battleships of the sailing era. British battleships were the archetypes of the era. They displaced around 13,000 to 15,000 tons and their most useful armament was a battery of six 6-inch quick-firing guns on each side. These stood the best chance of successful hitting given the primitive fire control techniques of the day, although skilled gunnery officers might use them to gain the range for accurate shooting by the slow-firing 12-inch guns, two of which were mounted in covered barbette turrets (armored structures to protect the guns) at each end.
5. BIOLOGICAL WARFARE
In addition to the military use of natural or synthesized plant and animal toxins as poisons, biological warfare involves the use of disease-causing bacteria, viruses, rickettsia, or fungi to cause incapacitation or death in man, animals, or plants. Over the course of the twentieth century, biological weapons scientists, engineers, and physicians in various countries adopted existing technological and scientific practices, techniques, and instrumentation found in academic and industrial research to create a new weapon of mass destruction. Unlike the production of nuclear weapons, biological weapons research involves a synergistic relationship between the separate offensive and defensive components of each individual weapon system. Offensive research involves the identification, isolation, modification, and mass production of various pathogenic organisms and the creation of organismal delivery and storage systems. Offensive research is dependent in many cases upon the simultaneous success of a parallel defensive research program involving the creation of vaccines and protective health measures for researchers, military personnel, and civilians. In addition, defensive research involves the construction of accurate detection devices to indicate the existence of biological weapons whose presence can be masked during the initial phases of a natural epidemic.
6. BOMBER WARPLANES
Bombers apply aerospace technology to defeat an enemy through destruction of his will or ability to continue the conflict. In the twentieth century, the U.S. and the U.K found bombing particularly attractive because they were leaders in aerospace technology and disliked mobilizing large armies and suffering heavy casualties. Bombing requires aircraft that can carry sufficient bomb loads over great distances, penetrate enemy defenses, find targets in darkness and poor weather, and bomb accurately. Effective campaigns require adequate bases, trained personnel, fuel, munitions, replacement aircraft, spare parts, and the intelligence capability to select and assess damage to the proper targets.
7. CHEMICAL WARFARE
Popular fiction forecast the use of poison gas in warfare from the 1890s. While an effort was made to ban the wartime use of gas at The Hague International Peace Conference in 1899, military strategists and tacticians dismissed chemical weapons as a fanciful notion. The stalemate of World War I changed this mindset. Under Fritz Haber, a chemist at the Kaiser Wilhelm Institute, Germany’s chemical industry began making gas weapons. Compressed chlorine gas in 5730 cylinders was released against French Algerian and Canadian troops at Ypres, Belgium, on April 22, 1915. The gas attack resulted in approximately 3000 casualties, including some 800 deaths. Within months the British and French developed both gas agents of their own and protective gear, ensuring that chemical warfare would become a regular feature of the war. A variety of lethal and nonlethal chemical agents were developed in World War I. Lethal agents included the asphyxiating gases such as chlorine, phosgene, and diphosgene that drown their victims in mucous, choking off the supply of oxygen from the lungs. A second type were blood gases like hydrogen cyanide, which block the body’s ability to absorb oxygen from red corpuscles. Incapacitating gases included lachrymatorics (tear gases) and vesicants (blistering gases). The most notorious of these is mustard gas (Bis-[2- chloroethyl] sulphide), a blistering agent that produces horrible burns on the exposed skin and destroys mucous tissue and also persists on the soil for as long as 48 hours after its initial dispersion.
8. DEFENSIVE MISSILES
Missile defenses are complex systems composed of three major components: sensors to detect the launch of missiles and track them as they advance toward their targets, weapon systems to destroy the attacking missiles, and a command and control system that interconnects sensors and weapons. As a result of technological advances, these three components have evolved over the years since World War II, producing two major periods in the history of missile defense and suggesting the advent of a third by about 2025.
9. EXPLOSIVES
All chemical explosives obtain their energy from the almost instantaneous transformation from an inherently unstable chemical compound into more stable molecules. The breakthrough from the 2000- year old ‘‘black powder’’ to the high explosive of today was achieved with the discovery of the molecular explosive nitroglycerine, produced by nitrating glycerin with a mixture of strong nitric and sulfuric acids. Nitroglycerin, because of its extreme sensitivity and instability, remained a laboratory curiosity until Alfred Nobel solved the problem of how to safely and reliably initiate it with the discovery of the detonator in 1863, a discovery that has been hailed as key to both the principle and practice of explosives. Apart from the detonator, Nobel’s major contribution was the invention of dynamite in 1865. This invention tamed nitroglycerine by simply mixing it with an absorbent material called kieselguhr (diatomous earth) as 75 percent nitroglycerin and 25 percent kieselguhr. These two inventions were the basis for the twentieth century explosives industry. Explosives are ideally suited to provide high energy in airless conditions. For that reason explosives have played and will continue to play a vital role in the exploration of space.
10. FIGHTER AND FIGHTER BOMBER WARPLANES
Although new as weapons, fighters played an important role in World War I. Early in the war, reconnaissance planes and bombers were joined by fighters whose task it was to engage the enemy in aerial combat. Light machine guns were synchronized to fire through aircraft propellers. It was the German firm of Fokker which developed the first effective synchronizing device; this gave the Fokker planes, agile monoplanes, superiority over the Allies comparatively slow and less maneuverable biplanes. Aircraft development was then marked by a continuous catching-up process between German fighters on the one hand and French and British fighters on the other.
Research during World War II, especially in Germany, had shown that swept-back wings eased shockwave problems at high speeds. Important U.S. and Soviet aircraft developed shortly after the war, such as the Lockheed Sabre and MiG-15, had swept-back wings, and others adopted delta-wing layouts. Research and development in aerodynamics, structural engineering, materials science, and related fields led to the development of fighters and fighter–bombers with improved performance characteristics.
11. FISSION AND FUSION BOMBS
Fission weapons were developed first in the U.S., then in the Soviet Union, and later in Britain, France, China, India, and Pakistan. By the first decade of the twenty-first century, there were seven countries that announced that they had nuclear weapons, and several others suspected of developing them.
12. HIGH EXPLOSIVE SHELLS AND BOMBS
Among the most baleful of twentieth century technological accomplishments was the vast elaboration of the means for inflicting death and destruction in war. While nuclear and chemical weapons occasioned more revulsion, conventional high-explosive weapons wrought far wider harm. A revolution began in the nineteenth century with the introduction of rifled cannon and effective explosive shells. This, in turn, brought an escalating contest between weapons and protection both for fortifications and ships. At the beginning of the twentieth century, shells were beginning to move from black powder fill to modern high explosives such as ammonium picrate and trinitrotoluene (TNT). High-explosive (HE) shells needed steel walls thick enough to withstand the shock of firing, limiting weights of bursting charges to no more than about 25 percent of the whole. Depending on the target, they might use either point-detonating or time fuses. The early time fuses continued, as they had in the nineteenth century, to depend on the time taken for a powder train of precut length to burn to its end.
13. HIGH-FREQUENCY AND HIGH-POWER RADARS
While early radar designers were driven to frequencies of more than 1000 megahertz by considerations of the availability of high-power components, it was appreciated very early on that higher frequencies and thus shorter wavelengths would allow better precision. Frequency and wavelength are inversely related according to the equation
Wavelength = c/frequency
where c = velocity of light.
Radars operating in the high-frequency (HF) band (3 to 30 megahertz) may detect targets well beyond the nominal horizon through two mechanisms: ‘‘sky wave’’ and ‘‘surface wave.’’ Early in the century, it was discovered that high-frequency radio waves were strongly refracted by the ionosphere. A HF beam aimed near the horizon would, under suitable conditions, be effectively reflected, returning to sea level some hundreds to thousands of kilometers from its transmission site. From the 1940s, interest developed in using this sky-wave transmission phenomenon to provide surveillance at great ranges. Early HF over-the-horizon radars (OTHRs) were bistatic ‘‘forward scatter’’ systems in which a widely separated transmitter and receiver detected and tracked targets lying between them. Ballistic missile tracking was a major application.
14. LONG RANGE BALLISTIC MISSILES
During the 1960s, the U.S. and the Soviet Union began to develop and deploy long-range ballistic missiles, both intercontinental ballistic missiles (ICBMs) and intermediate-range ballistic missiles (IRBMs). The former would have ranges over 8000 kilometers, and the latter would be limited to about 2400 kilometers. The German V-2 rocket built during World War II represented a short- or medium-range ballistic missile. The efficiency and long range of these missiles derived from the fact that they required fuel only to be launched up through the atmosphere and directed towards the target. They used virtually no fuel traveling through near outer space. They were ‘‘ballistic’’ rather than guided in that they fell at their target after a ballistic arc, like a bullet.
15. LONG RANGE CRUISE MISSILES
A cruise missile is an air-breathing missile that can carry a high-explosive warhead or a weapon of mass destruction such as a nuclear warhead for an intermediate range of up to several hundred kilometers. When launched from the ground, such missiles are known as ground-launched cruise missiles (GLCMs). Some historians of weapons technology regard the German V-1 or ‘‘buzzbomb’’ operated in World War II, propelled with a ram-jet, air-breathing engine, as the first GLCM. The weapons do not require remote guidance, but automatically home in on pre-assigned targets, acting autonomously.
16. LONG RANGE RADARS AND EARLY WARNING SYSTEMS
During the 1930s, Great Britain was one of several countries, including most notably Germany and the U.S. that experimented with radar for early warning of air attacks. The British ‘‘Chain Home’’ system, designed by Sir Robert Watson-Watt and established by 1939, included a string of stations along the east and south coasts. By mid-1940, most of the stations featured two 73-meter wooden towers, one holding fixed transmitter aerials and the other receivers. When it was discovered that low-flying aircraft could slip undetected beneath the original fence, Britain created a second string of ‘‘Chain Home Low’’ stations, beginning with Truleigh Hill. The latter sites consisted of two separate aerials, one to transmit and the other to receive, mounted on 6-meter-high gantries and short enough to allow an operator inside the equipment hut beneath the gantry to manually rotate the arrays. Together, Chain Home and Chain Home Low provided a detection range of 40 to 190 kilometers depending on an incoming aircraft’s altitude. This early warning capability contributed immeasurably to the RAF victory over the Luftwaffe in the Battle of Britain.
17. MILITARY VERSUS CIVIL TECHNOLOGIES
The exchange of technical ideas between the military world and the civilian world can be found throughout the history of technology, from the defensive machines of Archimedes in Syracuse about 250 BC, through the first application of the telescope by Galileo in military and commercial intelligence, to the application of nuclear fission to both weaponry and power production. In the twentieth century, as the military establishments of the great powers sought to harness inventive capabilities, they turned to precedents in the commercial and academic world, seeking new ways to organize research and development. By the 1960s, the phrase ‘‘technology transfer’’ described the exchange of technique and device between civilian and military cultures, as well as between one nation and another, and provided a name for the phenomenon that had always characterized the development of tools, technique, process, and application.
18. NUCLEAR REACTORS AND WEAPONS MATERIAL
The first successful nuclear reactor, called an ‘‘atomic pile’’ because of its structure of graphite bricks, was completed and operational on December 2, 1942, in Chicago in the U.S. Although originally built to demonstrate a controlled nuclear reaction, the reactor was later dismantled and the depleted uranium removed in order to recover minute amounts of plutonium for use in a nuclear weapon. In effect, Chicago Pile- One (CP-1) was not only the world’s first nuclear reactor but also the world’s first reactor used to produce material for a nuclear weapon.
19. ORIGINS OF RADAR
Reflection was an important part of Heinrich Hertz’s 1887 demonstration of the existence of electromagnetic waves, and the idea of using that property to ‘‘see’’ in darkness or fog was developed shortly afterwards.
By the early 1930s, serious efforts were underway in the U.S., Germany, and Britain to construct radio-location devices using relatively long wavelengths. (Russian efforts were ahead in the early 1930s, but they yielded little as a result of serious organizational problems and purges that sent key engineers to the gulag.) The German company GEMA built the first device that can be called a functioning radar set in 1935 with Britain and America following only months behind. Two groups in the U.S.—the Signal Corps and the Naval Research Laboratories—proceeded independently but on lines very similar to those of the Germans in using dipole arrays. They had air-warning and searchlight-pointing prototype sets ready for production in 1939.
The British physicists Robert Watson Watt and Arnold Wilkins proceeded along a different line using wavelengths of tens of meters with broadcast rather than ‘‘searchlight’’ transmission. This equipment, although inferior to that working on shorter wavelengths, was seen by Air Vice-Marshal Hugh Dowding as the key to the air defense of Britain from expected German attack. As commander of the newly created Fighter Command, he created a system of radar stations and ground observers linked by secure telephone lines to the fighter units. He drilled Fighter Command to use the new technique, and when the Luftwaffe came in the summer of 1940, the attacking squadrons were ambushed by defending fighters positioned by radar.
20. RADAR ABOARD AIRCRAFT
As with much else in radar, airborne radar sets were first developed during World War II, and most of the modern uses for such sets were explored during that war. While airborne radar shares much in common with surface and naval sets, there are many factors involved that make airborne installations very different from either of the latter.
At the beginning of the 21st century, combat aircraft continue to use radar for the same purposes as in World War II: navigation, air and surface search, and targeting. Using computers and guided munitions, they could also automatically release bombs or launch missiles at the appropriate time. The big difference between 1945 and 2018 is that most, if not all, of these functions can be done by a single aircraft carrying a single radar with a range and resolution much greater than any airborne set used during the war.
21. RADAR DISPLAYS
Those who first conceived of radar early in the century often envisioned systems that would simply indicate, perhaps by sounding a buzzer or lighting a lamp, that a target had been detected and where it was located. Those who first reduced radar to practice in the 1930s, however, were radio scientists who knew that the returning radar signals would somehow have to be distinguished against a background of radio-frequency interference and noise. They were accustomed to displaying signals visually on cathode ray tube (CRT) oscilloscopes, and they naturally turned to such means for radar. This made the operator an essential part of the radar system, responsible for the final stages of the detection process and extraction of target data.
22. RADAR SYSTEMS IN WORLD WAR II
With the onset of war in September 1939, Britain, Germany, and the U.S. had advanced radar designs while France, Russia, The Netherlands, Italy, and Japan had little of value in comparison although they had made research efforts along those lines. Of these endeavors, only Britain had proceeded past the prototype stage into a state of war readiness in the form of the Chain Home air defense. Germany had technically the best radar designs, but the Wehrmacht intended to wage a war of aggression and initially gave little support to a technology whose strength lay overwhelmingly in defense. In the U.S., because of the contentious battleship–bomber disputes of the 1920s, the Navy had pressed for any new technical method to defend ships against air attack, and the Army had sought to perfect its anti-aircraft artillery with methods of combating bombers at night.
23. RECONNAISSANCE WARPLANES
After World War I and during World War II, technological effort was aimed at putting the camera at higher altitudes, theoretically out of the ability of the enemy to reach and destroy it, and to further increase its operational effectiveness by allowing it to operate in the dark. This led to development of electrical heating apparatus that prevented camera shutters from being adversely affected by the cold at high altitudes and to the slit camera that adjusted the speed at which film was fed through the camera to the speed of the aircraft, an advance that improved the production of maps of enemy territory. Nighttime operations were aided by aerial flash equipment designed by Harold Edgerton of the Massachusetts Institute of Technology, which provided not only well-lit scenery, but also frozen imagery of the target, an asset vital to effective bomb sighting.
With the opening of the Cold War, the mindset that kept pushing reconnaissance to increasingly high altitudes and greater speeds took on a new importance as it not only kept the camera out of the enemy’s physical reach, but out of his legal and political reach as well. The Royal Air Force’s first jet bomber, the Canberra, counted on both speed and altitude to keep it away from enemy fighters. These advantages were to prove useful to reconnaissance as well and the Canberra still serves in the RAF inventory. The quest for speed and height led ultimately to the two best-known Cold War reconnaissance aircraft, the U-2 and the SR-71. Capable of cruising at 740 kilometers per hour (km/h), with a range of 3540 kilometers, and a ceiling of 17,000 meters. (21,000 meters and above in the later models), the U-2 represented the cutting edge in aerial intelligence gathering until it was superceded by the faster and higher flying SR-71. The Blackbird pushed the altitude envelope to over 26,000 meters and was able to maintain speeds of Mach 3.2.
24. SHORT RANGE AND GUIDED MISSILES
With most nations surrounding their military capabilities with considerable secrecy, different published range figures and guidance types sometimes contradicted each other. Furthermore, the distinction between an intermediate-range missile (up to about 2400 kilometers) and an intercontinental- or long-range missile was a matter of definition over which there was never complete agreement. Some publications would include submarine- launched missiles up to intermediate range in short- and medium-range ballistic missile listings. Although the missiles listed here were generally capable of carrying a nuclear warhead, most were also loaded with conventional explosive warheads.
25. SONAR
The word ‘‘sonar’’ originated in the U.S. Navy during World War II as an acronym for ‘‘SOund NAvigation and Ranging,’’ which referred to the systematic use of sound waves, transmitted or reflected, to determine water depths as well as to detect and locate submerged objects. Until it adopted that term in 1963, the British Admiralty had used ‘‘ASDIC,’’ an abbreviation for the Anti- Submarine Detection Investigation Committee that led the effort among British, French, and American scientists during World War I to locate submarines and icebergs using acoustic echoes. American shipbuilder Lewis Nixon invented the first sonar-type device in 1906. Physicist Karl Alexander Behm in Kiel, Germany, disturbed by the Titanic disaster of April 1912, invented an echo depth sounder for iceberg detection in July 1913. Although developed and improved primarily for military purposes in World War I, sonar devices became useful in such fields as oceanography and medical practice (e.g., ultrasound).
26. SUBMARINES
The basic technology of the submarine is quite simple and has remained constant since its inception. The boat submerges by taking on water through vents to decrease its buoyancy and surfaces by expelling the water with compressed air. The outward appearance of the military submarine has remained remarkably constant throughout its modern development—a cigar-shaped hull topped by the immediately recognizable conning tower with a periscope for viewing the surface.
We can break submarine technology into five categories:
▪	Propulsion
▪	Hull design
▪	Weaponry
▪	Stealthiness
▪	Ancillary technologies.
27. SURFACE-TO-AIR AND ANTI-BALLISTIC MISSILES
In World War II, when Japanese kamikaze aircraft showed the amount of damage that could be inflicted with a single explosive-laden plane, it became apparent that machine gun and antiaircraft fire were insufficient protection against current and future weapons. The answer was to combine radar detection, guided rockets, and the proximity fuse into surface-to-air missiles or SAMs. Intensive development in the postwar years produced the Sea Sparrow in the 1950s as one of the first, successful SAMS. When identifying a Warsaw Pact weapon as a surface-to-air missile, NATO forces would assign it an ‘‘SA’’ or surface– air, number.
28. TANKS
Despite some curiosity on the part of a handful of other nations, the development of the tank in the twentieth century was largely a British affair. Yet even Britain did not intentionally set out to develop it. Like many things, the tank was the result of other technologies being developed as well as a response to the dangers some of those very technologies presented. Although tanks were plagued with problems of power, protection, and a lack of vision about their use at the beginning of the 20th century, and despite the massive advances in weapons of all kinds during the century, at the beginning of the 21st century the tank remained a vital instrument of warfare.
WARFARE AND MILITARY TECHNOLOGY IN THE 20TH CENTURY
 Twentieth-century warfare begins with World War I (1914–1918) even though this conflict had more in common with wars of the previous century than it did with those that followed. The Great War opened with maneuvers by huge field armies that culminated in frontal assaults by masses of infantry. After only a few months of mobile warfare, heavy casualties forced opposing armies to take shelter in trench systems that stretched all across France. Faced with the ensuing stalemate of the trenches, both sides adopted an attrition strategy that would defeat the opposition by bleeding his manpower and depleting his material resources. The strategy finally succeeded when an exhausted Germany surrendered in November 1918.
In spite of its similarity to nineteenth-century warfare, World War I witnessed several new developments, most notably the airplane, the tank, and the truck. Between 1919 and 1939, the implications of these new developments were worked out, producing new operational approaches that transformed warfare.
During World War II (1939–1945), European land warfare was dominated by mobile armored forces that swept back and forth across the continent. While armies fought on the ground, air forces contended for control of European skies. In this massive air war, Allied bombers devastated Germany’s industrial base and population centers.
Meanwhile, in the Pacific region, the war centered on aircraft carrier task forces that battled each other and supported amphibious operations. The war started with Japan conquering much of the western Pacific, only to be pushed back by superior Allied arms and forced to surrender when American B-29 bombers dropped the only two atomic bombs ever used in war.
By the time World War II ended in the Pacific, Japan’s military resources had been severely reduced by Allied military actions. The reduction of Japanese resources, along with the progressive weakening of Germany in the European theater, suggest that World War II, like the first, was an attrition war in which industrial capacity was as important as military forces.
Two of the most revolutionary developments of World War II were the atomic bomb and the long-range ballistic missile. When more fully developed and mated to each other during the Cold War (1946–1991), they became what is perhaps the most revolutionary weapon in military history, the nuclear-tipped, intercontinental-range ballistic missile (ICBM). In the end, the Cold War was another attrition conflict, ending with the economic exhaustion and collapse of the Soviet Empire.
The end of the Cold War reduced the tensions that had kept nuclear strike forces on hair-trigger alert since the 1950s. Although nuclear weapons still existed, relations between the U.S. and the Russian Federation that emerged from the defunct Soviet Union were no longer based on mutually assured destruction (MAD), as both sides reduced their nuclear forces and the U.S. continued developing missile defenses.
While there were a number of significant ‘‘limited’’ wars during the twentieth century, the five major episodes described above unleashed the greatest national energies. These energies were molded into major new military systems through the process of command technology that is rooted in England of the 1880s according to historian William McNeill. Before this time, weapons were either developed in government-owned arsenals or by private entrepreneur inventors. A major change began in 1886 when the British Admiralty, dissatisfied with the performance of the government arsenal at Woolwich, started contracting with private arms makers for the development of new weapons. Under this approach, the Admiralty established the specifications for a new weapon and effectively challenged the contractor to produce it. This contracting system marks the beginning of command technology. Tantamount to invention on demand, this process of state-sponsored research and development spread throughout the West, becoming the dominant paradigm for weapons acquisition by 1945.
One product of command technology during World War I was the tank, which was developed by the British to cross fire-swept terrain between the trenches and breech the German defenses. While the tank proved capable of completing its mission, its successes were limited due to technical limitations and a lack of understanding of how best to use the new weapon.
The principal enabling technology for the tank was the internal combustion engine, which also powered World War I trucks and airplanes. The former improved logistics by connecting troops in forward positions with railheads and supply depots in the rear. The latter opened an entirely new realm of warfare and, over the course of the war, suggested all the missions the airplane would perform in future wars.
Building on the lessons of World War I, air power advocates used the period between the two world wars to develop a rigorous body of air power doctrine. At the same time, the world’s leading powers developed aircraft of increasing capabilities to execute the missions defined in their doctrines.
The U.S. emphasized long-range bombers to execute daylight, precision bombardment—the dominant doctrine in America’s air force. England also developed bombers, but she also pursued fighter development because of the threat posed by the air force of a rearming Germany. In addition to bombers, Germany developed tactical aircraft to support its new approach to ground warfare—Blitzkrieg.
The basic ideas behind Blitzkrieg had emerged by the end of World War I, as the capabilities of tanks and aircraft improved. After the war, the Germans developed these ideas further and mated them to the panzer division, which included tan"
11,"Information overload is the difficulty in understanding an issue and effectively making decisions when one has too much information about that issue.
The quality of a decision is decreased when a decision-maker is given many sets of information.
Modern information technology has brought information overload, which is associated with over-exposure, excessive viewing of information and input abundance of information and data.
The concept of information overload is not new. Simmel observed that city dwellers become jaded due to an overload of sensations in the modern urban world.
People can process 7 chunks of information at a time, and can become confused if they receive too much information.
The concept of ""information overload"" was introduced by Diderot long before the term ""information overload"" was coined.
The term ""information glut"" has evolved into phrases such as ""information smog"", ""data glut"", and ""information glut"" in the internet age.
The production of information, especially books, has been documented in the past. The 3rd or 4th century BC saw the writer comment that of making books there is no end, and the 1st century AD saw Seneca the Elder comment on the abundance of books.
The printing press caused the creation of an overload of information by lowering production costs and making ancient texts available to the average person.
Following Gutenberg's invention, the introduction of mass printing was introduced in Western Europe. Information became readily available and accessible, allowing the educated to purchase books.
Although scholars were initially elated by the number of books that were readily available, they also complained about information overload, and asked, ""Is there anywhere on earth exempt from these swarms of new books?""
In 1795 German bookseller Johann Georg Heinzmann expressed concerns about the rise of books in Europe.
Scholars recorded information in paper slips for easier access to information. This system influenced the development of the library card catalog and the index card.
In his book The Information: A History, A Theory, A Flood, author James Gleick discusses how information theory was created to bridge mathematics, engineering, and computing.
In the Information Age, people are bombarded with information, and technology is changing to serve our social culture.
Information technology in today's society exacerbates the number of interruptions, which result in more poor decisions.
As the world moves into a new era of globalization, more people are connecting to the Internet to conduct their own research, and viewing information. This has created the possibility for the spread of misinformation.
In the 21st century, information overload is the feeling that mankind is being drowned by the waves of data coming at it. IT corporate management implements training to improve the productivity of knowledge workers, but information overload is the feeling of being burdened, stressed, and overwhelmed.
At the Web 2.0 Expo in New York in 2008, Clay Shirky spoke about the effects of information overload on productivity and decision-making, as well as the contamination of useful information with inaccurate information.
The causes of information overload include a rapid rate of new information being produced, a continuous news culture, an increase in available channels of incoming information, a lack of comparison and processing methods, and a lack of a signal-to-noise ratio.
E-mail is a major source of information overload, and email attachments are a major problem.
A December 2007 New York Times blog post described e-mail as a $650 billion drag on the economy.
In January 2011, Eve Tahmincioglu wrote an article titled It's Time to Deal With That Overflowing Inbox. Using action and reference folders to manage email is more efficient than just replying to every email.
Nicholas Carr, former executive editor of the Harvard Business Review and author of the book The Shallows, has stated that email causes people to become addicted to searching for new information. He also stated that a lack of information can make learning harder.
The World Wide Web provides users with access to billions of pages of information, including search engines to help them manage their own research, but the information may not always be reliable.
Viktor Mayer-Schönberger argues that everyone can be a participant on the internet, and that we can no longer control the information we share with others.
Social media applications and websites can add to information overload by presenting many different views and outlooks on subject matters. The problem is that information overload may not be the core reason for people's anxieties about the amount of information they receive in their daily lives.
Researchers believe that information overload can result in searchers being less systematic and searchers being less able to search effectively.
Filtering and withdrawal are common responses to information, with the former avoiding information overload and the latter limiting the number of sources of information with which one interacts.
There are two general approaches to mitigate information overload: reduce the amount of incoming information, or enhance the ability to process information.
Johnson advises people to stop using their phones as alarm clocks and to stop receiving notifications.
Information overload can be reduced by using Internet applications and add-ons such as the Inbox Pause add-on for Gmail.
A study done by Humboldt University showed that students took several strategies to deal with IO while using Facebook.
When performing complex tasks, decision makers have little extra capacity, and can lose information. When the number of distractions/interruptions increases, performance deteriorates more severely.
Some cognitive scientists and graphic designers have emphasized the distinction between information in a form that can be used in thinking and raw information, suggesting that information overload may actually be an issue of inadequate organization.
In a study by Soucek and Moser (2010), training on information overload had a positive impact on employees.
Researchers suggest that an attention economy will emerge from information overload, allowing users to control their online experience. This could be implemented by attaching a small fee to e-mail messages.
Lincoln suggests a holistic approach to the study of IO that takes into account many different factors that contribute to the effect of IO.
It is impossible for a person to read all the academic papers published in their field, so systematic reviews are published.
Information pollution, interruption overload, analysis paralysis and memory loss are all symptoms of information overload."
12,"Data science
Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data,[1][2] and apply knowledge and actionable insights from data across a broad range of application domains. 
Data science is related to data mining, machine learning and big data.

Data science is a ""concept to unify statistics, data analysis, informatics, and their related methods"" in order to ""understand and analyse actual phenomena"" with data.[3] It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge.
However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a ""fourth paradigm"" of science (empirical, theoretical, computational, and now data-driven) and asserted that ""everything about science is changing because of the impact of information technology"" and the data deluge.
[5][6] A data scientist is someone who creates programming code and combines it with statistical knowledge to create insights from data.

Foundations
Data science is an interdisciplinary field focused on extracting knowledge from data sets, which are typically large (see big data), and applying the knowledge and actionable insights from data to solve problems in a wide range of application domains.
[8] The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, data visualization, information visualization, data sonification, data integration, graphic design, complex systems, communication and business.[9][10]
 Statistician Nathan Yau, drawing on Ben Fry, also links data science to human–computer interaction: users should be able to intuitively control and explore data.[11][12] In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.[13]

Relationship to statistics
Many statisticians, including Nate Silver, have argued that data science is not a new field, but rather another name for statistics.[14] Others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data.[15] Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g. images) and emphasizes prediction and action.[16]
 Andrew Gelman of Columbia University has described statistics as a nonessential part of data science.[17] Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing, and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics.[18]
 In summary, data science can be therefore described as an applied branch of statistics.

Etymology
Early usage
In 1962, John Tukey described a field he called ""data analysis"", which resembles modern data science.[18] In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C. F. Jeff Wu used the term ""data science"" for the first time as an alternative name for statistics.[19] Later, attendees at a 1992 statistics symposium at the University of Montpellier II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.[20][21]

The term ""data science"" has been traced back to 1974, when Peter Naur proposed it as an alternative name for computer science.[22] In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic.[22] However, the definition was still in flux. After the 1985 lecture in the Chinese Academy of Sciences in Beijing, in 1997 C. F. Jeff Wu again suggested that statistics should be renamed data science.
 He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting, or limited to describing data.[23] In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.[21]

During the 1990s, popular terms for the process of finding patterns in datasets (which were increasingly large) included ""knowledge discovery"" and ""data mining"".[24][22]

Modern usage
The modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland.[25] In a 2001 paper, he advocated an expansion of statistics beyond theory into technical areas; because this would significantly change the field, it warranted a new name.[24] ""Data science"" became more widely used in the next few years: in 2002, the Committee on Data for Science and Technology launched Data Science Journal. In 2003, Columbia University launched The Journal of Data Science.[24] In 2014, the American Statistical Association's Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science.[26]

The professional title of ""data scientist"" has been attributed to DJ Patil and Jeff Hammerbacher in 2008.[27] Though it was used by the National Science Board in their 2005 report ""Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century"", it referred broadly to any key role in managing a digital data collection.[28]

There is still no consensus on the definition of data science, and it is considered by some to be a buzzword.[29] Big data is a related marketing term.[30] Data scientists are responsible for breaking down big data into usable information and creating software and algorithms that help companies and organizations determine optimal operations."
13,"Artificial intelligence
Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by animals including humans. AI research has been defined as the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals.[a]

The term ""artificial intelligence"" had previously been used to describe machines that mimic and display ""human"" cognitive skills that are associated with the human mind, such as ""learning"" and ""problem-solving"". This definition has since been rejected by major AI researchers who now describe AI in terms of rationality and acting rationally, which does not limit how intelligence can be articulated.[b]

AI applications include advanced web search engines (e.g., Google), recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Tesla), automated decision-making and competing at the highest level in strategic game systems (such as chess and Go).[2][citation needed] As machines become increasingly capable, tasks considered to require ""intelligence"" are often removed from the definition of AI, a phenomenon known as the AI effect.[3] For instance, optical character recognition is frequently excluded from things considered to be AI,[4] having become a routine technology.[5]

Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism,[6][7] followed by disappointment and the loss of funding (known as an ""AI winter""),[8][9] followed by new approaches, success and renewed funding.[7][10] AI research has tried and discarded many different approaches since its founding, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge and imitating animal behavior. In the first decades of the 21st century, highly mathematical-statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.[11][10]

The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects.[c] General intelligence (the ability to solve an arbitrary problem) is among the field's long-term goals.[12] To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques—including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, probability and economics. AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields.

The field was founded on the assumption that human intelligence ""can be so precisely described that a machine can be made to simulate it"".[d] This raised philosophical arguments about the mind and the ethical consequences of creating artificial beings endowed with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity.[14] Science fiction writers and futurologists have since suggested that AI may become an existential risk to humanity if its rational capacities are not overseen.[15][16]


History
Artificial beings with intelligence appeared as storytelling devices in antiquity,[17] and have been common in fiction, as in Mary Shelley's Frankenstein or Karel Čapek's R.U.R.[18] These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence.[19]

The study of mechanical or ""formal"" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as ""0"" and ""1"", could simulate any conceivable act of mathematical deduction. This insight that digital computers can simulate any process of formal reasoning is known as the Church–Turing thesis.[20]

The Church-Turing thesis, along with concurrent discoveries in neurobiology, information theory and cybernetics, led researchers to consider the possibility of building an electronic brain.[21] The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete ""artificial neurons"".[22]

By the 1950s, two visions for how to achieve machine intelligence emerged. One vision, known Symbolic AI or GOFAI, was to use computers to create a symbolic representation of the world and systems that could reason about the world. Proponents included Allen Newell, Herbert A. Simon, and Marvin Minsky. Closely associated with this approach was the ""heuristic search"" approach, which likened intelligence to a problem of exploring a space of possibilities for answers. The second vision, known as the connectionist approach, sought to achieve intelligence through learning. Proponents of this approach, most prominently Frank Rosenblatt, sought to connect Perceptron in ways inspired by connections of neurons.[23] James Manyika and others have compared the two approaches to the mind (Symbolic AI) and the brain (connectionist). Manyika argues that symbolic approaches dominated the push for artificial intelligence in this period, due in part to its connection to intellectual traditions of Descarte, Boole, Gottlob Frege, Bertrand Russell, and others. Connectionist approaches based on cybernetics or artificial neural networks were pushed to the background but have gained new prominence in recent decades.[24]

The field of AI research was born at a workshop at Dartmouth College in 1956.[e][27] The attendees became the founders and leaders of AI research.[f] They and their students produced programs that the press described as ""astonishing"":[g] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[h][29] By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense[30] and laboratories had been established around the world.[31]

Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.[32] Herbert Simon predicted, ""machines will be capable, within twenty years, of doing any work a man can do"".[33] Marvin Minsky agreed, writing, ""within a generation ... the problem of creating 'artificial intelligence' will substantially be solved"".[34]

They failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill[35] and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an ""AI winter"", a period when obtaining funding for AI projects was difficult. [8]

In the early 1980s, AI research was revived by the commercial success of expert systems,[36] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S and British governments to restore funding for academic research.[7] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[9]

Many researchers began to doubt that the symbolic approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into ""sub-symbolic"" approaches to specific AI problems.[37] Robotics researchers, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move, survive, and learn their environment.[i] Interest in neural networks and ""connectionism"" was revived by Geoffrey Hinton, David Rumelhart and others in the middle of the 1980s.[42] Soft computing tools were developed in the 80s, such as neural networks, fuzzy systems, Grey system theory, evolutionary computation and many tools drawn from statistics or mathematical optimization.

AI gradually restored its reputation in the late 1990s and early 21st century by finding specific solutions to specific problems. The narrow focus allowed researchers to produce verifiable results, exploit more mathematical methods, and collaborate with other fields (such as statistics, economics and mathematics).[43] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as ""artificial intelligence"".[11]

Faster computers, algorithmic improvements, and access to large amounts of data enabled advances in machine learning and perception; data-hungry deep learning methods started to dominate accuracy benchmarks around 2012.[44] According to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a ""sporadic usage"" in 2012 to more than 2,700 projects.[j] He attributes this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets.[10] In a 2017 survey, one in five companies reported they had ""incorporated AI in some offerings or processes"".[45] The amount of research into AI (measured by total publications) increased by 50% in the years 2015–2019.[46]

Numerous academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines. Much of current research involves statistical AI, which is overwhelmingly used to solve specific problems, even highly successful techniques such as deep learning. This concern has led to the subfield of artificial general intelligence (or ""AGI""), which had several well-funded institutions by the 2010s.[12]

Goals
The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.[c]

Reasoning, problem-solving
Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[47] By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.[48]

Many of these algorithms proved to be insufficient for solving large reasoning problems because they experienced a ""combinatorial explosion"": they became exponentially slower as the problems grew larger.[49] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[50]

Knowledge representation
Main articles: Knowledge representation, Commonsense knowledge, Description logic, and Ontology

An ontology represents knowledge as a set of concepts within a domain and the relationships between those concepts.
Knowledge representation and knowledge engineering[51] allow AI programs to answer questions intelligently and make deductions about real-world facts.

A representation of ""what exists"" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them.[52] The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge and act as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). A truly intelligent program would also need access to commonsense knowledge; the set of facts that an average person knows. The semantics of an ontology is typically represented in description logic, such as the Web Ontology Language.[53]

AI research has developed tools to represent specific domains, such as objects, properties, categories and relations between objects;[53] situations, events, states and time;[54] causes and effects;[55] knowledge about knowledge (what we know about what other people know);.[56] default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); [57] as well as other domains. Among the most difficult problems in AI are: the breadth of commonsense knowledge (the number of atomic facts that the average person knows is enormous);[58] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as ""facts"" or ""statements"" that they could express verbally).[50]

Formal knowledge representations are used in content-based indexing and retrieval,[59] scene interpretation,[60] clinical decision support,[61] knowledge discovery (mining ""interesting"" and actionable inferences from large databases),[62] and other areas.[63]

Planning
Main article: Automated planning and scheduling
An intelligent agent that can plan makes a representation of the state of the world, makes predictions about how their actions will change it and make choices that maximize the utility (or ""value"") of the available choices.[64] In classical planning problems, the agent can assume that it is the only system acting in the world, allowing the agent to be certain of the consequences of its actions.[65] However, if the agent is not the only actor, then it requires that the agent reason under uncertainty, and continuously re-assess its environment and adapt.[66] Multi-agent planning uses the cooperation and competition of many agents to achieve a given goal. Emergent behavior such as this is used by evolutionary algorithms and swarm intelligence.[67]

Learning"
14,"Main article: Machine learning
Machine learning (ML), a fundamental concept of AI research since the field's inception,[k] is the study of computer algorithms that improve automatically through experience.[l]

Unsupervised learning finds patterns in a stream of input. Supervised learning requires a human to label the input data first, and comes in two main varieties: classification and numerical regression. Classification is used to determine what category something belongs in—the program sees a number of examples of things from several categories and will learn to classify new inputs. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. Both classifiers and regression learners can be viewed as ""function approximators"" trying to learn an unknown (possibly implicit) function; for example, a spam classifier can be viewed as learning a function that maps from the text of an email to one of two categories, ""spam"" or ""not spam"".[71] In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent classifies its responses to form a strategy for operating in its problem space.[72] Transfer learning is when the knowledge gained from one problem is applied to a new problem.[73]

Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[74]

Natural language processing
Natural language processing (NLP)[75] allows machines to read and understand human language. A sufficiently powerful natural language processing system would enable natural-language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of NLP include information retrieval, question answering and machine translation.[76]

Symbolic AI used formal syntax to translate the deep structure of sentences into logic. This failed to produce useful applications, due to the intractability of logic[49] and the breadth of commonsense knowledge.[58] Modern statistical techniques include co-occurrence frequencies (how often one word appears near another), ""Keyword spotting"" (searching for a particular word to retrieve information), transformer-based deep learning (which finds patterns in text), and others.[77] They have achieved acceptable accuracy at the page or paragraph level, and, by 2019, could generate coherent text.[78]

Perception
Main articles: Machine perception, Computer vision, and Speech recognition

Feature detection (pictured: edge detection) helps AI compose informative abstract structures out of raw data.
Machine perception[79] is the ability to use input from sensors (such as cameras, microphones, wireless signals, and active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Applications include speech recognition,[80] facial recognition, and object recognition.[81]

Computer vision is the ability to analyze visual input.[82]"
15,"Motion and manipulation
Main article: Robotics
AI is heavily used in robotics.[83] Localization is how a robot knows its location and maps its environment. When given a small, static, and visible environment, this is easy; however, dynamic environments, such as (in endoscopy) the interior of a patient's breathing body, pose a greater challenge.[84]

Motion planning is the process of breaking down a movement task into ""primitives"" such as individual joint movements. Such movement often involves compliant motion, a process where movement requires maintaining physical contact with an object. Robots can learn from experience how to move efficiently despite the presence of friction and gear slippage.[85]


Social intelligence
Affective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process or simulate human feeling, emotion and mood.[87] For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction. However, this tends to give naïve users an unrealistic conception of how intelligent existing computer agents actually are.[88]

Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis), wherein AI classifies the affects displayed by a videotaped subject.[89]

General intelligence"
16,"Main article: Artificial general intelligence
A machine with general intelligence can solve a wide variety of problems with breadth and versatility similar to human intelligence. There are several competing ideas about how to develop artificial general intelligence. Hans Moravec and Marvin Minsky argue that work in different individual domains can be incorporated into an advanced multi-agent system or cognitive architecture with general intelligence.[90] Pedro Domingos hopes that there is a conceptually straightforward, but mathematically difficult, ""master algorithm"" that could lead to AGI.[91] Others believe that anthropomorphic features like an artificial brain[92] or simulated child development[m] will someday reach a critical point where general intelligence emerges.

Tools
Search and optimization
Main articles: Search algorithm, Mathematical optimization, and Evolutionary computation
Many problems in AI can be solved theoretically by intelligently searching through many possible solutions:[93] Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule.[94] Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[95] Robotics algorithms for moving limbs and grasping objects use local searches in configuration space.[96]

Simple exhaustive searches[97] are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use ""heuristics"" or ""rules of thumb"" that prioritize choices in favor of those more likely to reach a goal and to do so in a shorter number of steps. In some search methodologies, heuristics can also serve to eliminate some choices unlikely to lead to a goal (called ""pruning the search tree""). Heuristics supply the program with a ""best guess"" for the path on which the solution lies.[98] Heuristics limit the search for solutions into a smaller sample size.[99]


A particle swarm seeking the global minimum
A very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other related optimization algorithms include random optimization, beam search and metaheuristics like simulated annealing.[100] Evolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Classic evolutionary algorithms include genetic algorithms, gene expression programming, and genetic programming.[101] Alternatively, distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[102]

Logic"
17,"Main articles: Logic programming and Automated reasoning
Logic[103] is used for knowledge representation and problem-solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning[104] and inductive logic programming is a method for learning.[105]

Several different forms of logic are used in AI research. Propositional logic[106] involves truth functions such as ""or"" and ""not"". First-order logic[107] adds quantifiers and predicates and can express facts about objects, their properties, and their relations with each other. Fuzzy logic assigns a ""degree of truth"" (between 0 and 1) to vague statements such as ""Alice is old"" (or rich, or tall, or hungry), that are too linguistically imprecise to be completely true or false.[108] Default logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem.[57] Several extensions of logic have been designed to handle specific domains of knowledge, such as description logics;[53] situation calculus, event calculus and fluent calculus (for representing events and time);[54] causal calculus;[55] belief calculus (belief revision); and modal logics.[56] Logics to model contradictory or inconsistent statements arising in multi-agent systems have also been designed, such as paraconsistent logics.[citation needed]

Probabilistic methods for uncertain reasoning
Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.[109] Bayesian networks[110] are a very general tool that can be used for various problems, including reasoning (using the Bayesian inference algorithm),[n][112] learning (using the expectation-maximization algorithm),[o][114] planning (using decision networks)[115] and perception (using dynamic Bayesian networks).[116] Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[116]

A key concept from the science of economics is ""utility"", a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[117] and information value theory.[118] These tools include models such as Markov decision processes,[119] dynamic decision networks,[116] game theory and mechanism design.[120]

Classifiers and statistical learning methods
Main articles: Classifier (mathematics), Statistical classification, and Machine learning
The simplest AI applications can be divided into two types: classifiers (""if shiny then diamond"") and controllers (""if diamond then pick up""). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine the closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class is a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[121]

A classifier can be trained in various ways; there are many statistical and machine learning approaches. The decision tree is the simplest and most widely used symbolic machine learning algorithm.[122] K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s.[123] Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[124] The naive Bayes classifier is reportedly the ""most widely used learner""[125] at Google, due in part to its scalability.[126] Neural networks are also used for classification.[127]

Classifier performance depends greatly on the characteristics of the data to be classified, such as the dataset size, distribution of samples across classes, dimensionality, and the level of noise. Model-based classifiers perform well if the assumed model is an extremely good fit for the actual data. Otherwise, if no matching model is available, and if accuracy (rather than speed or scalability) is the sole concern, conventional wisdom is that discriminative classifiers (especially SVM) tend to be more accurate than model-based classifiers such as ""naive Bayes"" on most practical data sets.[128]"
18,"Artificial neural networks
Main articles: Artificial neural network and Connectionism

A neural network is an interconnected group of nodes, akin to the vast network of neurons in the human brain.
Neural networks[127] were inspired by the architecture of neurons in the human brain. A simple ""neuron"" N accepts input from other neurons, each of which, when activated (or ""fired""), casts a weighted ""vote"" for or against whether neuron N should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed ""fire together, wire together"") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes.

Modern neural networks model complex relationships between inputs and outputs and find patterns in data. They can learn continuous functions and even digital logical operations. Neural networks can be viewed as a type of mathematical optimization — they perform gradient descent on a multi-dimensional topology that was created by training the network. The most common training technique is the backpropagation algorithm.[129] Other learning techniques for neural networks are Hebbian learning (""fire together, wire together""), GMDH or competitive learning.[130]

The main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks.[131]"
19,"Deep learning
Representing Images on Multiple Layers of Abstraction in Deep Learning
Representing images on multiple layers of abstraction in deep learning[132]
Deep learning[133] uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.[134] Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification[135] and others.

Deep learning often uses convolutional neural networks for many or all of its layers. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. This can substantially reduce the number of weighted connections between neurons,[136] and creates a hierarchy similar to the organization of the animal visual cortex.[137]

In a recurrent neural network the signal will propagate through a layer more than once;[138] thus, an RNN is an example of deep learning.[139] RNNs can be trained by gradient descent,[140] however long-term gradients which are back-propagated can ""vanish"" (that is, they can tend to zero) or ""explode"" (that is, they can tend to infinity), known as the vanishing gradient problem.[141] The long short term memory (LSTM) technique can prevent this in most cases.[142]

Specialized languages and hardware
Main articles: Programming languages for artificial intelligence and Hardware for artificial intelligence
Specialized languages for artificial intelligence have been developed, such as Lisp, Prolog, TensorFlow and many others. Hardware developed for AI includes AI accelerators and neuromorphic computing."
20,"Applications
AI is relevant to any intellectual task.[143] Modern artificial intelligence techniques are pervasive and are too numerous to list here.[144] Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.[145]

In the 2010s, AI applications were at the heart of the most commercially successful areas of computing, and have become a ubiquitous feature of daily life. AI is used in search engines (such as Google Search), targeting online advertisements,[146][non-primary source needed] recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic,[147][148] targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa),[149] autonomous vehicles (including drones and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace), image labeling (used by Facebook, Apple's iPhoto and TikTok) and spam filtering.

There are also thousands of successful AI applications used to solve problems for specific industries or institutions. A few examples are energy storage,[150] deepfakes,[151] medical diagnosis, military logistics, or supply chain management.

Game playing has been a test of AI's strength since the 1950s. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.[152] In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[153] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps.[154] Other programs handle imperfect-information games; such as for poker at a superhuman level, Pluribus[p] and Cepheus.[156] DeepMind in the 2010s developed a ""generalized artificial intelligence"" that could learn many diverse Atari games on its own.[157]

By 2020, Natural Language Processing systems such as the enormous GPT-3 (then by far the largest artificial neural network) were matching human performance on pre-existing benchmarks, albeit without the system attaining a commonsense understanding of the contents of the benchmarks.[158] DeepMind's AlphaFold 2 (2020) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[159] Other applications predict the result of judicial decisions,[160] create art (such as poetry or painting) and prove mathematical theorems.

In 2019, WIPO reported that AI was the most prolific emerging technology in terms of number of patent applications and granted patents, the Internet of things was estimated to be the largest in terms of market size. It was followed, again in market size, by big data technologies, robotics, AI, 3D printing and the fifth generation of mobile services (5G).[161] Since AI emerged in the 1950s, 340000 AI-related patent applications were filed by innovators and 1.6 million scientific papers have been published by researchers, with the majority of all AI-related patent filings published since 2013. Companies represent 26 out of the top 30 AI patent applicants, with universities or public research organizations accounting for the remaining four.[162] The ratio of scientific papers to inventions has significantly decreased from 8:1 in 2010 to 3:1 in 2016, which is attributed to be indicative of a shift from theoretical research to the use of AI technologies in commercial products and services. Machine learning is the dominant AI technique disclosed in patents and is included in more than one-third of all identified inventions (134777 machine learning patents filed for a total of 167038 AI patents filed in 2016), with computer vision being the most popular functional application. AI-related patents not only disclose AI techniques and applications, they often also refer to an application field or industry. Twenty application fields were identified in 2016 and included, in order of magnitude: telecommunications (15 percent), transportation (15 percent), life and medical sciences (12 percent), and personal devices, computing and human–computer interaction (11 percent). Other sectors included banking, entertainment, security, industry and manufacturing, agriculture, and networks (including social networks, smart cities and the Internet of things). IBM has the largest portfolio of AI patents with 8,290 patent applications, followed by Microsoft with 5,930 patent applications.[162]"
21,"Philosophy
Main article: Philosophy of artificial intelligence
Defining artificial intelligence
Thinking vs. acting: the Turing test
Main articles: Turing test, Dartmouth Workshop, and Synthetic intelligence
Alan Turing wrote in 1950 ""I propose to consider the question 'can machines think'?""[163] He advised changing the question from whether a machine ""thinks"", to ""whether or not it is possible for machinery to show intelligent behaviour"".[164] The only thing visible is the behavior of the machine, so it does not matter if the machine is conscious, or has a mind, or whether the intelligence is merely a ""simulation"" and not ""the real thing"". He noted that we also don't know these things about other people, but that we extend a ""polite convention"" that they are actually ""thinking"". This idea forms the basis of the Turing test.[165][q]"
22,"Acting humanly vs. acting intelligently: intelligent agents
Main article: Intelligent agents
AI founder John McCarthy said: ""Artificial intelligence is not, by definition, simulation of human intelligence"".[167] Russell and Norvig agree and criticize the Turing test. They wrote: ""Aeronautical engineering texts do not define the goal of their field as 'making machines that fly so exactly like pigeons that they can fool other pigeons.'""[166] Other researchers and analysts disagree and have argued that AI should simulate natural intelligence by studying psychology or neurobiology.[r]

The intelligent agent paradigm[169] defines intelligent behavior in general, without reference to human beings. An intelligent agent is a system that perceives its environment and takes actions that maximize its chances of success. Any system that has goal-directed behavior can be analyzed as an intelligent agent: something as simple as a thermostat, as complex as a human being, as well as large systems such as firms, biomes or nations. The intelligent agent paradigm became widely accepted during the 1990s, and currently serves as the definition of the field.[a]

The paradigm has other advantages for AI. It provides a reliable and scientific way to test programs; researchers can directly compare or even combine different approaches to isolated problems, by asking which agent is best at maximizing a given ""goal function"". It also gives them a common language to communicate with other fields — such as mathematical optimization (which is defined in terms of ""goals"") or economics (which uses the same definition of a ""rational agent"").[170]

Evaluating approaches to AI
No established unifying theory or paradigm has guided AI research for most of its history.[s] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term ""artificial intelligence"" to mean ""machine learning with neural networks""). This approach is mostly sub-symbolic, neat, soft and narrow (see below). Critics argue that these questions may have to be revisited by future generations of AI researchers."
23,"Symbolic AI and its limits
Main articles: Symbolic AI, Physical symbol systems hypothesis, Moravec's paradox, and Dreyfus' critique of artificial intelligence
Symbolic AI (or ""GOFAI"")[172] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at ""intelligent"" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: ""A physical symbol system has the necessary and sufficient means of general intelligent action.""[173]

However, the symbolic approach failed dismally on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level ""intelligent"" tasks were easy for AI, but low level ""instinctive"" tasks were extremely difficult.[174] Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a ""feel"" for the situation, rather than explicit symbolic knowledge.[175] Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree.[t][50]

The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[177][178] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision."
24,"Neat vs. scruffy
Main article: Neats and scruffies
""Neats"" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). ""Scruffies"" expect that it necessarily requires solving a large number of unrelated problems. This issue was actively discussed in the 70s and 80s,[179] but in the 1990s mathematical methods and solid scientific standards became the norm, a transition that Russell and Norvig termed ""the victory of the neats"".[180]"
25,"Soft vs. hard computing
Main article: Soft computing
Finding a provably correct or optimal solution is intractable for many important problems.[49] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 80s and most successful AI programs in the 21st century are examples of soft computing with neural networks."
26,"Narrow vs. general AI
Main article: Artificial general intelligence
AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence (general AI) directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals[181][182] General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focussing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively."
27,"Machine consciousness, sentience and mind
Main articles: Philosophy of artificial intelligence and Artificial Consciousness
The philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field. Stuart Russell and Peter Norvig observe that most AI researchers ""don't care about the [philosophy of AI] — as long as the program works, they don't care whether you call it a simulation of intelligence or real intelligence.""[183] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction."
28,"Consciousness
Main articles: Hard problem of consciousness and Theory of mind
David Chalmers identified two problems in understanding the mind, which he named the ""hard"" and ""easy"" problems of consciousness.[184] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all. Human information processing is easy to explain, however, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.[185]"
29,"Computationalism and functionalism
Main articles: Computationalism, Functionalism (philosophy of mind), and Chinese room
Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[186]

Philosopher John Searle characterized this position as ""strong AI"": ""The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.""[u] Searle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.[189]"
30,"Robot rights
Main article: Robot rights
If a machine has a mind and subjective experience, then it may also have sentience (the ability to feel), and if so, then it could also suffer, and thus it would be entitled to certain rights.[190] Any hypothetical robot rights would lie on a spectrum with animal rights and human rights.[191] This issue has been considered in fiction for centuries,[192] and is now being considered by, for example, California's Institute for the Future, however, critics argue that the discussion is premature.[193]

"
31,"Future Superintelligence
Main articles: Superintelligence, Technological singularity, and Transhumanism
A superintelligence, hyperintelligence, or superhuman intelligence, is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. Superintelligence may also refer to the form or degree of intelligence possessed by such an agent.[182]

If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement.[194] Its intelligence would increase exponentially in an intelligence explosion and could dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario the ""singularity"".[195] Because it is difficult or impossible to know the limits of intelligence or the capabilities of superintelligent machines, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.[196]

Robot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.[197]

Edward Fredkin argues that ""artificial intelligence is the next stage in evolution"", an idea first proposed by Samuel Butler's ""Darwin among the Machines"" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.[198]
"
32,"Risks Technological unemployment
Main articles: Workplace impact of artificial intelligence and Technological unemployment
In the past technology has tended to increase rather than reduce total employment, but economists acknowledge that ""we're in uncharted territory"" with AI.[199] A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[200] Subjective estimates of the risk vary widely; for example, Michael Osborne and Carl Benedikt Frey estimate 47% of U.S. jobs are at ""high risk"" of potential automation, while an OECD report classifies only 9% of U.S. jobs as ""high risk"".[v][202]

Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist states that ""the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution"" is ""worth taking seriously"".[203] Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[204]

Bad actors and weaponized AI
Main articles: Lethal autonomous weapon and Artificial intelligence arms race
AI provides a number of tools that are particularly useful for authoritarian governments: smart spyware, face recognition and voice recognition allow widespread surveillance; such surveillance allows machine learning to classify potential enemies of the state and can prevent them from hiding; recommendation systems can precisely target propaganda and misinformation for maximum effect; deepfakes aid in producing misinformation; advanced AI can make centralized decision making more competitive with liberal and decentralized systems such as markets.[205]

Terrorists, criminals and rogue states may use other forms of weaponized AI such as advanced digital warfare and lethal autonomous weapons. By 2015, over fifty countries were reported to be researching battlefield robots.[206]

Machine-learning AI is also able to design tens of thousands of toxic molecules in a matter of hours.[207]

Algorithmic bias
Main article: Algorithmic bias
AI programs can become biased after learning from real-world data. It is not typically introduced by the system designers but is learned by the program, and thus the programmers are often unaware that the bias exists.[208] Bias can be inadvertently introduced by the way training data is selected.[209] It can also emerge from correlations: AI is used to classify individuals into groups and then make predictions assuming that the individual will resemble other members of the group. In some cases, this assumption may be unfair.[210] An example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the COMPAS-assigned recidivism risk level of black defendants is far more likely to be overestimated than that of white defendants, despite the fact that the program was not told the races of the defendants.[211] Other examples where algorithmic bias can lead to unfair outcomes are when AI is used for credit rating or hiring."
33,"Existential risk
Main articles: Existential risk from artificial general intelligence and Superintelligence
Superintelligent AI may be able to improve itself to the point that humans could not control it. This could, as physicist Stephen Hawking puts it, ""spell the end of the human race"".[212] Philosopher Nick Bostrom argues that sufficiently intelligent AI if it chooses actions based on achieving some goal, will exhibit convergent behavior such as acquiring resources or protecting itself from being shut down. If this AI's goals do not fully reflect humanity's, it might need to harm humanity to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal. He concludes that AI poses a risk to mankind, however humble or ""friendly"" its stated goals might be.[213] Political scientist Charles T. Rubin argues that ""any sufficiently advanced benevolence may be indistinguishable from malevolence."" Humans should not assume machines or robots would treat us favorably because there is no a priori reason to believe that they would share our system of morality.[214]

The opinion of experts and industry insiders is mixed, with sizable fractions both concerned and unconcerned by risk from eventual superhumanly-capable AI.[215] Stephen Hawking, Microsoft founder Bill Gates, history professor Yuval Noah Harari, and SpaceX founder Elon Musk have all expressed serious misgivings about the future of AI.[216] Prominent tech titans including Peter Thiel (Amazon Web Services) and Musk have committed more than $1 billion to nonprofit companies that champion responsible AI development, such as OpenAI and the Future of Life Institute.[217] Mark Zuckerberg (CEO, Facebook) has said that artificial intelligence is helpful in its current form and will continue to assist humans.[218] Other experts argue is that the risks are far enough in the future to not be worth researching, or that humans will be valuable from the perspective of a superintelligent machine.[219] Rodney Brooks, in particular, has said that ""malevolent"" AI is still centuries away.[w]"
34,"Ethical machines
Main articles: Machine ethics, Friendly AI, Artificial moral agents, and Human Compatible
Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.[221]

Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.[222] Machine ethics is also called machine morality, computational ethics or computational morality,[222] and was founded at an AAAI symposium in 2005.[223]

Other approaches include Wendell Wallach's ""artificial moral agents""[224] and Stuart J. Russell's three principles for developing provably beneficial machines.[225]"
35,"Regulation
Main articles: Regulation of artificial intelligence, Regulation of algorithms, and AI control problem
The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms.[226] The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.[227] Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[46] Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, USA and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[46] The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[46] Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.[228]

In fiction
Thought-capable artificial beings have appeared as storytelling devices since antiquity,[17] and have been a persistent theme in science fiction.[19]

A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[229]

Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the ""Multivac"" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics;[230] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[231]

Transhumanism (the merging of humans and machines) is explored in the manga Ghost in the Shell and the science-fiction series Dune.

Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[232]"
36,"Scientific Diplomacy
Warfare
As technology and research evolve and the world enters the third revolution of warfare following gunpowder and nuclear weapons, the artificial intelligence arms race ensues between the United States, China, and Russia, three countries with the world’s top five highest military budgets.[233] Intentions of being a world leader in AI research by 2030[234]have been declared by China’s leader Xi Jinping, and President Putin of Russia has stated that “Whoever becomes the leader in this sphere will become the ruler of the world”[235].If Russia were to become the leader in AI research, President Putin has stated Russia’s intent to share some of their research with the world so as to not monopolize the field,[235] similar to their current sharing of nuclear technologies, maintaining science diplomacy relations. The United States, China, and Russia, are some examples of countries that have taken their stances toward military artificial intelligence since as early as 2014, having established military programs to develop cyber weapons, control lethal autonomous weapons, and drones that can be used for surveillance.

Russo-Ukrainian War
President Putin announced that artificial intelligence is the future for all mankind [235] and recognizes the power and opportunities that the development and deployment of lethal autonomous weapons AI technology can hold in warfare and homeland security, as well as its threats. President Putin’s prediction that future wars will be fought using AI has started to come to fruition to an extent after Russia invaded Ukraine on February 24, 2022.  The Ukrainian military is making use of the Turkish Bayraktar TB2-drones[236] that still require human operation to deploy laser-guided bombs but can take off, land, and cruise autonomously. Ukraine has also been using Switchblade drones supplied by the US and receiving information gathering by the United States’s own surveillance operations regarding battlefield intelligence and national security about Russia[237]. Similarly, Russia can use AI to help analyze battlefield data from surveillance footage taken by drones. Reports and images show that Russia’s military has deployed KUB- BLA suicide drones [238] into Ukraine, with speculations of intentions to assassinate Ukrainian President Volodymyr Zelenskyy.

Warfare Regulations
As research in the AI realm progresses, there is pushback about the use of AI from the Campaign to Stop Killer Robots and world technology leaders have sent a petition[239] to the United Nations calling for new regulations on the development and use of AI technologies in 2017, including a ban on the use of lethal autonomous weapons due to ethical concerns for innocent civilian populations.

Cybersecurity
With the ever evolving cyber-attacks and generation of devices, AI can be used for threat detection and more effective response by risk prioritization. With this tool, some challenges are also presented such as privacy, informed consent, and responsible use[240][241]. According to CISA, the cyberspace is difficult to secure for the following factors: the ability of malicious actors to operate from anywhere in the world, the linkages between cyberspace and physical systems, and the difficulty of reducing vulnerabilities and consequences in complex cyber networks[242]. With the increased technological advances of the world, the risk for wide scale consequential events rises. Paradoxically, the ability to protect information and create a line of communication between the scientific and diplomatic community thrives. The role of cybersecurity in diplomacy has become increasingly relevant, creating the term of cyber diplomacy - which is not uniformly defined and not synonymous with cyber defence[243]. Many nations have developed unique approaches to scientific diplomacy in cyberspace.

Czech Republic’s Approach
Dating back to 2011, when the Czech National Security Authority (NSA) was appointed as the national authority for the cyber agenda. The role of cyber diplomacy strengthened in 2017 when the Czech Ministry of Foreign Affairs (MFA) detected a serious cyber campaign directed against its own computer networks[244]. In 2016, three cyber diplomats were deployed to Washington, D.C., Brussels and Tel Aviv, with the goal of establishing active international cooperation focused on engagement with the EU and NATO. The main agenda for these scientific diplomacy efforts is to bolster research on artificial intelligence and how it can be utilized in cybersecurity research, development, and overall consumer trust[245]. CzechInvest is a key stakeholder in scientific diplomacy and cybersecurity. For example, in September 2018, they organized a mission to Canada in September 2018 with a special focus on artificial intelligence. The main goal of this particular mission was a promotional effort on behalf of Prague, attempting to establish it as a future knowledge hub for the industry for interested Canadian firms[246].

Germany's Approach
Cybersecurity is recognized as a governmental task, dividing into three ministries of responsibility: the Federal Ministry of the Interior, the Federal Ministry of Defence, and the Federal Foreign Office[247]. These distinctions promoted the creation of various institutions, such as The German National Office for Information Security, The National Cyberdefence Centre, The German National Cyber Security Council, and The Cyber and Information Domain Service[245]. In 2018, a new strategy for artificial intelligence was established by the German government, with the creation of a German-French virtual research and innovation network[248], holding opportunity for research expansion into cybersecurity.

European Union's Approach
The adoption of The Cybersecurity Strategy of the European Union — An Open, Safe and Secure Cyberspace document in 2013 by the European commission[245] pushed forth cybersecurity efforts integrated with scientific diplomacy and artificial intelligence. Efforts are strong, as the EU funds various programs and institutions in the effort to bring science to diplomacy and bring diplomacy to science. Some examples are the cyber security programme Competence Research Innovation (CONCORDIA), which brings together 14 member states[249], Cybersecurity for Europe (CSE)- which brings together 43 partners involving 20 member states[250]. In addition, The European Network of Cybersecurity Centres and Competence Hub for Innovation and Operations (ECHO) gathers 30 partners with 15 member states[251] and SPARTA gathers 44 partners involving 14 member states[252]. These efforts reflect the overall goals of the EU, to innovate cybersecurity for defense and protection, establish a highly integrated cyberspace among many nations, and further contribute to the security of artificial intelligence[245]."
37,"Russo-Ukrainian War
With the invasion of Ukraine, there has been a rise in malicious cyber activity against the United States[253], Ukraine, and Russia. A prominent and rare documented use of artificial intelligence in conflict is on behalf of Ukraine, using facial recognition software to uncover Russian assailants and identify Ukrainians killed in the ongoing war[254]. Though these governmental figures are not primarily focused on scientific and cyber diplomacy, other institutions are commenting on the use of artificial intelligence in cybersecurity with that focus. For example, Georgetown University’s Center for Security and Emerging Technology (CSET) has the Cyber-AI Project, with one goal being to attract policymakers’ attention to the growing body of academic research, whichexposes the exploitive consequences of AI and machine-learning (ML) algorithms[255]. This vulnerability can be a plausible explanation as to why Russia is not engaging in the use of AI in conflict per, Andrew Lohn, a senior fellow at CSET. In addition to use on the battlefield, AI is being used by the Pentagon to analyze data from the war, analyzing to strengthen cybersecurity and warfare intelligence for the United States[237][256].

Election Security
As artificial intelligence grows and the overwhelming amount of news portrayed through cyberspace expands, it is becoming extremely overwhelming for a voter to know what to believe. There are many intelligent codes, referred to as bots, written to portray people on social media with the goal of spreading miss information[257]. The 2016 USA election is a victim of such actions. During the Hillary Clinton and Donald Trump campaign, artificial intelligent bots from Russia were spreading miss information about the candidates in order to help the Trump campaign[258]. Analysts concluded that approximately 19% of Twitter tweets centered around the 2016 election were detected to come from bots[258]. Youtube in recent years has been used to spread political information as well. Although there is no proof that the platform attempts to manipulate its viewers opinions, Youtubes AI algorithm recommends videos of similar variety[259]. If a person begins to research right wing political podcasts, then Youtube's algorithm will recommend more right wing videos[260]. The uprising in a program called Deepfake, a software used to replicate someone's face and words, has also shown its potential threat. In 2018 a Deepfake video of Barack Obama was released saying words he claims to have never said[261]. While in a national election a Deepfake will quickly be debunked, the software has the capability to heavily sway a smaller local election. This tool holds a lot of potential for spreading misinformation and is monitored with great attention[262]. Although it may be seen as a tool used for harm, AI can help enhance election campaigns as well. AI bots can be programed to target articles with known misinformation. The bots can then indicate what is being misinformed to help shine light on the truth. AI can also be used to inform a person where each parts stands on a certain topic such as healthcare or climate change[263]. The political leaders of a nation have heavy sway on international affairs. Thus, a political leader with a lack of interest for international collaborative scientific advancement can have a negative impact in the scientific diplomacy of that nation[264]

Future of Work
Facial Recognition
The use of Artificial Intelligence (AI) has subtly grown to become part of everyday life. It is used every day in facial recognition software. It is the first measure of security for many companies in the form of a biometric authentication. This means of authentication allows even the most official organizations such as the United States Internal Revenue Service to verify a person’s identity [265] via a database generated from machine learning. As of the year 2022, the United States IRS requires those who do not undergo a live interview with an agent to complete a biometric verification of their identity via ID.me’s facial recognition tool. [265]

AI and School
In Japan and South Korea, artificial intelligence software is used in the instruction of English language via the company Riiid. [266] Riiid is a Korean education company working alongside Japan to give students the means to learn and use their English communication skills via engaging with artificial intelligence in a live chat.[266] Riid is not the only company to do this. An American company such as Duolingo is very well known for their automated teaching of 41 languages. Babbel, a German language learning program also uses artificial intelligence in its teaching automation, allowing for European students to learn vital communication skills needed in social, economic, and diplomatic settings.  Artificial intelligence will also automate the routine tasks that teachers need to do such as grading, taking attendance, and handling routine student inquiries. [267] This enables the teacher to carry on with the complexities of teaching that an automated machine cannot handle. These include creating exams, explaining complex material in a way that will benefit students individually and handling unique questions from students.

AI and Medicine
Unlike the human brain, which possess generalized intelligence, the specialized intelligence of AI can serve as a means of support to physicians internationally. The medical field has a diverse and profound amount of data in which AI can employ to generate a predictive diagnosis. Researchers at an Oxford hospital have developed artificial intelligence that can diagnose heart scans for heart disease and cancer. [268] This artificial intelligence can pick up diminutive details in the scans that doctors may miss. As such, artificial intelligence in medicine will better the industry, giving doctors the means to precisely diagnose their patients using the tools available. The artificial intelligence algorithms will also be used to further improve diagnosis over time, via an application of machine learning called precision medicine[269]. Furthermore, the narrow application of artificial intelligence can use “deep learning” in order to improve medical image analysis. In radiology imaging, AI uses deep learning algorithms to identify potentially cancerous lesions which is an important process assisting in early diagnosis. [270]

AI in Business
Data analysis is a fundamental property of artificial intelligence that enables it to be used in every facet of life from search results to the way people buy product. According to NewVantage Partners [271], over 90% of top businesses have ongoing investments in artificial intelligence. According to IBM, one of the world’s leaders in technology, 45% of respondents from companies with over 1,000 employees have adopted AI [272]. Recent data shows that the business market [273] for artificial intelligence during the year 2020 was valued at $51.08 billion. The business market for artificial intelligence is projected to be over $640.3 billion by the year  2028. [273]

Business and Diplomacy
With the exponential surge of artificial technology and communication, the distribution of one’s ideals and values has been evident in daily life. Digital information is spread via communication apps such as Whatsapp, Facebook/Meta, Snapchat, Instagram and Twitter. However, it is known that these sites relay specific information corresponding to data analysis. If a right-winged individual were to do a google search, Google’s algorithms would target that individual and relay data pertinent to that target audience. US President Bill Clinton noted in 2000:""In the new century, liberty will spread by cell phone and cable modem. [...] We know how much the Internet has changed America, and we are already an open society. [274] However, when the private sector uses artificial intelligence to gather data, a shift in power from the state to the private sector may be seen. This shift in power, specifically in large technological corporations, could profoundly change how diplomacy functions in society. The rise in digital technology and usage of artificial technology enabled the private sector to gather immense data on the public, which is then further categorized by race, location, age, gender, etc. [275]The New York Times calculates that ""the ten largest tech firms, which have become gatekeepers in commerce, finance, entertainment and communications, now have a combined market capitalization of more than $10 trillion. In gross domestic product terms, that would rank them as the world’s third-largest economy.” [276]Beyond the general lobbying of congressmen/congresswomen, companies such as Facebook/Meta or Google use collected data in order to reach their intended audiences with targeted information. [276]

AI and Foreign Policy
Multiple nations around the globe employ Artificial Intelligence to assist with their foreign policy decisions. The Chinese Department of External Security Affairs – under the Ministry of Foreign Affairs - uses AI to review almost all its foreign investment projects for risk mitigation[277]. The government of China plans to utilize Artificial Intelligence in its $900 Billion global infrastructure development plan, called the “Belt and Road Initiative” for political, economic, and environmental risk alleviation[278].

Over 200 applications of Artificial Intelligence are being used by over 46 United Nations agencies, in sectors ranging from health care dealing with issues such as combating covid 19 to smart agriculture, to assist the UN in political and diplomatic relations[279]. One example is the use of AI by the UN Global Pulse program to model the effect of the spread of Covid 19 on internally Displace People (IDP) and refugee settlements to assist them in creating an appropriate global health policy. [280],[281]

Novel AI tools such as remote sensing can also be employed by diplomats for collecting and analyzing data and near-real-time tracking of objects such as troop or refuge movements along borders in violent conflict zones[282],[283].

Artificial Intelligence can be used to mitigate vital cross-national diplomatic talks to prevent translation errors caused by human translators[284]. A major example is the 2021 Anchorage meetings held between US and China aimed at stabilizing foreign relations, only for it to have the opposite effect, increasing tension and aggressiveness between the two nations, due to translation errors caused by human translators[285]. In the meeting, when United States National Security Advisor to President Joe Biden, Jacob Jeremiah Sullivan stated, “We do not seek conflict, but we welcome stiff competition and we will always stand up for our principles, for our people, and for our friends”, it was mistranslated into Chinese as “we will face competition between us, and will present our stance in a very clear manner”, adding an aggressive tone to the speech[286]. AI’s ability for fast and efficient Natural language processing and real-time translation/transliteration makes it an important tool for foreign-policy communication between nations and prevents unintended mistranslation[287]. "
38,"Deep learning 
Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.[2]

Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5]

Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analogue.[6][7]

The adjective ""deep"" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can. Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability, whence the ""structured"" part.
"
39,"Definition
Deep learning is a class of machine learning algorithms that[8]: 199–200  uses multiple layers to progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.

Overview
Most modern deep learning models are based on artificial neural networks, specifically convolutional neural networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.[9]

In deep learning, each level learns to transform its input data into a slightly more abstract and composite representation. In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place in which level on its own. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.[10][11]

The word ""deep"" in ""deep learning"" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.[12] No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than 2. CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function.[13] Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.

Deep learning architectures can be constructed with a greedy layer-by-layer method.[14] Deep learning helps to disentangle these abstractions and pick out which features improve performance.[10]

For supervised learning tasks, deep learning methods eliminate feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.

Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.[10][15]

Interpretations
Deep neural networks are generally interpreted in terms of the universal approximation theorem[16][17][18][19][20] or probabilistic inference.[8][10][12][21]

The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.[16][17][18][19] In 1989, the first proof was published by George Cybenko for sigmoid activation functions[16] and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.[17] Recent work also showed that universal approximation also holds for non-bounded activation functions such as the rectified linear unit.[22]

The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al.[20] proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; If the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.

The probabilistic interpretation[21] derives from the field of machine learning. It features inference,[8][9][10][12][15][21] as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.[21] The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.[23]

History
Some sources point out that Frank Rosenblatt developed and explored all of the basic ingredients of the deep learning systems of today.[24] He described it in his book ""Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms"", published by Cornell Aeronautical Laboratory, Inc., Cornell University in 1962.

The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967.[25] A 1971 paper described a deep network with eight layers trained by the group method of data handling.[26] Other deep learning working architectures, specifically those built for computer vision, began with the Neocognitron introduced by Kunihiko Fukushima in 1980.[27]

The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986,[28] and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.[29][30]

In 1989, Yann LeCun et al. applied the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since 1970,[31][32][33][34] to a deep neural network with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days.[35]

In 1994, André de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.[36]

In 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton.[37] Many factors contribute to the slow speed, including the vanishing gradient problem analyzed in 1991 by Sepp Hochreiter.[38][39]

Since 1997, Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid[40] by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities.

Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of artificial neural network's (ANN) computational cost and a lack of understanding of how the brain wires its biological networks.

Both shallow and deep learning (e.g., recurrent nets) of ANNs have been explored for many years.[41][42][43] These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.[44] Key difficulties have been analyzed, including gradient diminishing[38] and weak temporal correlation structure in neural predictive models.[45][46] Additional difficulties were the lack of training data and limited computing power.

Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI studied deep neural networks in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation.[47] The SRI deep neural network was then deployed in the Nuance Verifier, representing the first major industrial application of deep learning.[48]

The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s,[48] showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.[49]

Many aspects of speech recognition were taken over by a deep learning method called long short-term memory (LSTM), a recurrent neural network published by Hochreiter and Schmidhuber in 1997.[50] LSTM RNNs avoid the vanishing gradient problem and can learn ""Very Deep Learning"" tasks[12] that require memories of events that happened thousands of discrete time steps before, which is important for speech. In 2003, LSTM started to become competitive with traditional speech recognizers on certain tasks.[51] Later it was combined with connectionist temporal classification (CTC)[52] in stacks of LSTM RNNs.[53] In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which they made available through Google Voice Search.[54]

In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh[55][56][57] showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation.[58] The papers referred to learning for deep belief nets.

Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved.[59][60] Convolutional neural networks (CNNs) were superseded for ASR by CTC[52] for LSTM.[50][54][61][62][63] but are more successful in computer vision.

The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun.[64] Industrial applications of deep learning to large-scale speech recognition started around 2010.

The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets (DNN) might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems.[59] The nature of the recognition errors produced by the two types of systems was characteristically different,[65] offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems.[8][66][67] Analysis around 2009–2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition,[65] eventually leading to pervasive and dominant use in that industry. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.[59][65][68]

In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.[69][70][71][66]

Advances in hardware have driven renewed interest in deep learning. In 2009, Nvidia was involved in what was called the “big bang” of deep learning, “as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs).”[72] That year, Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times.[73] In particular, GPUs are well-suited for the matrix/vector computations involved in machine learning.[74][75][76] GPUs speed up training algorithms by orders of magnitude, reducing running times from weeks to days.[77][78] Further, specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models.[79]"
40,"Deep learning revolution

How deep learning is a subset of machine learning and how machine learning is a subset of artificial intelligence (AI).
In 2012, a team led by George E. Dahl won the ""Merck Molecular Activity Challenge"" using multi-task deep neural networks to predict the biomolecular target of one drug.[80][81] In 2014, Hochreiter's group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients, household products and drugs and won the ""Tox21 Data Challenge"" of NIH, FDA and NCATS.[82][83][84]

Significant additional impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades, and GPU implementations of NNs for years, including CNNs, fast implementations of CNNs on GPUs were needed to progress on computer vision.[74][76][35][85][12] In 2011, this approach achieved for the first time superhuman performance in a visual pattern recognition contest. Also in 2011, it won the ICDAR Chinese handwriting contest, and in May 2012, it won the ISBI image segmentation contest.[86] Until 2011, CNNs did not play a major role at computer vision conferences, but in June 2012, a paper by Ciresan et al. at the leading conference CVPR[3] showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records. In October 2012, a similar system by Krizhevsky et al.[4] won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. In November 2012, Ciresan et al.'s system also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic.[87] In 2013 and 2014, the error rate on the ImageNet task using deep learning was further reduced, following a similar trend in large-scale speech recognition.

Image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.[88][89][90]

Some researchers state that the October 2012 ImageNet victory anchored the start of a ""deep learning revolution"" that has transformed the AI industry.[91]

In March 2019, Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing."
41,"Neural networks
Artificial neural networks
Main article: Artificial neural network
Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.

An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.

Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.

The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.

Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.

As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, playing ""Go""[92] ).

Deep neural networks
A deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers.[9][12] There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions.[93] These components functioning similar to the human brains and can be trained like any other ML algorithm.[citation needed]

For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer,[citation needed] and complex DNN have many layers, hence the name ""deep"" networks.

DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives.[94] The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.[9] For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.[95]

Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.

DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or ""weights"", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights.[96] That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.

Recurrent neural networks (RNNs), in which data can flow in any direction, are used for applications such as language modeling.[97][98][99][100][101] Long short-term memory is particularly effective for this use.[50][102]

Convolutional deep neural networks (CNNs) are used in computer vision.[103] CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).[104]

Challenges
As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.

DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning[26] or weight decay ({\displaystyle \ell _{2}}\ell _{2}-regularization) or sparsity ({\displaystyle \ell _{1}}\ell _{1}-regularization) can be applied during training to combat overfitting.[105] Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies.[106] Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.[107]

DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples)[108] speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.[109][110]

Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights for CMAC. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.[111][112]

Hardware
Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.[113] By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.[114] OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.[115][116]

Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones[117] and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform.[118]

Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage. In 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).[119]

In 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing.[120] The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds.[120] Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications.[120]"
42,"Applications
Automatic speech recognition
Main article: Speech recognition
Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn ""Very Deep Learning"" tasks[12] that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates[102] is competitive with traditional speech recognizers on certain tasks.[51]

The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences.[121] Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.

Method	Percent phone
error rate (PER) (%)
Randomly Initialized RNN[122]	26.1
Bayesian Triphone GMM-HMM	25.6
Hidden Trajectory (Generative) Model	24.8
Monophone Randomly Initialized DNN	23.4
Monophone DBN-DNN	22.4
Triphone GMM-HMM with BMMI Training	21.7
Monophone DBN-DNN on fbank	20.7
Convolutional DNN[123]	20.0
Convolutional DNN w. Heterogeneous Pooling	18.7
Ensemble DNN/CNN/RNN[124]	18.3
Bidirectional LSTM	17.8
Hierarchical Convolutional Deep Maxout Network[125]	16.5
The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007, accelerated progress in eight major areas:[8][68][66]

Scale-up/out and accelerated DNN training and decoding
Sequence discriminative training
Feature processing by deep models with solid understanding of the underlying mechanisms
Adaptation of DNNs and related deep models
Multi-task and transfer learning by DNNs and related deep models
CNNs and how to design them to best exploit domain knowledge of speech
RNN and its rich LSTM variants
Other types of deep models including tensor-based models and integrated deep generative/discriminative models.
All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.[8][126][127]"
43,"Image recognition
Main article: Computer vision
A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.[128]

Deep learning-based image recognition has become ""superhuman"", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces.[129][130]

Deep learning-trained vehicles now interpret 360° camera views.[131] Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes."
44,"Visual art processing
Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of

identifying the style period of a given painting[132][133]
Neural Style Transfer – capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video[132][133]
generating striking imagery based on random visual input fields.[132][133]
Natural language processing
Main article: Natural language processing
Neural networks have been used for implementing language models since the early 2000s.[97] LSTM helped to improve machine translation and language modeling.[98][99][100]

Other key techniques in this field are negative sampling[134] and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN.[135] Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing.[135] Deep neural architectures provide the best results for constituency parsing,[136] sentiment analysis,[137] information retrieval,[138][139] spoken language understanding,[140] machine translation,[98][141] contextual entity linking,[141] writing style recognition,[142] Text classification and others.[143]

Recent developments generalize word embedding to sentence embedding.

Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network.[144][145][146][147] Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system ""learns from millions of examples.""[145] It translates ""whole sentences at a time, rather than pieces. Google Translate supports over one hundred languages.[145] The network encodes the ""semantics of the sentence rather than simply memorizing phrase-to-phrase translations"".[145][148] GT uses English as an intermediate between most language pairs.[148]"
45,"Drug discovery and toxicology
For more information, see Drug discovery and Toxicology.
A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects.[149][150] Research has explored use of deep learning to predict the biomolecular targets,[80][81] off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.[82][83][84]

AtomNet is a deep learning system for structure-based rational drug design.[151] AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus[152] and multiple sclerosis.[153][154]

In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set.[155] In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.[156][157]"
46,"Customer relationship management
Main article: Customer relationship management
Deep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.[158]

Recommendation systems
Main article: Recommender system
Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations.[159][160] Multi-view deep learning has been applied for learning user preferences from multiple domains.[161] The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.

Bioinformatics
Main article: Bioinformatics
An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.[162]

In medical informatics, deep learning was used to predict sleep quality based on data from wearables[163] and predictions of health complications from electronic health record data.[164]

Medical image analysis
Deep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement.[165][166] Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency.[167][168]

Mobile advertising
Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server.[169] Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.

Image restoration
Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization.[170] These applications include learning methods such as ""Shrinkage Fields for Effective Image Restoration""[171] which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.

Financial fraud detection
Deep learning is being successfully applied to financial fraud detection, tax evasion detection,[172] and anti-money laundering.[173]

Military
The United States Department of Defense applied deep learning to train robots in new tasks through observation.[174]

Partial differential equations
Physics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner.[175] One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods relies on.[176][177]

Image Reconstruction
Image reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging [178] and ultrasound imaging.[179]

Relation to human cognitive and brain development
Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s.[180][181][182][183] These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, ""...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature.""[184]

A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism.[185][186] Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality.[187][188] In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.[189]

Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons[190] and neural populations.[191] Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system[192] both at the single-unit[193] and at the population[194] levels.

Commercial activity
Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.[195]

Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player.[196][197][198] Google Translate uses a neural network to translate between more than 100 languages.

In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.[199]

As of 2008,[200] researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor.[174] First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot the ability to learn new tasks through observation.[174] Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as “good job” and “bad job.”[201]

Criticism and comment
Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science."
47,"Theory
See also: Explainable AI
A main criticism concerns the lack of theory surrounding some methods.[202] Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear.[citation needed] (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.[203]

Others point out that deep learning should be looked at as a step towards realizing strong AI, not as an all-encompassing solution. Despite the power of deep learning methods, they still lack much of the functionality needed for realizing this goal entirely. Research psychologist Gary Marcus noted:

""Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning.""[204]

In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained[205] demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's[206] website.

Errors
Some deep learning architectures display problematic behaviors,[207] such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014)[208] and misclassifying minuscule perturbations of correctly classified images (2013).[209] Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures.[207] These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar[210] decompositions of observed entities and events.[207] Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition[211] and artificial intelligence (AI).[212]

Cyber threat
As deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception.[213] By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an “adversarial attack.”[214]

In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system.[215] One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.[216]

Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.[215]

ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.[215]

In 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and hypothesized that this could ""serve as a stepping stone for further attacks (e.g., opening a web page hosting drive-by malware).""[215]

In “data poisoning,” false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.[215]

Reliance on human microwork

This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: ""Deep learning"" – news · newspapers · books · scholar · JSTOR (April 2021) (Learn how and when to remove this template message)
Most Deep Learning systems rely on training and verification data that is generated and/or annotated by humans. It has been argued in media philosophy that not only low-paid clickwork (e.g. on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such.[217] The philosopher Rainer Mühlhoff distinguishes five types of ""machinic capture"" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) ""trapping and tracking"" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.[217]

Mühlhoff argues that in most commercial end-user applications of Deep Learning such as Facebook's face recognition system, the need for training data does not stop once an ANN is trained. Rather, there is a continued demand for human-generated verification data to constantly calibrate and update the ANN. For this purpose Facebook introduced the feature that once a user is automatically recognized in an image, they receive a notification. They can choose whether of not they like to be publicly labeled on the image, or tell Facebook that it is not them in the picture.[218] This user interface is a mechanism to generate ""a constant stream of verification data""[217] to further train the network in real-time. As Mühlhoff argues, involvement of human users to generate training and verification data is so typical for most commercial end-user applications of Deep Learning that such systems may be referred to as ""human-aided artificial intelligence"".[217]"
48,"Natural language processing : Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of ""understanding"" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.

Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.            History
Further information: History of natural language processing
Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled ""Computing Machinery and Intelligence"" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.

Symbolic NLP (1950s – early 1990s)
The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.

1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s when the first statistical machine translation systems were developed.
1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted ""blocks worlds"" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the ""patient"" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to ""My head hurts"" with ""Why do you say your head hurts?"".
1970s: During the 1970s, many programmers began to write ""conceptual ontologies"", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, the first chatterbots were written (e.g., PARRY).
1980s: The 1980s and early 1990s mark the hey-day of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology[3]), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory[4]) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.[5]
Statistical NLP (1990s–2010s)
Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[6]

1990s: Many of the notable early successes on statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.
2000s: With the growth of the web, increasing amounts of raw (unannotated) language data has become available since the mid-1990s. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical.
Neural NLP (present)
In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques[7][8] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling[9] and parsing.[10][11] This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care.[12]

Methods: Rules, statistics, neural networks
In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup:[13][14] such as by writing grammars or devising heuristic rules for stemming.

More recent systems based on machine-learning algorithms have many advantages over hand-produced rules:

The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed.
Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input gracefully with handwritten rules, or, more generally, creating systems of handwritten rules that make soft decisions, is extremely difficult, error-prone and time-consuming.
Systems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on handwritten rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task. In particular, there is a limit to the complexity of systems based on handwritten rules, beyond which the systems become more and more unmanageable. However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process.
Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used:

when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system,
for preprocessing in NLP pipelines, e.g., tokenization, or
for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.
Statistical methods
Since the so-called ""statistical revolution""[15][16] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora (the plural form of corpus, is a set of documents, possibly with human or computer annotations) of typical real-world examples.

Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of ""features"" that are generated from the input data. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature (complex-valued embeddings,[17] and neural networks in general have also been proposed, for e.g. speech[18]). Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.

Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.

Since the neural turn, statistical methods in NLP research have been largely replaced by neural networks. However, they continue to be relevant for contexts in which statistical interpretability and transparency is required.

Neural networks
Further information: Artificial neural network
A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[19] the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning. Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT).

Common NLP tasks
The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.

Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.

Text and speech processing
Optical character recognition (OCR)
Given an image representing printed text, determine the corresponding text.
Speech recognition
Given a sound clip of a person or people speaking, determine the textual representation of the speech. This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed ""AI-complete"" (see above). In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process. Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent.
Speech segmentation
Given a sound clip of a person or people speaking, separate it into words. A subtask of speech recognition and typically grouped with it.
Text-to-speech
Given a text, transform those units and produce a spoken representation. Text-to-speech can be used to aid the visually impaired.[20]
Word segmentation (Tokenization)
Separate a chunk of continuous text into separate words. For a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. Sometimes this process is also used in cases like bag of words (BOW) creation in data mining.
Morphological analysis
Lemmatization
The task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. Lemmatization is another technique for reducing words to their normalized form. But in this case, the transformation actually uses a dictionary to map words to their actual form.[21]
Morphological segmentation
Separate words into individual morphemes and identify the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g., ""open, opens, opened, opening"") as separate words. In languages such as Turkish or Meitei,[22] a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.
Part-of-speech tagging
Given a sentence, determine the part of speech (POS) for each word. Many words, especially common ones, can serve as multiple parts of speech. For example, ""book"" can be a noun (""the book on the table"") or verb (""to book a flight""); ""set"" can be a noun, verb or adjective; and ""out"" can be any of at least five different parts of speech.
Stemming
The process of reducing inflected (or sometimes derived) words to a base form (e.g., ""close"" will be the root for ""closed"", ""closing"", ""close"", ""closer"" etc.). Stemming yields similar results as lemmatization, but does so on grounds of rules, not a dictionary.
Syntactic analysis
Grammar induction[23]
Generate a formal grammar that describes a language's syntax.
Sentence breaking (also known as ""sentence boundary disambiguation"")
Given a chunk of text, find the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g., marking abbreviations).
Parsing
Determine the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses: perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human). There are two primary types of parsing: dependency parsing and constituency parsing. Dependency parsing focuses on the relationships between words in a sentence (marking things like primary objects and predicates), whereas constituency parsing focuses on building out the parse tree using a probabilistic context-free grammar (PCFG) (see also stochastic grammar).
Lexical semantics (of individual words in context)
Lexical semantics
What is the computational meaning of individual words in context?
Distributional semantics
How can we learn semantic representations from data?
Named entity recognition (NER)
Given a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case, is often inaccurate or insufficient. For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized. Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives.
Sentiment analysis (see also Multimodal sentiment analysis)
Extract subjective information usually from a set of documents, often using online reviews to determine ""polarity"" about specific objects. It is especially useful for identifying trends of public opinion in social media, for marketing.
Terminology extraction
The goal of terminology extraction is to automatically extract relevant terms from a given corpus.
Word-sense disambiguation (WSD)
Many words have more than one meaning; we have to select the meaning which makes the most sense in context. For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or an online resource such as WordNet.
Entity linking
Many words - typically proper names - refer to named entities; here we have to select the entity (a famous individual, a location, a company, etc.) which is referred to in context.
Relational semantics (semantics of individual sentences)
Relationship extraction
Given a chunk of text, identify the relationships among named entities (e.g. who is married to whom).
Semantic parsing
Given a piece of text (typically a sentence), produce a formal representation of its semantics, either as a graph (e.g., in AMR parsing) or in accordance with a logical formalism (e.g., in DRT parsing). This challenge typically includes aspects of several more elementary NLP tasks from semantics (e.g., semantic role labelling, word-sense disambiguation) and can be extended to include full-fledged discourse analysis (e.g., discourse analysis, coreference; see Natural language understanding below).
Semantic role labelling (see also implicit semantic role labelling below)
Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames), then identify and classify the frame elements (semantic roles).
Discourse (semantics beyond individual sentences)
Coreference resolution
Given a sentence or larger chunk of text, determine which words (""mentions"") refer to the same objects (""entities""). Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. The more general task of coreference resolution also includes identifying so-called ""bridging relationships"" involving referring expressions. For example, in a sentence such as ""He entered John's house through the front door"", ""the front door"" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to).
Discourse analysis
This rubric includes several related tasks. One task is discourse parsing, i.e., identifying the discourse structure of a connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast). Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes-no question, content question, statement, assertion, etc.).
Implicit semantic role labelling
Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames) and their explicit semantic roles in the current sentence (see Semantic role labelling above). Then, identify semantic roles that are not explicitly realized in the current sentence, classify them into arguments that are explicitly realized elsewhere in the text and those that are not specified, and resolve the former against the local text. A closely related task is zero anaphora resolution, i.e., the extension of coreference resolution to pro-drop languages.
Recognizing textual entailment
Given two text fragments, determine if one being true entails the other, entails the other's negation, or allows the other to be either true or false.[24]
Topic segmentation and recognition
Given a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment.
Argument mining
The goal of argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs.[25] Such argumentative structures include the premise, conclusions, the argument scheme and the relationship between the main and subsidiary argument, or the main and counter-argument within discourse.[26][27]
Higher-level NLP applications
Automatic summarization (text summarization)
Produce a readable summary of a chunk of text. Often used to provide summaries of the text of a known type, such as research papers, articles in the financial section of a newspaper.
Book generation
Not an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. The first machine-generated book was created by a rule-based system in 1984 (Racter, The policeman's beard is half-constructed).[28] The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models. The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham).[29] Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization.
Dialogue management
Computer systems intended to converse with a human.
Document AI
A Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types. NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants.[30]
Grammatical error correction
Grammatical error detection and correction involves a great band-width of problems on all levels of linguistic analysis (phonology/orthography, morphology, syntax, semantics, pragmatics). Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language. It has thus been subject to a number of shared tasks since 2011.[31][32][33] As far as orthography, morphology, syntax and certain aspects of semantics are concerned, and due to the development of powerful neural language models such as GPT-2, this can now (2019) be considered a largely solved problem and is being marketed in various commercial applications.
Machine translation
Automatically translate text from one human language to another. This is one of the most difficult problems, and is a member of a class of problems colloquially termed ""AI-complete"", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) to solve properly.
Natural-language generation (NLG):
Convert information from computer databases or semantic intents into readable human language.
Natural-language understanding (NLU)
Convert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization.[34]
Question answering
Given a human-language question, determine its answer. Typical questions have a specific right answer (such as ""What is the capital of Canada?""), but sometimes open-ended questions are also considered (such as ""What is the meaning of life?"").
General tendencies and (possible) future directions
Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[35]

Interest on increasingly abstract, ""cognitive"" aspects of natural language (1999-2001: shallow parsing, 2002-03: named entity recognition, 2006-09/2017-18: dependency syntax, 2004-05/2008-09 semantic role labelling, 2011-12 coreference, 2015-16: discourse parsing, 2019: semantic parsing).
Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages)
Elimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems)
Cognition and NLP
Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).

Cognition refers to ""the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.""[36] Cognitive science is the interdisciplinary, scientific study of the mind and its processes.[37] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.[38] Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.

As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[39] with two defining aspects:

Apply the theory of conceptual metaphor, explained by Lakoff as “the understanding of one idea, in terms of another” which provides an idea of the intent of the author.[40] For example, consider the English word “big”. When used in a comparison (“That is a big tree”), the author's intent is to imply that the tree is ”physically large” relative to other trees or the authors experience. When used metaphorically (”Tomorrow is a big day”), the author’s intent to imply ”importance”. The intent behind other usages, like in ”She is a big person” will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information.
Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e.g., by means of a probabilistic context-free grammar (PCFG). The mathematical equation for such algorithms is presented in US patent 9269353:
{\displaystyle {RMM(token_{N})}={PMM(token_{N})}\times {\frac {1}{2d}}\left(\sum _{i=-d}^{d}{((PMM(token_{N-1})}\times {PF(token_{N},token_{N-1}))_{i}}\right)}{\displaystyle {RMM(token_{N})}={PMM(token_{N})}\times {\frac {1}{2d}}\left(\sum _{i=-d}^{d}{((PMM(token_{N-1})}\times {PF(token_{N},token_{N-1}))_{i}}\right)}
Where,
RMM, is the Relative Measure of Meaning
token, is any block of text, sentence, phrase or word
N, is the number of tokens being analyzed
PMM, is the Probable Measure of Meaning based on a corpora
d, is the location of the token along the sequence of N-1 tokens
PF, is the Probability Function specific to a language
Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[41] functional grammar,[42] construction grammar,[43] computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[44] of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of ""cognitive AI"".[45] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit).[46]"
